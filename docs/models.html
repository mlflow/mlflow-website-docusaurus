
  

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MLflow Models &mdash; MLflow 2.8.2.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/models.html">
  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/simple-cards.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="MLflow 2.8.2.dev0 documentation" href="index.html"/>
        <link rel="next" title="MLflow Model Registry" href="/model-registry.html"/>
        <link rel="prev" title="MLflow Projects" href="/projects.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="_static/jquery.js"></script>
<script type="text/javascript" src="_static/underscore.js"></script>
<script type="text/javascript" src="_static/doctools.js"></script>
<script type="text/javascript" src="_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="index.html" class="wy-nav-top-logo"
      ><img src="_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.8.2.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="index.html" class="main-navigation-home"><img src="_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="new-features/index.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="llms/index.html">LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="deployment/index.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="projects.html">MLflow Projects</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">MLflow Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#storage-format">Storage Format</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fields-in-the-mlmodel-format">Fields in the MLmodel Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#additional-logged-files">Additional Logged Files</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-signature-and-input-example">Model Signature And Input Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-inference-params">Model Inference Params</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-signature">Model Signature</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-signature-types">Model Signature Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#signature-enforcement">Signature Enforcement</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-log-models-with-signatures">How To Log Models With Signatures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-set-signatures-on-models">How To Set Signatures on Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-input-example">Model Input Example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-to-log-model-with-column-based-example">How To Log Model With Column-based Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-log-model-with-tensor-based-example">How To Log Model With Tensor-based Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-log-model-with-example-containing-params">How To Log Model With Example Containing Params</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-api">Model API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-model-flavors">Built-In Model Flavors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#python-function-python-function">Python Function (<code class="docutils literal notranslate"><span class="pre">python_function</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-to-save-model-as-python-function">How To Save Model As Python Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-load-and-score-python-function-models">How To Load And Score Python Function Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#r-function-crate">R Function (<code class="docutils literal notranslate"><span class="pre">crate</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#crate-usage"><code class="docutils literal notranslate"><span class="pre">crate</span></code> usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#h2o-h2o">H<sub>2</sub>O (<code class="docutils literal notranslate"><span class="pre">h2o</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#h2o-pyfunc-usage">h2o pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#keras-keras">Keras (<code class="docutils literal notranslate"><span class="pre">keras</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#keras-pyfunc-usage">Keras pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mleap-mleap">MLeap (<code class="docutils literal notranslate"><span class="pre">mleap</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-pytorch">PyTorch (<code class="docutils literal notranslate"><span class="pre">pytorch</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pytorch-pyfunc-usage">PyTorch pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#scikit-learn-sklearn">Scikit-learn (<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scikit-learn-pyfunc-usage">Scikit-learn pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-mllib-spark">Spark MLlib (<code class="docutils literal notranslate"><span class="pre">spark</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#spark-mllib-pyfunc-usage">Spark MLlib pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-tensorflow">TensorFlow (<code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onnx-onnx">ONNX (<code class="docutils literal notranslate"><span class="pre">onnx</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#onnx-pyfunc-usage-example">ONNX pyfunc usage example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mxnet-gluon-gluon">MXNet Gluon (<code class="docutils literal notranslate"><span class="pre">gluon</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gluon-pyfunc-usage">Gluon pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#xgboost-xgboost">XGBoost (<code class="docutils literal notranslate"><span class="pre">xgboost</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#xgboost-pyfunc-usage"><code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#lightgbm-lightgbm">LightGBM (<code class="docutils literal notranslate"><span class="pre">lightgbm</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lightgbm-pyfunc-usage"><code class="docutils literal notranslate"><span class="pre">LightGBM</span></code> pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#catboost-catboost">CatBoost (<code class="docutils literal notranslate"><span class="pre">catboost</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#catboost-pyfunc-usage"><code class="docutils literal notranslate"><span class="pre">CatBoost</span></code> pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spacy-spacy">Spacy(<code class="docutils literal notranslate"><span class="pre">spaCy</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#spacy-pyfunc-usage"><code class="docutils literal notranslate"><span class="pre">Spacy</span></code> pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fastai-fastai">Fastai(<code class="docutils literal notranslate"><span class="pre">fastai</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#statsmodels-statsmodels">Statsmodels (<code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#statsmodels-pyfunc-usage">Statsmodels pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#prophet-prophet">Prophet (<code class="docutils literal notranslate"><span class="pre">prophet</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prophet-pyfunc-usage">Prophet pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pmdarima-pmdarima">Pmdarima (<code class="docutils literal notranslate"><span class="pre">pmdarima</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openai-openai-experimental">OpenAI (<code class="docutils literal notranslate"><span class="pre">openai</span></code>) (Experimental)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#langchain-langchain-experimental">LangChain (<code class="docutils literal notranslate"><span class="pre">langchain</span></code>) (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#logging-retrievalqa-chains">Logging RetrievalQA Chains</a></li>
<li class="toctree-l4"><a class="reference internal" href="#logging-a-retriever-and-evaluate-it-individually">Logging a retriever and evaluate it individually</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#john-snow-labs-johnsnowlabs-experimental">John Snow Labs (<code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code>) (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#to-deploy-the-john-snow-labs-model-as-a-container">To deploy the John Snow Labs model as a container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#to-deploy-the-john-snow-labs-model-without-a-container">To deploy the John Snow Labs model without a container</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#diviner-diviner">Diviner (<code class="docutils literal notranslate"><span class="pre">diviner</span></code>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#diviner-types">Diviner Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#metrics-and-parameters-logging-for-diviner">Metrics and Parameters logging for Diviner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#diviner-pyfunc-usage">Diviner pyfunc usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#transformers-transformers-experimental">Transformers (<code class="docutils literal notranslate"><span class="pre">transformers</span></code>) (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#input-and-output-types-for-pyfunc">Input and Output types for PyFunc</a></li>
<li class="toctree-l4"><a class="reference internal" href="#save-and-load-options-for-transformers">Save and Load options for transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sentencetransformers-sentence-transformers-experimental">SentenceTransformers (<code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code>) (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-evaluation">Model Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#evaluating-with-llms">Evaluating with LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluating-with-extra-metrics">Evaluating with Extra Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluating-with-a-function">Evaluating with a Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluating-with-a-static-dataset">Evaluating with a Static Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-model-validation">Performing Model Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-validation-with-giskard-s-plugin">Model Validation with Giskard’s plugin</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-validation-with-trubrics-plugin">Model Validation with Trubrics’ plugin</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-customization">Model Customization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#custom-python-models">Custom Python Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-creating-a-custom-add-n-model">Example: Creating a custom “add n” model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-saving-an-xgboost-model-in-mlflow-format">Example: Saving an XGBoost model in MLflow format</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files">Example: Logging a transformers model with hf:/ schema to avoid copying large files</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#custom-flavors">Custom Flavors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-creating-a-custom-sktime-flavor">Example: Creating a custom “sktime” flavor</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-using-the-custom-sktime-flavor">Example: Using the custom “sktime” flavor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-deployment-tools">Built-In Deployment Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#local-model-deployment">Deploy MLflow models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#serving-with-mlserver">Serving with MLServer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#encoding-complex-data">Encoding complex data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="#environment-management-tools">Environment Management Tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-a-python-function-model-on-microsoft-azure-ml">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Microsoft Azure ML</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-a-python-function-model-on-amazon-sagemaker">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Amazon SageMaker</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#commands">Commands</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#export-a-python-function-model-as-an-apache-spark-udf">Export a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model as an Apache Spark UDF</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deployment-to-custom-targets">Deployment to Custom Targets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id24">Commands</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id26">Community Model Flavors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>MLflow Models</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/models.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="mlflow-models">
<span id="models"></span><h1>MLflow Models<a class="headerlink" href="#mlflow-models" title="Permalink to this headline"> </a></h1>
<p>An MLflow Model is a standard format for packaging machine learning models that can be used in a
variety of downstream tools—for example, real-time serving through a REST API or batch inference
on Apache Spark. The format defines a convention that lets you save a model in different “flavors”
that can be understood by different downstream tools.</p>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#storage-format" id="id32">Storage Format</a></p></li>
<li><p><a class="reference internal" href="#model-signature-and-input-example" id="id33">Model Signature And Input Example</a></p></li>
<li><p><a class="reference internal" href="#model-api" id="id34">Model API</a></p></li>
<li><p><a class="reference internal" href="#built-in-model-flavors" id="id35">Built-In Model Flavors</a></p></li>
<li><p><a class="reference internal" href="#model-evaluation" id="id36">Model Evaluation</a></p></li>
<li><p><a class="reference internal" href="#model-customization" id="id37">Model Customization</a></p></li>
<li><p><a class="reference internal" href="#built-in-deployment-tools" id="id38">Built-In Deployment Tools</a></p></li>
<li><p><a class="reference internal" href="#deployment-to-custom-targets" id="id39">Deployment to Custom Targets</a></p></li>
<li><p><a class="reference internal" href="#id26" id="id40">Community Model Flavors</a></p></li>
</ul>
</div>
<div class="section" id="storage-format">
<span id="model-storage-format"></span><h2><a class="toc-backref" href="#id32">Storage Format</a><a class="headerlink" href="#storage-format" title="Permalink to this headline"> </a></h2>
<p>Each MLflow Model is a directory containing arbitrary files, together with an <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code>
file in the root of the directory that can define multiple <em>flavors</em> that the model can be viewed
in.</p>
<p>Flavors are the key concept that makes MLflow Models powerful: they are a convention that deployment
tools can use to understand the model, which makes it possible to write tools that work with models
from any ML library without having to integrate each tool with each library. MLflow defines
several “standard” flavors that all of its built-in deployment tools support, such as a “Python
function” flavor that describes how to run the model as a Python function. However, libraries can
also define and use other flavors. For example, MLflow’s <a class="reference internal" href="python_api/mlflow.sklearn.html#module-mlflow.sklearn" title="mlflow.sklearn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sklearn</span></code></a> library allows
loading models back as a scikit-learn <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> object for use in code that is aware of
scikit-learn, or as a generic Python function for use in tools that just need to apply the model
(for example, the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">deployments</span></code> tool with the option <code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">sagemaker</span></code> for deploying models
to Amazon SageMaker).</p>
<p>All of the flavors that a particular model supports are defined in its <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file in YAML
format. For example, <a class="reference internal" href="python_api/mlflow.sklearn.html#module-mlflow.sklearn" title="mlflow.sklearn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sklearn</span></code></a> outputs models as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Directory written by mlflow.sklearn.save_model(model, &quot;my_model&quot;)
my_model/
├── MLmodel
├── model.pkl
├── conda.yaml
├── python_env.yaml
└── requirements.txt
</pre></div>
</div>
<p>And its <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file describes two flavors:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">time_created</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2018-05-25T17:28:53.35</span>

<span class="nt">flavors</span><span class="p">:</span>
<span class="w">  </span><span class="nt">sklearn</span><span class="p">:</span>
<span class="w">    </span><span class="nt">sklearn_version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.19.1</span>
<span class="w">    </span><span class="nt">pickled_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model.pkl</span>
<span class="w">  </span><span class="nt">python_function</span><span class="p">:</span>
<span class="w">    </span><span class="nt">loader_module</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlflow.sklearn</span>
</pre></div>
</div>
<p>This model can then be used with any tool that supports either the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> or
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> model flavor. For example, the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span> <span class="pre">serve</span></code> command
can serve a model with the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> or the <code class="docutils literal notranslate"><span class="pre">crate</span></code> (R Function) flavor:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>my_model
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you wish to serve a model from inside a docker container (or to
query it from another machine), you need to change the network address to <code class="docutils literal notranslate"><span class="pre">0.0.0.0</span></code>
using the <code class="docutils literal notranslate"><span class="pre">-h</span></code> argument.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-h<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>-m<span class="w"> </span>my_model
</pre></div>
</div>
</div>
<p>In addition, the <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">deployments</span></code> command-line tool can package and deploy models to AWS
SageMaker as long as they support the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>deployments<span class="w"> </span>create<span class="w"> </span>-t<span class="w"> </span>sagemaker<span class="w"> </span>-m<span class="w"> </span>my_model<span class="w"> </span><span class="o">[</span>other<span class="w"> </span>options<span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When a model registered in the MLflow Model Registry is downloaded, a YAML file named
<cite>registered_model_meta</cite> is added to the model directory on the downloader’s side.
This file contains the name and version of the model referenced in the MLflow Model Registry,
and will be used for deployment and other purposes.</p>
</div>
<div class="section" id="fields-in-the-mlmodel-format">
<h3>Fields in the MLmodel Format<a class="headerlink" href="#fields-in-the-mlmodel-format" title="Permalink to this headline"> </a></h3>
<p>Apart from a <strong>flavors</strong> field listing the model flavors, the MLmodel YAML format can contain
the following fields:</p>
<dl class="simple">
<dt>time_created</dt><dd><p>Date and time when the model was created, in UTC ISO 8601 format.</p>
</dd>
<dt>run_id</dt><dd><p>ID of the run that created the model, if the model was saved using <a class="reference internal" href="tracking.html#tracking"><span class="std std-ref">MLflow Tracking</span></a>.</p>
</dd>
<dt>signature</dt><dd><p><a class="reference internal" href="#model-signature"><span class="std std-ref">model signature</span></a> in JSON format.</p>
</dd>
<dt>input_example</dt><dd><p>reference to an artifact with <a class="reference internal" href="#input-example"><span class="std std-ref">input example</span></a>.</p>
</dd>
<dt>databricks_runtime</dt><dd><p>Databricks runtime version and type, if the model was trained in a Databricks notebook or job.</p>
</dd>
<dt>mlflow_version</dt><dd><p>The version of MLflow that was used to log the model.</p>
</dd>
</dl>
</div>
<div class="section" id="additional-logged-files">
<h3>Additional Logged Files<a class="headerlink" href="#additional-logged-files" title="Permalink to this headline"> </a></h3>
<p>For environment recreation, we automatically log <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code>, <code class="docutils literal notranslate"><span class="pre">python_env.yaml</span></code>, and <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> files whenever a model is logged. These files can then be used to reinstall dependencies using <code class="docutils literal notranslate"><span class="pre">conda</span></code> or <code class="docutils literal notranslate"><span class="pre">virtualenv</span></code> with <code class="docutils literal notranslate"><span class="pre">pip</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Anaconda Inc. updated their <a class="reference external" href="https://www.anaconda.com/terms-of-service">terms of service</a> for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See <a class="reference external" href="https://www.anaconda.com/blog/anaconda-commercial-edition-faq">Anaconda Commercial Edition FAQ</a> for more information. Your use of any Anaconda channels is governed by their terms of service.</p>
<p>MLflow models logged before <a class="reference external" href="https://mlflow.org/news/2021/06/18/1.18.0-release/index.html">v1.18</a> were by default logged with the conda <code class="docutils literal notranslate"><span class="pre">defaults</span></code> channel (<a class="reference external" href="https://repo.anaconda.com/pkgs/">https://repo.anaconda.com/pkgs/</a>) as a dependency. Because of this license change, MLflow has stopped the use of the <code class="docutils literal notranslate"><span class="pre">defaults</span></code> channel for models logged using MLflow v1.18 and above. The default channel logged is now <code class="docutils literal notranslate"><span class="pre">conda-forge</span></code>, which points at the community managed <a class="reference external" href="https://conda-forge.org/">https://conda-forge.org/</a>.</p>
<p>If you logged a model before MLflow v1.18 without excluding the <code class="docutils literal notranslate"><span class="pre">defaults</span></code> channel from the conda environment for the model, that model may have a dependency on the <code class="docutils literal notranslate"><span class="pre">defaults</span></code> channel that you may not have intended.
To manually confirm whether a model has this dependency, you can examine <code class="docutils literal notranslate"><span class="pre">channel</span></code> value in the <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> file that is packaged with the logged model. For example, a model’s <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> with a <code class="docutils literal notranslate"><span class="pre">defaults</span></code> channel dependency may look like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlflow-env</span>
<span class="nt">channels</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">defaults</span>
<span class="nt">dependencies</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python=3.8.8</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">pip</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlflow==2.3</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==0.23.2</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cloudpickle==1.6.0</span>
</pre></div>
</div>
<p>If you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code>. You can do this by specifying the channel in the <code class="docutils literal notranslate"><span class="pre">conda_env</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">log_model()</span></code>.</p>
<p>For more information on the <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> API, see the MLflow documentation for the model flavor you are working with, for example, <a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sklearn.log_model()</span></code></a>.</p>
</div>
<dl class="simple">
<dt>conda.yaml</dt><dd><p>When saving a model, MLflow provides the option to pass in a conda environment parameter that can contain dependencies used by the model. If no conda environment is provided, a default environment is created based on the flavor of the model. This conda environment is then saved in <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code>.</p>
</dd>
<dt>python_env.yaml</dt><dd><p>This file contains the following information that’s required to restore a model environment using virtualenv:</p>
<ul class="simple">
<li><p>Python version</p></li>
<li><p>Version specifiers for <code class="docutils literal notranslate"><span class="pre">pip</span></code>, <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, and <code class="docutils literal notranslate"><span class="pre">wheel</span></code></p></li>
<li><p>Pip requirements of the model (reference to <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>)</p></li>
</ul>
</dd>
<dt>requirements.txt</dt><dd><p>The requirements file is created from the <a class="reference external" href="https://www.anaconda.com/blog/using-pip-in-a-conda-environment">pip portion</a> of the <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> environment specification. Additional pip dependencies can be added to <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> by including them as a pip dependency in a conda environment and logging the model with the environment or using the <code class="docutils literal notranslate"><span class="pre">pip_requirements</span></code> argument of the <cite>mlflow.&lt;flavor&gt;.log_model</cite> API.</p>
</dd>
</dl>
<p>The following shows an example of saving a model with a manually specified conda environment and the corresponding content of the generated <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> and <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conda_env</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;channels&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;conda-forge&quot;</span><span class="p">],</span>
    <span class="s2">&quot;dependencies&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;python=3.8.8&quot;</span><span class="p">,</span> <span class="s2">&quot;pip&quot;</span><span class="p">],</span>
    <span class="s2">&quot;pip&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;mlflow==2.3&quot;</span><span class="p">,</span> <span class="s2">&quot;scikit-learn==0.23.2&quot;</span><span class="p">,</span> <span class="s2">&quot;cloudpickle==1.6.0&quot;</span><span class="p">],</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;mlflow-env&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">)</span>
</pre></div>
</div>
<p>The written <code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlflow-env</span>
<span class="nt">channels</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">conda-forge</span>
<span class="nt">dependencies</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python=3.8.8</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">pip</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlflow==2.3</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scikit-learn==0.23.2</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cloudpickle==1.6.0</span>
</pre></div>
</div>
<p>The written <code class="docutils literal notranslate"><span class="pre">python_env.yaml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">python</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3.8.8</span>
<span class="nt">build_dependencies</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip==21.1.3</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">setuptools==57.4.0</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">wheel==0.37.0</span>
<span class="nt">dependencies</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-r requirements.txt</span>
</pre></div>
</div>
<p>The written <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mlflow==2.3
scikit-learn==0.23.2
cloudpickle==1.6.0
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-signature-and-input-example">
<span id="model-metadata"></span><h2><a class="toc-backref" href="#id33">Model Signature And Input Example</a><a class="headerlink" href="#model-signature-and-input-example" title="Permalink to this headline"> </a></h2>
<p>When working with ML models you often need to know some basic functional properties of the model
at hand, such as “What inputs does it expect?” and “What output does it produce?”. MLflow models can
include the following additional metadata about model inputs, outputs and params that can be used by
downstream tooling:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#inference-params"><span class="std std-ref">Model Inference Params</span></a> - description of params used for model inference.</p></li>
<li><p><a class="reference internal" href="#model-signature"><span class="std std-ref">Model Signature</span></a> - description of a model’s inputs, outputs and parameters.</p></li>
<li><p><a class="reference internal" href="#input-example"><span class="std std-ref">Model Input Example</span></a> - example of a valid model input.</p></li>
</ul>
<div class="section" id="model-inference-params">
<span id="inference-params"></span><h3>Model Inference Params<a class="headerlink" href="#model-inference-params" title="Permalink to this headline"> </a></h3>
<p>Inference params are parameters that are passed to the model at inference time. These parameters
do not need to be specified when training the model, but could be useful for inference. With the
advances in foundational models, more often “inference configuration” is used to modify the behavior
of a model. In some cases, especially popular LLMs, the same model may require different parameter
configurations for different samples at inference time.</p>
<p>With this newly introduced feature, you can now specify a dictionary of inference params during
model inference, providing a broader utility and improved control over the generated inference
results, particularly for LLM use cases. By passing different params such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>,
<code class="docutils literal notranslate"><span class="pre">max_length</span></code>, etc. to the model at inference time, you can easily control the output of the model.</p>
<p>In order to use params at inference time, a valid <a class="reference internal" href="#model-signature"><span class="std std-ref">Model Signature</span></a> with
<code class="docutils literal notranslate"><span class="pre">params</span></code> must be defined. The params are passed to the model at inference time as a dictionary
and each param value will be validated against the corresponding param type defined in the model
signature. Valid param types are <code class="docutils literal notranslate"><span class="pre">DataType</span></code> or <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">list</span> <span class="pre">of</span> <span class="pre">DataType</span></code> as listed below.</p>
<ul class="simple">
<li><p>DataType.string or an array of DataType.string</p></li>
<li><p>DataType.integer or an array of DataType.integer</p></li>
<li><p>DataType.boolean or an array of DataType.boolean</p></li>
<li><p>DataType.double or an array of DataType.double</p></li>
<li><p>DataType.float or an array of DataType.float</p></li>
<li><p>DataType.long or an array of DataType.long</p></li>
<li><p>DataType.datetime or an array of DataType.datetime</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When validating param values, the values will be converted to python native types.
For example, <code class="docutils literal notranslate"><span class="pre">np.float32(0.1)</span></code> will be converted to <code class="docutils literal notranslate"><span class="pre">float(0.1)</span></code>.</p>
</div>
<p>A simple example of using params for model inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>


<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;str_param&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;int_array&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="c1"># params&#39; default values are saved with ModelSignature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">([</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">MyModel</span><span class="p">(),</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span>

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Not passing params -- predict with default values</span>
<span class="n">loaded_predict</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="s2">&quot;input&quot;</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">loaded_predict</span> <span class="o">==</span> <span class="p">[</span><span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>

<span class="c1"># Passing some params -- add default values</span>
<span class="n">loaded_predict</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;str_param&quot;</span><span class="p">:</span> <span class="s2">&quot;new_string&quot;</span><span class="p">})</span>
<span class="k">assert</span> <span class="n">loaded_predict</span> <span class="o">==</span> <span class="p">[</span><span class="s2">&quot;new_string&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>

<span class="c1"># Passing all params -- override</span>
<span class="n">loaded_predict</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;str_param&quot;</span><span class="p">:</span> <span class="s2">&quot;new_string&quot;</span><span class="p">,</span> <span class="s2">&quot;int_array&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]}</span>
<span class="p">)</span>
<span class="k">assert</span> <span class="n">loaded_predict</span> <span class="o">==</span> <span class="p">[</span><span class="s2">&quot;new_string&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="section" id="model-signature">
<span id="id1"></span><h3>Model Signature<a class="headerlink" href="#model-signature" title="Permalink to this headline"> </a></h3>
<p>Model signatures define input, output and parameters schemas for MLflow models, providing a standard
interface to codify and enforce the correct use of your models. Signatures are fetched by the MLflow Tracking
UI and Model Registry UI to display model inputs, outputs and params. They are also utilized by
<a class="reference internal" href="#built-in-deployment"><span class="std std-ref">MLflow model deployment tools</span></a> to validate inference inputs according to
the model’s assigned signature (see the <a class="reference internal" href="#signature-enforcement"><span class="std std-ref">Signature enforcement</span></a> section
for more details).</p>
<p>To include a signature with your model, pass a <a class="reference internal" href="#input-example"><span class="std std-ref">model input example</span></a> as an argument to the
appropriate log_model or save_model call, e.g. <a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.log_model()</span></code></a>,
and the model signature will be automatically inferred
(see the <a class="reference internal" href="#how-to-log-models-with-signatures"><span class="std std-ref">How to log models with signatures</span></a> section for more details).
The model signature is stored in JSON format in the <a class="reference internal" href="python_api/mlflow.pyfunc.html#pyfunc-model-config"><span class="std std-ref">MLmodel file</span></a> in your
model artifacts, together with other model metadata. To set a signature on a logged or
saved model, use the <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.set_signature" title="mlflow.models.set_signature"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_signature()</span></code></a> API
(see the <a class="reference internal" href="#how-to-set-signatures-on-models"><span class="std std-ref">How to set signatures on models</span></a> section for more details).</p>
<div class="section" id="model-signature-types">
<h4>Model Signature Types<a class="headerlink" href="#model-signature-types" title="Permalink to this headline"> </a></h4>
<p>A model signature consists on inputs and outputs schemas, each of which can be either column-based or tensor-based.
Column-based schemas are a sequence of (optionally) named columns with type specified as one of the
<a class="reference internal" href="python_api/mlflow.types.html#mlflow.types.DataType" title="mlflow.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">data</span> <span class="pre">types</span></code></a>.
Tensor-based schemas are a sequence of (optionally) named tensors with type specified as one of the
<a class="reference external" href="https://numpy.org/devdocs/user/basics.types.html">numpy data types</a>.
Params schema is a sequence of ParamSpec, each of which contains <code class="docutils literal notranslate"><span class="pre">name</span></code>, <code class="docutils literal notranslate"><span class="pre">type</span></code>, <code class="docutils literal notranslate"><span class="pre">default</span></code> and <code class="docutils literal notranslate"><span class="pre">shape</span></code> fields.
<code class="docutils literal notranslate"><span class="pre">type</span></code> field must be specified as one of the <a class="reference internal" href="python_api/mlflow.types.html#mlflow.types.DataType" title="mlflow.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">data</span> <span class="pre">types</span></code></a>, and <code class="docutils literal notranslate"><span class="pre">shape</span></code>
field should be <code class="docutils literal notranslate"><span class="pre">None</span></code> for scalar parameters, or <code class="docutils literal notranslate"><span class="pre">(-1,)</span></code> for list parameters.
See some examples of constructing them below.</p>
<div class="section" id="column-based-signature-example">
<h5>Column-based Signature Example<a class="headerlink" href="#column-based-signature-example" title="Permalink to this headline"> </a></h5>
<p>Each column-based input and output is represented by a type corresponding to one of
<a class="reference internal" href="python_api/mlflow.types.html#mlflow.types.DataType" title="mlflow.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">data</span> <span class="pre">types</span></code></a> and an optional name. Input columns can also be marked
as <code class="docutils literal notranslate"><span class="pre">optional</span></code>, indicating whether they are required as input to the model or can be omitted. The following example
displays a modified MLmodel file excerpt containing the model signature for a classification model trained on
the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/iris">Iris dataset</a>. The input has 4 named, numeric columns and 1 named,
optional string column.
The output is an unnamed integer specifying the predicted class.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">signature</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;sepal</span><span class="nv"> </span><span class="s">length</span><span class="nv"> </span><span class="s">(cm)&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;double&quot;},</span><span class="nv"> </span><span class="s">{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;sepal</span><span class="nv"> </span><span class="s">width</span>
<span class="w">      </span><span class="s">(cm)&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;double&quot;},</span><span class="nv"> </span><span class="s">{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;petal</span><span class="nv"> </span><span class="s">length</span><span class="nv"> </span><span class="s">(cm)&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;double&quot;},</span><span class="nv"> </span><span class="s">{&quot;name&quot;:</span>
<span class="w">      </span><span class="s">&quot;petal</span><span class="nv"> </span><span class="s">width</span><span class="nv"> </span><span class="s">(cm)&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;double&quot;},</span><span class="nv"> </span><span class="s">{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;class&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;string&quot;,</span><span class="nv"> </span><span class="s">&quot;optional&quot;:</span><span class="nv"> </span><span class="s">&quot;true&quot;}]&#39;</span>
<span class="w">    </span><span class="nt">outputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;integer&quot;}]&#39;</span>
<span class="w">    </span><span class="nt">params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</pre></div>
</div>
</div>
<div class="section" id="tensor-based-signature-example">
<h5>Tensor-based Signature Example<a class="headerlink" href="#tensor-based-signature-example" title="Permalink to this headline"> </a></h5>
<p>Each tensor-based input and output is represented by a dtype corresponding to one of
<a class="reference external" href="https://numpy.org/devdocs/user/basics.types.html">numpy data types</a>, shape and an optional name.
Tensor-based signatures do not support optional inputs.
When specifying the shape, -1 is used for axes that may be variable in size.
The following example displays an MLmodel file excerpt containing the model signature for a
classification model trained on the <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>.
The input has one named tensor where input sample is an image represented by a 28 × 28 × 1 array
of float32 numbers. The output is an unnamed tensor that has 10 units specifying the
likelihood corresponding to each of the 10 classes. Note that the first dimension of the input
and the output is the batch size and is thus set to -1 to allow for variable batch sizes.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">signature</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;images&quot;,</span><span class="nv"> </span><span class="s">&quot;dtype&quot;:</span><span class="nv"> </span><span class="s">&quot;uint8&quot;,</span><span class="nv"> </span><span class="s">&quot;shape&quot;:</span><span class="nv"> </span><span class="s">[-1,</span><span class="nv"> </span><span class="s">28,</span><span class="nv"> </span><span class="s">28,</span><span class="nv"> </span><span class="s">1]}]&#39;</span>
<span class="w">    </span><span class="nt">outputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;shape&quot;:</span><span class="nv"> </span><span class="s">[-1,</span><span class="nv"> </span><span class="s">10],</span><span class="nv"> </span><span class="s">&quot;dtype&quot;:</span><span class="nv"> </span><span class="s">&quot;float32&quot;}]&#39;</span>
<span class="w">    </span><span class="nt">params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</pre></div>
</div>
</div>
<div class="section" id="signature-with-params-example">
<h5>Signature with params Example<a class="headerlink" href="#signature-with-params-example" title="Permalink to this headline"> </a></h5>
<p>The params field is optional and is used to specify parameters that can be used for model inference.
Params accept scalar values of type <a class="reference internal" href="python_api/mlflow.types.html#mlflow.types.DataType" title="mlflow.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">data</span> <span class="pre">types</span></code></a>, or a list
of such values. The default value of a parameter is specified by setting the <code class="docutils literal notranslate"><span class="pre">default</span></code> field, and the value
should be of the type specified by <code class="docutils literal notranslate"><span class="pre">type</span></code> field. The <code class="docutils literal notranslate"><span class="pre">shape</span></code> field can be used to specify the shape
of the value, it should be <code class="docutils literal notranslate"><span class="pre">None</span></code> for scalar values and <code class="docutils literal notranslate"><span class="pre">(-1,)</span></code> for a list.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">signature</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;text&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;string&quot;}]&#39;</span>
<span class="w">    </span><span class="nt">outputs</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;output&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;string&quot;}]&#39;</span>
<span class="w">    </span><span class="nt">params</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;[{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;temperature&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;float&quot;,</span><span class="nv"> </span><span class="s">&quot;default&quot;:</span><span class="nv"> </span><span class="s">0.5,</span><span class="nv"> </span><span class="s">&quot;shape&quot;:</span><span class="nv"> </span><span class="s">null},</span>
<span class="w">              </span><span class="s">{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;top_k&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;integer&quot;,</span><span class="nv"> </span><span class="s">&quot;default&quot;:</span><span class="nv"> </span><span class="s">1,</span><span class="nv"> </span><span class="s">&quot;shape&quot;:</span><span class="nv"> </span><span class="s">null},</span>
<span class="w">              </span><span class="s">{&quot;name&quot;:</span><span class="nv"> </span><span class="s">&quot;suppress_tokens&quot;,</span><span class="nv"> </span><span class="s">&quot;type&quot;:</span><span class="nv"> </span><span class="s">&quot;integer&quot;,</span><span class="nv"> </span><span class="s">&quot;default&quot;:</span><span class="nv"> </span><span class="s">[101,</span><span class="nv"> </span><span class="s">102],</span><span class="nv"> </span><span class="s">&quot;shape&quot;:</span><span class="nv"> </span><span class="s">[-1]}]&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="signature-enforcement">
<span id="id3"></span><h4>Signature Enforcement<a class="headerlink" href="#signature-enforcement" title="Permalink to this headline"> </a></h4>
<p>Schema enforcement checks the provided input and params against the model’s signature and
raises an exception if the input is not compatible and will issue a warning or raise an exception if the params are incompatible. This enforcement is applied in MLflow before
calling the underlying model implementation, and during model inference process.
Note that this enforcement only applies when using <a class="reference internal" href="#built-in-deployment"><span class="std std-ref">MLflow
model deployment tools</span></a> or when loading models as <code class="docutils literal notranslate"><span class="pre">python_function</span></code>. In
particular, it is not applied to models that are loaded in their native format (e.g. by calling
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.load_model" title="mlflow.sklearn.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sklearn.load_model()</span></code></a>).</p>
<div class="section" id="name-ordering-enforcement">
<h5>Name Ordering Enforcement<a class="headerlink" href="#name-ordering-enforcement" title="Permalink to this headline"> </a></h5>
<p>The input names are checked against the model signature. If there are any missing required inputs,
MLflow will raise an exception. Missing optional inputs will not raise an exception.
Extra inputs that were not declared in the signature will be ignored. If the input schema in the
signature defines input names, input matching is done by name and the inputs are reordered to match the
signature. If the input schema does not have input names, matching is done by position
(i.e. MLflow will only check the number of inputs).</p>
</div>
<div class="section" id="input-type-enforcement">
<h5>Input Type Enforcement<a class="headerlink" href="#input-type-enforcement" title="Permalink to this headline"> </a></h5>
<p>The input types are checked against the signature.</p>
<p>For models with column-based signatures (i.e DataFrame inputs), MLflow will perform safe type conversions
if necessary. Generally, only conversions that are guaranteed to be lossless are allowed. For
example, int -&gt; long or int -&gt; double conversions are ok, long -&gt; double is not. If the types cannot
be made compatible, MLflow will raise an error.</p>
<p>For models with tensor-based signatures, type checking is strict (i.e an exception will be thrown if
the input type does not match the type specified by the schema).</p>
</div>
<div class="section" id="params-type-and-shape-enforcement">
<h5>Params Type and Shape Enforcement<a class="headerlink" href="#params-type-and-shape-enforcement" title="Permalink to this headline"> </a></h5>
<p>The params types and shapes are checked against the signature.</p>
<p>MLflow verifies the compatibility of each parameter provided during inference by comparing its type and shape
with those specified in the signature. Scalar values should have a shape of <code class="docutils literal notranslate"><span class="pre">None</span></code>, while list values should have
a shape of <code class="docutils literal notranslate"><span class="pre">(-1,)</span></code>. If the parameter’s type or shape is incompatible, an exception will be raised.
Additionally, the value of the parameter is validated against the specified type in the signature. We attempt
to convert the value to the specified type, and if this conversion fails, an MlflowException will be raised.
A valid list of params is documented in <a class="reference internal" href="#inference-params"><span class="std std-ref">Model Inference Params</span></a> section.
Models that have signatures and are used for inference with declared params not part of the logged signature will
have a warning issued with each request and the invalid params ignored during inference.</p>
</div>
<div class="section" id="handling-integers-with-missing-values">
<h5>Handling Integers With Missing Values<a class="headerlink" href="#handling-integers-with-missing-values" title="Permalink to this headline"> </a></h5>
<p>Integer data with missing values is typically represented as floats in Python. Therefore, data
types of integer columns in Python can vary depending on the data sample. This type of variance can
cause schema enforcement errors at runtime since integer and float are not compatible types. For
example, if your training data did not have any missing values for integer column c, its type will
be integer. However, when you attempt to score a sample of the data that does include a missing
value in column c, its type will be float. If your model signature specified c to have integer type,
MLflow will raise an error since it can not convert float to int. Note that MLflow uses python to
serve models and to deploy models to Spark, so this can affect most model deployments. The best way
to avoid this problem is to declare integer columns as doubles (float64) whenever there are
missing values.</p>
</div>
<div class="section" id="handling-date-and-timestamp">
<h5>Handling Date and Timestamp<a class="headerlink" href="#handling-date-and-timestamp" title="Permalink to this headline"> </a></h5>
<p>For datetime values, Python has precision built into the type. For example, datetime values with
day precision have numpy type <code class="docutils literal notranslate"><span class="pre">datetime64[D]</span></code>, while values with nanosecond precision have
type <code class="docutils literal notranslate"><span class="pre">datetime64[ns]</span></code>. Datetime precision is ignored for column-based model signature but is
enforced for tensor-based signatures.</p>
</div>
<div class="section" id="handling-ragged-arrays">
<h5>Handling Ragged Arrays<a class="headerlink" href="#handling-ragged-arrays" title="Permalink to this headline"> </a></h5>
<p>Ragged arrays can be created in numpy and are produced with a shape of (-1,) and a dytpe of
object. This will be handled by default when using <code class="docutils literal notranslate"><span class="pre">infer_signature</span></code>, resulting in a
signature containing <code class="docutils literal notranslate"><span class="pre">Tensor('object',</span> <span class="pre">(-1,))</span></code>. A similar signature can be manually created
containing a more detailed representation of a ragged array, for a more expressive signature,
such as <code class="docutils literal notranslate"><span class="pre">Tensor('float64',</span> <span class="pre">(-1,</span> <span class="pre">-1,</span> <span class="pre">-1,</span> <span class="pre">3))</span></code>. Enforcement will then be done on as much detail
as possible given the signature provided, and will support ragged input arrays as well.</p>
</div>
</div>
<div class="section" id="how-to-log-models-with-signatures">
<span id="id4"></span><h4>How To Log Models With Signatures<a class="headerlink" href="#how-to-log-models-with-signatures" title="Permalink to this headline"> </a></h4>
<p>To include a signature with your model, pass a <a class="reference internal" href="#input-example"><span class="std std-ref">model input example</span></a> to the
appropriate log_model or save_model call, e.g. <a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.log_model()</span></code></a>,
and the model signature will be automatically inferred from the input example and the model’s
predicted output of the input example.</p>
<p>You may also include a signature object with your model by passing a <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.ModelSignature" title="mlflow.models.ModelSignature"><code class="xref py py-class docutils literal notranslate"><span class="pre">signature</span> <span class="pre">object</span></code></a> as an argument to your log_model or save_model call. The model signature
object can be created by hand or <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.infer_signature" title="mlflow.models.infer_signature"><code class="xref py py-func docutils literal notranslate"><span class="pre">inferred</span></code></a> from datasets with
valid model inputs (e.g. the training dataset with target column omitted), valid model outputs
(e.g. model predictions generated on the training dataset), and valid model parameters (a dictionary of
parameters passed to model for inference; e.g. <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig">Generation Configs for transformers</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Model signatures are utilized in <a class="reference internal" href="#built-in-deployment"><span class="std std-ref">MLflow model deployment tools</span></a>, which
commonly serve the Python Function (PyFunc) flavor of MLflow models. Hence, when passing a signature
object to your log_model or save_model call, it is recommended that the signature represent the
inputs and outputs of the model’s PyFunc flavor. This is especially important when the model loaded
as a PyFunc model has an input schema that is different from the test dataset schema (as is the case with
the <a class="reference internal" href="#pmdarima-flavor"><span class="std std-ref">pmdarima model flavor</span></a>).</p>
</div>
<div class="section" id="id5">
<h5>Column-based Signature Example<a class="headerlink" href="#id5" title="Permalink to this headline"> </a></h5>
<p>The following example demonstrates how to store a model signature for a simple classifier trained
on the <code class="docutils literal notranslate"><span class="pre">Iris</span> <span class="pre">dataset</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_train</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
    <span class="c1"># Take the first row of the training dataset as the model input example.</span>
    <span class="n">input_example</span> <span class="o">=</span> <span class="n">iris_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="c1"># The signature is automatically inferred from the input example and its predicted output.</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;iris_rf&quot;</span><span class="p">,</span> <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">)</span>
</pre></div>
</div>
<p>The same signature can be explicitly created and logged as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">ModelSignature</span><span class="p">,</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">Schema</span><span class="p">,</span> <span class="n">ColSpec</span>

<span class="c1"># Option 1: Manually construct the signature object</span>
<span class="n">input_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">ColSpec</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal length (cm)&quot;</span><span class="p">),</span>
        <span class="n">ColSpec</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal width (cm)&quot;</span><span class="p">),</span>
        <span class="n">ColSpec</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="s2">&quot;petal length (cm)&quot;</span><span class="p">),</span>
        <span class="n">ColSpec</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="s2">&quot;petal width (cm)&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">output_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="s2">&quot;long&quot;</span><span class="p">)])</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">ModelSignature</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_schema</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_schema</span><span class="p">)</span>

<span class="c1"># Option 2: Infer the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">iris_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris_train</span><span class="p">))</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;iris_rf&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h5>Tensor-based Signature Example<a class="headerlink" href="#id6" title="Permalink to this headline"> </a></h5>
<p>The following example demonstrates how to store a model signature for a simple classifier trained
on the <code class="docutils literal notranslate"><span class="pre">MNIST</span> <span class="pre">dataset</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Take the first three training examples as the model input example.</span>
    <span class="n">input_example</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;mnist_cnn&quot;</span><span class="p">,</span> <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">)</span>
</pre></div>
</div>
<p>The same signature can be explicitly created and logged as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">ModelSignature</span><span class="p">,</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">Schema</span><span class="p">,</span> <span class="n">TensorSpec</span>

<span class="c1"># Option 1: Manually construct the signature object</span>
<span class="n">input_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">TensorSpec</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">output_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">([</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))])</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">ModelSignature</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_schema</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_schema</span><span class="p">)</span>

<span class="c1"># Option 2: Infer the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">))</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;mnist_cnn&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h5>Signature with params Example<a class="headerlink" href="#id7" title="Permalink to this headline"> </a></h5>
<p>The following example demonstrates how to store a model signature with params
for a simple transformers model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;mrm8488/t5-base-finetuned-common_gen&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;pencil draw paper&quot;</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.62</span><span class="p">,</span>
    <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.85</span><span class="p">,</span>
    <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.15</span><span class="p">,</span>
    <span class="s2">&quot;begin_suppress_tokens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># infer signature with params</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span>
    <span class="n">params</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># save model with signature</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="s2">&quot;text2text&quot;</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;text2text&quot;</span><span class="p">)</span>

<span class="c1"># predict with params</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pyfunc_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>The same signature can be created explicitly as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">ModelSignature</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">ColSpec</span><span class="p">,</span> <span class="n">ParamSchema</span><span class="p">,</span> <span class="n">ParamSpec</span><span class="p">,</span> <span class="n">Schema</span>

<span class="n">input_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">)])</span>
<span class="n">output_schema</span> <span class="o">=</span> <span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">)])</span>
<span class="n">params_schema</span> <span class="o">=</span> <span class="n">ParamSchema</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;top_k&quot;</span><span class="p">,</span> <span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;num_beams&quot;</span><span class="p">,</span> <span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="mf">0.62</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;top_p&quot;</span><span class="p">,</span> <span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;repetition_penalty&quot;</span><span class="p">,</span> <span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="mf">1.15</span><span class="p">),</span>
        <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;begin_suppress_tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">ModelSignature</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">input_schema</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_schema</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params_schema</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-to-set-signatures-on-models">
<span id="id8"></span><h4>How To Set Signatures on Models<a class="headerlink" href="#how-to-set-signatures-on-models" title="Permalink to this headline"> </a></h4>
<p>Models can be saved with without model signatures or with incorrect ones. To add a signature to an
existing logged model, use the
<a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.set_signature" title="mlflow.models.set_signature"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.set_signature()</span></code></a> API. Here are some examples.</p>
<div class="section" id="set-signature-on-logged-model">
<h5>Set Signature on Logged Model<a class="headerlink" href="#set-signature-on-logged-model" title="Permalink to this headline"> </a></h5>
<p>The following example demonstrates how to set a model signature on a logged sklearn model.
Suppose you’ve logged a sklearn model without a signature like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;iris_rf&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can set a signature on the logged model as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models.model</span> <span class="kn">import</span> <span class="n">get_model_info</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span><span class="p">,</span> <span class="n">set_signature</span>

<span class="c1"># load the logged model</span>
<span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/iris_rf&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># construct the model signature from test dataset</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="c1"># set the signature for the logged model</span>
<span class="n">set_signature</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">signature</span><span class="p">)</span>

<span class="c1"># now when you load the model again, it will have the desired signature</span>
<span class="k">assert</span> <span class="n">get_model_info</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span><span class="o">.</span><span class="n">signature</span> <span class="o">==</span> <span class="n">signature</span>
</pre></div>
</div>
<p>Note that model signatures can also be set on model artifacts saved outside of MLflow Tracking. As
an example, you can easily set a signature on a locally saved iris model by altering the model_uri
variable in the previous code snippet to point to the model’s local directory.</p>
</div>
<div class="section" id="set-signature-on-model-version">
<span id="set-signature-on-mv"></span><h5>Set Signature on Model Version<a class="headerlink" href="#set-signature-on-model-version" title="Permalink to this headline"> </a></h5>
<p>As MLflow Model Registry artifacts are meant to be read-only, you cannot directly set a signature on
a model version or model artifacts represented by <code class="docutils literal notranslate"><span class="pre">models:/</span></code> URI schemes. Instead, you should first set
the signature on the source model artifacts and generate a new model version using the updated
model artifacts. The following example illustrates how this can be done.</p>
<p>Supposed you have created the following model version without a signature like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.client</span> <span class="kn">import</span> <span class="n">MlflowClient</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;add_signature_model&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(),</span> <span class="s2">&quot;sklearn-model&quot;</span><span class="p">)</span>

<span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/sklearn-model&quot;</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
<p>To set a signature on the model version, create a duplicate model version with the new signature
as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.store.artifact.models_artifact_repo</span> <span class="kn">import</span> <span class="n">ModelsArtifactRepository</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;add_signature_model&quot;</span>
<span class="n">model_version</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mv</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_model_version</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">model_version</span><span class="p">)</span>

<span class="c1"># set a dummy signature on the model version source</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">set_signature</span><span class="p">(</span><span class="n">mv</span><span class="o">.</span><span class="n">source</span><span class="p">,</span> <span class="n">signature</span><span class="p">)</span>

<span class="c1"># create a new model version with the updated source</span>
<span class="n">client</span><span class="o">.</span><span class="n">create_model_version</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">mv</span><span class="o">.</span><span class="n">source</span><span class="p">,</span> <span class="n">run_id</span><span class="o">=</span><span class="n">mv</span><span class="o">.</span><span class="n">run_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that this process overwrites the model artifacts from the source run of model version 1
with a new model signature.</p>
</div>
</div>
</div>
<div class="section" id="model-input-example">
<span id="input-example"></span><h3>Model Input Example<a class="headerlink" href="#model-input-example" title="Permalink to this headline"> </a></h3>
<p>A model input example provides an instance of a valid model input. Input examples are stored with
the model as separate artifacts and are referenced in the <a class="reference internal" href="python_api/mlflow.pyfunc.html#pyfunc-model-config"><span class="std std-ref">MLmodel file</span></a>.
To include an input example with your model, add it to the appropriate log_model call, e.g.
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.log_model()</span></code></a>. Input examples are also used to infer
model signatures in log_model calls when signatures aren’t specified.</p>
<p>Similar to model signatures, model inputs can be column-based (i.e DataFrames) or tensor-based
(i.e numpy.ndarrays). We offer support for input_example with params by using tuple to combine model
inputs and params. See examples below:</p>
<div class="section" id="how-to-log-model-with-column-based-example">
<h4>How To Log Model With Column-based Example<a class="headerlink" href="#how-to-log-model-with-column-based-example" title="Permalink to this headline"> </a></h4>
<p>For models accepting column-based inputs, an example can be a single record or a batch of records. The
sample input can be in the following formats:</p>
<ul class="simple">
<li><p>Pandas DataFrame</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> (of scalars, strings, or lists of scalar values)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bytes</span></code></p></li>
</ul>
<p>The given example will be converted to a Pandas DataFrame and then serialized to json using the Pandas split-oriented
format. Bytes are base64-encoded. The following example demonstrates how you can log a column-based
input example with your model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_example</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;sepal length (cm)&quot;</span><span class="p">:</span> <span class="mf">5.1</span><span class="p">,</span>
    <span class="s2">&quot;sepal width (cm)&quot;</span><span class="p">:</span> <span class="mf">3.5</span><span class="p">,</span>
    <span class="s2">&quot;petal length (cm)&quot;</span><span class="p">:</span> <span class="mf">1.4</span><span class="p">,</span>
    <span class="s2">&quot;petal width (cm)&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="how-to-log-model-with-tensor-based-example">
<h4>How To Log Model With Tensor-based Example<a class="headerlink" href="#how-to-log-model-with-tensor-based-example" title="Permalink to this headline"> </a></h4>
<p>For models accepting tensor-based inputs, an example must be a batch of inputs. By default, the axis 0
is the batch axis unless specified otherwise in the model signature. The sample input can be passed in
as any of the following formats:</p>
<ul class="simple">
<li><p>numpy ndarray</p></li>
<li><p>Python <code class="docutils literal notranslate"><span class="pre">dict</span></code> mapping a string to a numpy array</p></li>
<li><p>Scipy <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code> (sparse matrix)</p></li>
<li><p>Scipy <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> (sparse matrix).</p></li>
</ul>
<p>The following example demonstrates how you can log a tensor-based input example with your model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># each input has shape (4, 4)</span>
<span class="n">input_example</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">134</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">56</span><span class="p">],</span> <span class="p">[</span><span class="mi">253</span><span class="p">,</span> <span class="mi">242</span><span class="p">,</span> <span class="mi">195</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">82</span><span class="p">]],</span>
        <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">33</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">166</span><span class="p">],</span> <span class="p">[</span><span class="mi">76</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="p">[</span><span class="mi">33</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">82</span><span class="p">]],</span>
    <span class="p">],</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="how-to-log-model-with-example-containing-params">
<h4>How To Log Model With Example Containing Params<a class="headerlink" href="#how-to-log-model-with-example-containing-params" title="Permalink to this headline"> </a></h4>
<p>For models that require additional parameters during inference, you can include an input_example
containing params when saving or logging the model. To achieve this, the sample input should be
provided as a <code class="docutils literal notranslate"><span class="pre">tuple</span></code>. The first element of the tuple is the input data example, and the
second element is a <code class="docutils literal notranslate"><span class="pre">dict</span></code> of params. A comprehensive list of valid params is documented in
<a class="reference internal" href="#inference-params"><span class="std std-ref">Model Inference Params</span></a> section.</p>
<ul class="simple">
<li><p>Python <code class="docutils literal notranslate"><span class="pre">tuple</span></code>: (input_data, params)</p></li>
</ul>
<p>The following example demonstrates how to log a model with an example containing params:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># input_example could be column-based or tensor-based example as shown above</span>
<span class="c1"># params must be a valid dictionary of params</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="s2">&quot;Hello, Dolly!&quot;</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">input_example</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="model-api">
<span id="id9"></span><h2><a class="toc-backref" href="#id34">Model API</a><a class="headerlink" href="#model-api" title="Permalink to this headline"> </a></h2>
<p>You can save and load MLflow Models in multiple ways. First, MLflow includes integrations with
several common libraries. For example, <a class="reference internal" href="python_api/mlflow.sklearn.html#module-mlflow.sklearn" title="mlflow.sklearn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sklearn</span></code></a> contains
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.save_model" title="mlflow.sklearn.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model</span></code></a>, <a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model</span></code></a>,
and <a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.load_model" title="mlflow.sklearn.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_model</span></code></a> functions for scikit-learn models. Second,
you can use the <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model" title="mlflow.models.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlflow.models.Model</span></code></a> class to create and write models. This
class has four key functions:</p>
<ul class="simple">
<li><p><a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.add_flavor" title="mlflow.models.Model.add_flavor"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_flavor</span></code></a> to add a flavor to the model. Each flavor
has a string name and a dictionary of key-value attributes, where the values can be any object
that can be serialized to YAML.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.save" title="mlflow.models.Model.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">save</span></code></a> to save the model to a local directory.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.log" title="mlflow.models.Model.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">log</span></code></a> to log the model as an artifact in the
current run using MLflow Tracking.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.load" title="mlflow.models.Model.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">load</span></code></a> to load a model from a local directory or
from an artifact in a previous run.</p></li>
</ul>
</div>
<div class="section" id="built-in-model-flavors">
<span id="models-built-in-model-flavors"></span><h2><a class="toc-backref" href="#id35">Built-In Model Flavors</a><a class="headerlink" href="#built-in-model-flavors" title="Permalink to this headline"> </a></h2>
<p>MLflow provides several standard flavors that might be useful in your applications. Specifically,
many of its deployment tools support these flavors, so you can export your own model in one of these
flavors to benefit from all these tools:</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#python-function-python-function" id="id41">Python Function (<code class="docutils literal notranslate"><span class="pre">python_function</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#r-function-crate" id="id42">R Function (<code class="docutils literal notranslate"><span class="pre">crate</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#h2o-h2o" id="id43">H<sub>2</sub>O (<code class="docutils literal notranslate"><span class="pre">h2o</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#keras-keras" id="id44">Keras (<code class="docutils literal notranslate"><span class="pre">keras</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#mleap-mleap" id="id45">MLeap (<code class="docutils literal notranslate"><span class="pre">mleap</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#pytorch-pytorch" id="id46">PyTorch (<code class="docutils literal notranslate"><span class="pre">pytorch</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#scikit-learn-sklearn" id="id47">Scikit-learn (<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#spark-mllib-spark" id="id48">Spark MLlib (<code class="docutils literal notranslate"><span class="pre">spark</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#tensorflow-tensorflow" id="id49">TensorFlow (<code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#onnx-onnx" id="id50">ONNX (<code class="docutils literal notranslate"><span class="pre">onnx</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#mxnet-gluon-gluon" id="id51">MXNet Gluon (<code class="docutils literal notranslate"><span class="pre">gluon</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#xgboost-xgboost" id="id52">XGBoost (<code class="docutils literal notranslate"><span class="pre">xgboost</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#lightgbm-lightgbm" id="id53">LightGBM (<code class="docutils literal notranslate"><span class="pre">lightgbm</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#catboost-catboost" id="id54">CatBoost (<code class="docutils literal notranslate"><span class="pre">catboost</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#spacy-spacy" id="id55">Spacy(<code class="docutils literal notranslate"><span class="pre">spaCy</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#fastai-fastai" id="id56">Fastai(<code class="docutils literal notranslate"><span class="pre">fastai</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#statsmodels-statsmodels" id="id57">Statsmodels (<code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#prophet-prophet" id="id58">Prophet (<code class="docutils literal notranslate"><span class="pre">prophet</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#pmdarima-pmdarima" id="id59">Pmdarima (<code class="docutils literal notranslate"><span class="pre">pmdarima</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#openai-openai-experimental" id="id60">OpenAI (<code class="docutils literal notranslate"><span class="pre">openai</span></code>) (Experimental)</a></p></li>
<li><p><a class="reference internal" href="#langchain-langchain-experimental" id="id61">LangChain (<code class="docutils literal notranslate"><span class="pre">langchain</span></code>) (Experimental)</a></p></li>
<li><p><a class="reference internal" href="#john-snow-labs-johnsnowlabs-experimental" id="id62">John Snow Labs (<code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code>) (Experimental)</a></p></li>
<li><p><a class="reference internal" href="#diviner-diviner" id="id63">Diviner (<code class="docutils literal notranslate"><span class="pre">diviner</span></code>)</a></p></li>
<li><p><a class="reference internal" href="#transformers-transformers-experimental" id="id64">Transformers (<code class="docutils literal notranslate"><span class="pre">transformers</span></code>) (Experimental)</a></p></li>
<li><p><a class="reference internal" href="#sentencetransformers-sentence-transformers-experimental" id="id65">SentenceTransformers (<code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code>) (Experimental)</a></p></li>
</ul>
</div>
<div class="section" id="python-function-python-function">
<span id="pyfunc-model-flavor"></span><h3><a class="toc-backref" href="#id41">Python Function (<code class="docutils literal notranslate"><span class="pre">python_function</span></code>)</a><a class="headerlink" href="#python-function-python-function" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model flavor serves as a default model interface for MLflow Python models.
Any MLflow Python model is expected to be loadable as a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model. This enables
other MLflow tools to work with any python model regardless of which persistence module or
framework was used to produce the model. This interoperability is very powerful because it allows
any Python model to be productionized in a variety of environments.</p>
<p>In addition, the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model flavor defines a generic filesystem <a class="reference internal" href="python_api/mlflow.pyfunc.html#pyfunc-filesystem-format"><span class="std std-ref">model format</span></a> for Python models and provides utilities for saving and loading models
to and from this format. The format is self-contained in the sense that it includes all the
information necessary to load and use a model. Dependencies are stored either directly with the
model or referenced via conda environment. This model format allows other tools to integrate
their models with MLflow.</p>
<div class="section" id="how-to-save-model-as-python-function">
<h4>How To Save Model As Python Function<a class="headerlink" href="#how-to-save-model-as-python-function" title="Permalink to this headline"> </a></h4>
<p>Most <code class="docutils literal notranslate"><span class="pre">python_function</span></code> models are saved as part of other model flavors - for example, all mlflow
built-in flavors include the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor in the exported models. In addition, the
<a class="reference internal" href="python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" title="mlflow.pyfunc"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pyfunc</span></code></a> module defines functions for creating <code class="docutils literal notranslate"><span class="pre">python_function</span></code> models explicitly.
This module also includes utilities for creating custom Python models, which is a convenient way of
adding custom python code to ML models. For more information, see the <a class="reference internal" href="#custom-python-models"><span class="std std-ref">custom Python models
documentation</span></a>.</p>
</div>
<div class="section" id="how-to-load-and-score-python-function-models">
<h4>How To Load And Score Python Function Models<a class="headerlink" href="#how-to-load-and-score-python-function-models" title="Permalink to this headline"> </a></h4>
<p>You can load <code class="docutils literal notranslate"><span class="pre">python_function</span></code> models in Python by calling the <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>
function. Note that the <code class="docutils literal notranslate"><span class="pre">load_model</span></code> function assumes that all dependencies are already available
and <em>will not</em> check nor install any dependencies (
see <a class="reference internal" href="#built-in-deployment"><span class="std std-ref">model deployment section</span></a> for tools to deploy models with
automatic dependency management).</p>
<p>Once loaded, you can score the model by calling the <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel.predict" title="mlflow.pyfunc.PyFuncModel.predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">predict</span></code></a>
method, which has the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>predict(data: Union[pandas.(Series | DataFrame), numpy.ndarray, csc_matrix, csr_matrix, List[Any], Dict[str, Any], str],
        params: Optional[Dict[str, Any]] = None) → Union[pandas.(Series | DataFrame), numpy.ndarray, list, str]
</pre></div>
</div>
<p>All PyFunc models will support <cite>pandas.DataFrame</cite> as an input. In addition to <cite>pandas.DataFrame</cite>,
DL PyFunc models will also support tensor inputs in the form of <cite>numpy.ndarrays</cite>. To verify
whether a model flavor supports tensor inputs, please check the flavor’s documentation.</p>
<p>For models with a column-based schema, inputs are typically provided in the form of a <cite>pandas.DataFrame</cite>.
If a dictionary mapping column name to values is provided as input for schemas with named columns or if a
python <cite>List</cite> or a <cite>numpy.ndarray</cite> is provided as input for schemas with unnamed columns, MLflow will cast the
input to a DataFrame. Schema enforcement and casting with respect to the expected data types is performed against
the DataFrame.</p>
<p>For models with a tensor-based schema, inputs are typically provided in the form of a <cite>numpy.ndarray</cite> or a
dictionary mapping the tensor name to its np.ndarray value. Schema enforcement will check the provided input’s
shape and type against the shape and type specified in the model’s schema and throw an error if they do not match.</p>
<p>For models where no schema is defined, no changes to the model inputs and outputs are made. MLflow will
propagate any errors raised by the model if the model does not accept the provided input type.</p>
<p>The python environment that a PyFunc model is loaded into for prediction or inference may differ from the environment
in which it was trained. In the case of an environment mismatch, a warning message will be printed when calling
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This warning statement will identify the packages that have a version mismatch
between those used during training and the current environment.  In order to get the full dependencies of the
environment in which the model was trained, you can call <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.get_model_dependencies" title="mlflow.pyfunc.get_model_dependencies"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.get_model_dependencies()</span></code></a>.
Furthermore, if you want to run model inference in the same environment used in model training, you can call
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf" title="mlflow.pyfunc.spark_udf"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.spark_udf()</span></code></a> with the <cite>env_manager</cite> argument set as “conda”. This will generate the environment
from the <cite>conda.yaml</cite> file, ensuring that the python UDF will execute with the exact package versions that were used
during training.</p>
<p>Some PyFunc models may accept model load configuration, which controls how the model is loaded and predictions
computed. You can learn which configuration the model supports by inspecting the model’s flavor metadata:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get_model_info</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span><span class="p">][</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">MODEL_CONFIG</span><span class="p">]</span>
</pre></div>
</div>
<p>Alternatively, you can load the PyFunc model and inspect the <cite>model_config</cite> property:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pyfunc_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">pyfunc_model</span><span class="o">.</span><span class="n">model_config</span>
</pre></div>
</div>
<p>Model configuration can be changed at loading time by indicating <cite>model_config</cite> parameter in the
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pyfunc_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.93</span><span class="p">))</span>
</pre></div>
</div>
<p>When a model configuration value is changed, those values the configuration the model was saved with. Indicating an
invalid model configuration key for a model results in that configuration being ignored. A warning is displayed mentioning
the ignored entries.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Model configuration vs parameters with default values in signatures:</strong> Use model configuration when you need to provide
model publishers for a way to change how the model is loaded into memory and how predictions are computed for all the
samples. For instance, a key like <cite>user_gpu</cite>. Model consumers are not able to change those values at predict time. Use
parameters with default values in the signature to provide a users the ability to change how predictions are computed on
each data sample.</p>
</div>
</div>
</div>
<div class="section" id="r-function-crate">
<h3><a class="toc-backref" href="#id42">R Function (<code class="docutils literal notranslate"><span class="pre">crate</span></code>)</a><a class="headerlink" href="#r-function-crate" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">crate</span></code> model flavor defines a generic model format for representing an arbitrary R prediction
function as an MLflow model using the <code class="docutils literal notranslate"><span class="pre">crate</span></code> function from the
<a class="reference external" href="https://github.com/r-lib/carrier">carrier</a> package. The prediction function is expected to take a dataframe as input and
produce a dataframe, a vector or a list with the predictions as output.</p>
<p>This flavor requires R to be installed in order to be used.</p>
<div class="section" id="crate-usage">
<h4><code class="docutils literal notranslate"><span class="pre">crate</span></code> usage<a class="headerlink" href="#crate-usage" title="Permalink to this headline"> </a></h4>
<p>For a minimal crate model, an example configuration for the predict function is:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">mlflow</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">carrier</span><span class="p">)</span>
<span class="c1"># Load iris dataset</span>
<span class="nf">data</span><span class="p">(</span><span class="s">&quot;iris&quot;</span><span class="p">)</span>

<span class="c1"># Learn simple linear regression model</span>
<span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Sepal.Width</span><span class="o">~</span><span class="n">Sepal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris</span><span class="p">)</span>

<span class="c1"># Define a crate model</span>
<span class="c1"># call package functions with an explicit :: namespace.</span>
<span class="n">crate_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">crate</span><span class="p">(</span>
<span class="w">  </span><span class="nf">function</span><span class="p">(</span><span class="n">new_obs</span><span class="p">)</span><span class="w">  </span><span class="n">stats</span><span class="o">::</span><span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="s">&quot;Sepal.Length&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_obs</span><span class="p">)),</span>
<span class="w">  </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span>
<span class="p">)</span>

<span class="c1"># log the model</span>
<span class="n">model_path</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mlflow_log_model</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crate_model</span><span class="p">,</span><span class="w"> </span><span class="n">artifact_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;iris_prediction&quot;</span><span class="p">)</span>

<span class="c1"># load the logged model and make a prediction</span>
<span class="n">model_uri</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">paste0</span><span class="p">(</span><span class="nf">mlflow_get_run</span><span class="p">()</span><span class="o">$</span><span class="n">artifact_uri</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;/iris_prediction&quot;</span><span class="p">)</span>
<span class="n">mlflow_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mlflow_load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_uri</span><span class="p">,</span>
<span class="w">                                  </span><span class="n">flavor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span>
<span class="w">                                  </span><span class="n">client</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mlflow_client</span><span class="p">())</span>

<span class="n">prediction</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mlflow_predict</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mlflow_model</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="h2o-h2o">
<h3><a class="toc-backref" href="#id43">H<sub>2</sub>O (<code class="docutils literal notranslate"><span class="pre">h2o</span></code>)</a><a class="headerlink" href="#h2o-h2o" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">h2o</span></code> model flavor enables logging and loading H2O models.</p>
<p>The <a class="reference internal" href="python_api/mlflow.h2o.html#module-mlflow.h2o" title="mlflow.h2o"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.h2o</span></code></a> module defines <a class="reference internal" href="python_api/mlflow.h2o.html#mlflow.h2o.save_model" title="mlflow.h2o.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model()</span></code></a> and
<a class="reference internal" href="python_api/mlflow.h2o.html#mlflow.h2o.log_model" title="mlflow.h2o.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model()</span></code></a> methods in python, and
<a class="reference external" href="R-api.html#mlflow-save-model-h2o">mlflow_save_model</a> and
<a class="reference external" href="R-api.html#mlflow-log-model">mlflow_log_model</a> in R for saving H2O models in MLflow Model
format.
These methods produce MLflow Models with the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor, allowing you to load them
as generic Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can be scored with only DataFrame input. When you load
MLflow Models with the <code class="docutils literal notranslate"><span class="pre">h2o</span></code> flavor using <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>,
the <a class="reference external" href="http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init">h2o.init()</a> method is
called. Therefore, the correct version of <code class="docutils literal notranslate"><span class="pre">h2o(-py)</span></code> must be installed in the loader’s
environment. You can customize the arguments given to
<a class="reference external" href="http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.init">h2o.init()</a> by modifying the
<code class="docutils literal notranslate"><span class="pre">init</span></code> entry of the persisted H2O model’s YAML configuration file: <code class="docutils literal notranslate"><span class="pre">model.h2o/h2o.yaml</span></code>.</p>
<p>Finally, you can use the <a class="reference internal" href="python_api/mlflow.h2o.html#mlflow.h2o.load_model" title="mlflow.h2o.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.h2o.load_model()</span></code></a> method to load MLflow Models with the
<code class="docutils literal notranslate"><span class="pre">h2o</span></code> flavor as H2O model objects.</p>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.h2o.html#module-mlflow.h2o" title="mlflow.h2o"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.h2o</span></code></a>.</p>
<div class="section" id="h2o-pyfunc-usage">
<h4>h2o pyfunc usage<a class="headerlink" href="#h2o-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a minimal h2o model, here is an example of the pyfunc predict() method in a classification scenario :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">h2o</span>

<span class="n">h2o</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">h2o.estimators.glm</span> <span class="kn">import</span> <span class="n">H2OGeneralizedLinearEstimator</span>

<span class="c1"># import the prostate data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">h2o</span><span class="o">.</span><span class="n">import_file</span><span class="p">(</span>
    <span class="s2">&quot;http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip&quot;</span>
<span class="p">)</span>

<span class="c1"># convert the columns to factors</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;CAPSULE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;CAPSULE&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">asfactor</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;RACE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;RACE&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">asfactor</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;DCAPS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;DCAPS&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">asfactor</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;DPROS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;DPROS&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">asfactor</span><span class="p">()</span>

<span class="c1"># split the data</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">valid</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">split_frame</span><span class="p">(</span><span class="n">ratios</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>

<span class="c1"># generate a GLM model</span>
<span class="n">glm_classifier</span> <span class="o">=</span> <span class="n">H2OGeneralizedLinearEstimator</span><span class="p">(</span>
    <span class="n">family</span><span class="o">=</span><span class="s2">&quot;binomial&quot;</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nfolds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">compute_p_values</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">glm_classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
        <span class="n">y</span><span class="o">=</span><span class="s2">&quot;CAPSULE&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;AGE&quot;</span><span class="p">,</span> <span class="s2">&quot;RACE&quot;</span><span class="p">,</span> <span class="s2">&quot;VOL&quot;</span><span class="p">,</span> <span class="s2">&quot;GLEASON&quot;</span><span class="p">],</span> <span class="n">training_frame</span><span class="o">=</span><span class="n">train</span>
    <span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">glm_classifier</span><span class="o">.</span><span class="n">model_performance</span><span class="p">()</span>
    <span class="n">metrics_to_track</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="s2">&quot;r2&quot;</span><span class="p">,</span> <span class="s2">&quot;logloss&quot;</span><span class="p">]</span>
    <span class="n">metrics_to_log</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">value</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">_metric_json</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">metrics_to_track</span>
    <span class="p">}</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">glm_classifier</span><span class="o">.</span><span class="n">params</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics_to_log</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">h2o</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">glm_classifier</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;h2o_model_info&quot;</span><span class="p">)</span>

<span class="c1"># load h2o model and make a prediction</span>
<span class="n">h2o_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">as_data_frame</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">h2o_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

<span class="c1"># it is also possible to load the model and predict using h2o methods on the h2o frame</span>

<span class="c1"># h2o_model = mlflow.h2o.load_model(model_info.model_uri)</span>
<span class="c1"># predictions = h2o_model.predict(test)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="keras-keras">
<span id="tf-keras-example"></span><h3><a class="toc-backref" href="#id44">Keras (<code class="docutils literal notranslate"><span class="pre">keras</span></code>)</a><a class="headerlink" href="#keras-keras" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">keras</span></code> model flavor enables logging and loading Keras models. It is available in both Python
and R clients. In R, you can save or log the model using
<code class="docutils literal notranslate"><span class="pre">mlflow_save_model</span></code> and <code class="docutils literal notranslate"><span class="pre">mlflow_log_model</span></code>.
These functions serialize Keras models as HDF5 files using the Keras library’s built-in
model persistence functions. You can use
<code class="docutils literal notranslate"><span class="pre">mlflow_load_model</span></code> function in R to load MLflow Models
with the <code class="docutils literal notranslate"><span class="pre">keras</span></code> flavor as <a class="reference external" href="https://keras.io/models/about-keras-models/">Keras Model objects</a>.</p>
<div class="section" id="keras-pyfunc-usage">
<h4>Keras pyfunc usage<a class="headerlink" href="#keras-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a minimal Sequential model, an example configuration for the pyfunc predict() method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">local_artifact_dir</span> <span class="o">=</span> <span class="s2">&quot;/tmp/mlflow/keras_model&quot;</span>
<span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">local_artifact_dir</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">mlflow</span><span class="o">.</span><span class="n">last_active_run</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/model&quot;</span>
<span class="n">keras_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">dst_path</span><span class="o">=</span><span class="n">local_artifact_dir</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">keras_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">local_artifact_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="mleap-mleap">
<h3><a class="toc-backref" href="#id45">MLeap (<code class="docutils literal notranslate"><span class="pre">mleap</span></code>)</a><a class="headerlink" href="#mleap-mleap" title="Permalink to this headline"> </a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mleap</span></code> model flavor is deprecated as of MLflow 2.6.0 and will be removed in a future release.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">mleap</span></code> model flavor supports saving Spark models in MLflow format using the
<a class="reference external" href="https://combust.github.io/mleap-docs/">MLeap</a> persistence mechanism. MLeap is an inference-optimized
format and execution engine for Spark models that does not depend on
<a class="reference external" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext">SparkContext</a>
to evaluate inputs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can save Spark models in MLflow format with the <code class="docutils literal notranslate"><span class="pre">mleap</span></code> flavor by specifying the
<code class="docutils literal notranslate"><span class="pre">sample_input</span></code> argument of the <a class="reference internal" href="python_api/mlflow.spark.html#mlflow.spark.save_model" title="mlflow.spark.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spark.save_model()</span></code></a> or
<a class="reference internal" href="python_api/mlflow.spark.html#mlflow.spark.log_model" title="mlflow.spark.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spark.log_model()</span></code></a> method (recommended). For more details see <a class="reference internal" href="#model-spark"><span class="std std-ref">Spark MLlib</span></a>.</p>
</div>
<p>The <a class="reference internal" href="python_api/mlflow.mleap.html#module-mlflow.mleap" title="mlflow.mleap"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.mleap</span></code></a> module also
defines <a class="reference internal" href="python_api/mlflow.mleap.html#mlflow.mleap.save_model" title="mlflow.mleap.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model()</span></code></a> and
<a class="reference internal" href="python_api/mlflow.mleap.html#mlflow.mleap.log_model" title="mlflow.mleap.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model()</span></code></a> methods for saving MLeap models in MLflow format,
but these methods do not include the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor in the models they produce.
Similarly, <code class="docutils literal notranslate"><span class="pre">mleap</span></code> models can be saved in R with <code class="docutils literal notranslate"><span class="pre">mlflow_save_model</span></code> and loaded with <code class="docutils literal notranslate"><span class="pre">mlflow_load_model</span></code>, with
<code class="docutils literal notranslate"><span class="pre">mlflow_save_model</span></code> requiring <cite>sample_input</cite> to be specified as a
sample Spark dataframe containing input data to the model is required by MLeap for data schema
inference.</p>
<p>A companion module for loading MLflow Models with the MLeap flavor is available in the
<code class="docutils literal notranslate"><span class="pre">mlflow/java</span></code> package.</p>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.spark.html#module-mlflow.spark" title="mlflow.spark"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.spark</span></code></a>, <a class="reference internal" href="python_api/mlflow.mleap.html#module-mlflow.mleap" title="mlflow.mleap"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.mleap</span></code></a>, and the
<a class="reference external" href="https://combust.github.io/mleap-docs/">MLeap documentation</a>.</p>
</div>
<div class="section" id="pytorch-pytorch">
<h3><a class="toc-backref" href="#id46">PyTorch (<code class="docutils literal notranslate"><span class="pre">pytorch</span></code>)</a><a class="headerlink" href="#pytorch-pytorch" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> model flavor enables logging and loading PyTorch models.</p>
<p>The <a class="reference internal" href="python_api/mlflow.pytorch.html#module-mlflow.pytorch" title="mlflow.pytorch"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pytorch</span></code></a> module defines utilities for saving and loading MLflow Models with the
<code class="docutils literal notranslate"><span class="pre">pytorch</span></code> flavor. You can use the <a class="reference internal" href="python_api/mlflow.pytorch.html#mlflow.pytorch.save_model" title="mlflow.pytorch.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pytorch.save_model()</span></code></a> and
<a class="reference internal" href="python_api/mlflow.pytorch.html#mlflow.pytorch.log_model" title="mlflow.pytorch.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pytorch.log_model()</span></code></a> methods to save PyTorch models in MLflow format; both of these
functions use the <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.save">torch.save()</a> method to
serialize PyTorch models. Additionally, you can use the <a class="reference internal" href="python_api/mlflow.pytorch.html#mlflow.pytorch.load_model" title="mlflow.pytorch.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pytorch.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> flavor as PyTorch model objects. This loaded
PyFunc model can be scored with both DataFrame input and numpy array input. Finally, models
produced by <a class="reference internal" href="python_api/mlflow.pytorch.html#mlflow.pytorch.save_model" title="mlflow.pytorch.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pytorch.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.pytorch.html#mlflow.pytorch.log_model" title="mlflow.pytorch.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pytorch.log_model()</span></code></a> contain
the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor, allowing you to load them as generic Python functions for inference
via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the PyTorch flavor, if a GPU is available at prediction time, the default GPU will be used to run
inference. To disable this behavior, users can use the
<a class="reference external" href="python_api/mlflow.environment_variables.html#mlflow.environment_variables.MLFLOW_DEFAULT_PREDICTION_DEVICE">MLFLOW_DEFAULT_PREDICTION_DEVICE</a>
or pass in a device with the <cite>device</cite> parameter for the <cite>predict</cite> function.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In case of multi gpu training, ensure to save the model only with global rank 0 gpu. This avoids
logging multiple copies of the same model.</p>
</div>
<div class="section" id="pytorch-pyfunc-usage">
<h4>PyTorch pyfunc usage<a class="headerlink" href="#pytorch-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a minimal PyTorch model, an example configuration for the pyfunc predict() method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>

<span class="n">pytorch_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">pytorch_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.pytorch.html#module-mlflow.pytorch" title="mlflow.pytorch"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pytorch</span></code></a>.</p>
</div>
</div>
<div class="section" id="scikit-learn-sklearn">
<h3><a class="toc-backref" href="#id47">Scikit-learn (<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>)</a><a class="headerlink" href="#scikit-learn-sklearn" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> model flavor provides an easy-to-use interface for saving and loading scikit-learn
models. The <a class="reference internal" href="python_api/mlflow.sklearn.html#module-mlflow.sklearn" title="mlflow.sklearn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sklearn</span></code></a> module defines
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.save_model" title="mlflow.sklearn.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model()</span></code></a> and
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.log_model" title="mlflow.sklearn.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model()</span></code></a> functions that save scikit-learn models in
MLflow format, using either Python’s pickle module (Pickle) or CloudPickle for model serialization.
These functions produce MLflow Models with the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor, allowing them to
be loaded as generic Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can only be scored with DataFrame input. Finally, you can use the
<a class="reference internal" href="python_api/mlflow.sklearn.html#mlflow.sklearn.load_model" title="mlflow.sklearn.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sklearn.load_model()</span></code></a> method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> flavor as
scikit-learn model objects.</p>
<div class="section" id="scikit-learn-pyfunc-usage">
<h4>Scikit-learn pyfunc usage<a class="headerlink" href="#scikit-learn-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a Scikit-learn LogisticRegression model, an example configuration for the pyfunc predict() method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">sk_model</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span>

<span class="n">sklearn_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">sklearn_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.sklearn.html#module-mlflow.sklearn" title="mlflow.sklearn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sklearn</span></code></a>.</p>
</div>
</div>
<div class="section" id="spark-mllib-spark">
<span id="model-spark"></span><h3><a class="toc-backref" href="#id48">Spark MLlib (<code class="docutils literal notranslate"><span class="pre">spark</span></code>)</a><a class="headerlink" href="#spark-mllib-spark" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">spark</span></code> model flavor enables exporting Spark MLlib models as MLflow Models.</p>
<p>The <a class="reference internal" href="python_api/mlflow.spark.html#module-mlflow.spark" title="mlflow.spark"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.spark</span></code></a> module defines</p>
<ul class="simple">
<li><p><a class="reference internal" href="python_api/mlflow.spark.html#mlflow.spark.save_model" title="mlflow.spark.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model()</span></code></a> to save a Spark MLlib model to a DBFS path.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.spark.html#mlflow.spark.log_model" title="mlflow.spark.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model()</span></code></a> to upload a Spark MLlib model to the tracking server.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.spark.html#mlflow.spark.load_model" title="mlflow.spark.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spark.load_model()</span></code></a> to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">spark</span></code> flavor as Spark MLlib pipelines.</p></li>
</ul>
<p>MLflow Models produced by these functions contain the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor,
allowing you to load them as generic Python functions via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can only be scored with DataFrame input.
When a model with the <code class="docutils literal notranslate"><span class="pre">spark</span></code> flavor is loaded as a Python function via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>, a new
<a class="reference external" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext">SparkContext</a>
is created for model inference; additionally, the function converts all Pandas DataFrame inputs to
Spark DataFrames before scoring. While this initialization overhead and format translation latency
is not ideal for high-performance use cases, it enables you to easily deploy any
<a class="reference external" href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=pipelinemodel#pyspark.ml.Pipeline">MLlib PipelineModel</a> to any production environment supported by MLflow
(SageMaker, AzureML, etc).</p>
<div class="section" id="spark-mllib-pyfunc-usage">
<h4>Spark MLlib pyfunc usage<a class="headerlink" href="#spark-mllib-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.classification</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.linalg</span> <span class="kn">import</span> <span class="n">Vectors</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Prepare training data from a list of (label, features) tuples.</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;LogisticRegressionExample&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">training</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])),</span>
        <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])),</span>
        <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])),</span>
        <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])),</span>
    <span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;features&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Create and fit a LogisticRegression instance</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">maxIter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">regParam</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lr_model</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>

<span class="c1"># Serialize the Model</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="s2">&quot;spark-model&quot;</span><span class="p">)</span>

<span class="c1"># Load saved model</span>
<span class="n">lr_model_saved</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Make predictions on test data.</span>
<span class="c1"># The DataFrame used in the predict method must be a Pandas DataFrame</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">])),</span>
        <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">])),</span>
        <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">])),</span>
    <span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;features&quot;</span><span class="p">],</span>
<span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">lr_model_saved</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that when the <code class="docutils literal notranslate"><span class="pre">sample_input</span></code> parameter is provided to <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> or
<code class="docutils literal notranslate"><span class="pre">save_model()</span></code>, the Spark model is automatically saved as an <code class="docutils literal notranslate"><span class="pre">mleap</span></code> flavor
by invoking <a class="reference internal" href="python_api/mlflow.mleap.html#mlflow.mleap.add_to_model" title="mlflow.mleap.add_to_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.mleap.add_to_model()</span></code></a>.</p>
<p>For example, the follow code block:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">training_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a b c d e spark&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b d&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;spark f g h&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;hadoop mapreduce&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="p">],</span> <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;words&quot;</span><span class="p">)</span>
<span class="n">hashingTF</span> <span class="o">=</span> <span class="n">HashingTF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">getOutputCol</span><span class="p">(),</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">maxIter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">regParam</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">hashingTF</span><span class="p">,</span> <span class="n">lr</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_df</span><span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;spark-model&quot;</span><span class="p">,</span> <span class="n">sample_input</span><span class="o">=</span><span class="n">training_df</span><span class="p">)</span>
</pre></div>
</div>
<p>results in the following directory structure logged to the MLflow Experiment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Directory written by with the addition of mlflow.mleap.add_to_model(model, &quot;spark-model&quot;, training_df)
# Note the addition of the mleap directory
spark-model/
├── mleap
├── sparkml
├── MLmodel
├── conda.yaml
├── python_env.yaml
└── requirements.txt
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.mleap.html#module-mlflow.mleap" title="mlflow.mleap"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.mleap</span></code></a>.</p>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.spark.html#module-mlflow.spark" title="mlflow.spark"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.spark</span></code></a>.</p>
</div>
</div>
<div class="section" id="tensorflow-tensorflow">
<h3><a class="toc-backref" href="#id49">TensorFlow (<code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>)</a><a class="headerlink" href="#tensorflow-tensorflow" title="Permalink to this headline"> </a></h3>
<p>The simple example below shows how to log params and metrics in mlflow for a custom training loop
using low-level TensorFlow API. See <a class="reference internal" href="#tf-keras-example">tf-keras-example</a>. for an example of mlflow and <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">10</span>

<span class="c1"># estimate w and b where y = w * x + b</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># initial values</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="c1"># calculate MSE = 0.5 * (y_predict - y_train)^2</span>
            <span class="n">y_predict</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x_train</span> <span class="o">+</span> <span class="n">b</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_predict</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">))</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

        <span class="c1"># Update the trainable variables</span>
        <span class="c1"># w = w - learning_rate * gradient of loss function w.r.t. w</span>
        <span class="c1"># b = b - learning_rate * gradient of loss function w.r.t. b</span>
        <span class="n">w</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
        <span class="n">b</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W = </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="onnx-onnx">
<h3><a class="toc-backref" href="#id50">ONNX (<code class="docutils literal notranslate"><span class="pre">onnx</span></code>)</a><a class="headerlink" href="#onnx-onnx" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">onnx</span></code> model flavor enables logging of <a class="reference external" href="http://onnx.ai/">ONNX models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.onnx.html#mlflow.onnx.save_model" title="mlflow.onnx.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.onnx.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.onnx.html#mlflow.onnx.log_model" title="mlflow.onnx.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.onnx.log_model()</span></code></a> methods. These
methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can be scored with
both DataFrame input and numpy array input. The <code class="docutils literal notranslate"><span class="pre">python_function</span></code> representation of an MLflow
ONNX model uses the <a class="reference external" href="https://github.com/microsoft/onnxruntime">ONNX Runtime execution engine</a> for
evaluation. Finally, you can use the <a class="reference internal" href="python_api/mlflow.onnx.html#mlflow.onnx.load_model" title="mlflow.onnx.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.onnx.load_model()</span></code></a> method to load MLflow
Models with the <code class="docutils literal notranslate"><span class="pre">onnx</span></code> flavor in native ONNX format.</p>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.onnx.html#module-mlflow.onnx" title="mlflow.onnx"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.onnx</span></code></a> and <a class="reference external" href="http://onnx.ai/">http://onnx.ai/</a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The default behavior for saving ONNX files is to use the ONNX save option <code class="docutils literal notranslate"><span class="pre">save_as_external_data=True</span></code>
in order to support model files that are <strong>in excess of 2GB</strong>. For edge deployments of small model files, this
may create issues. If you need to save a small model as a single file for such deployment considerations,
you can set the parameter <code class="docutils literal notranslate"><span class="pre">save_as_external_data=False</span></code> in either <a class="reference internal" href="python_api/mlflow.onnx.html#mlflow.onnx.save_model" title="mlflow.onnx.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.onnx.save_model()</span></code></a> or
<a class="reference internal" href="python_api/mlflow.onnx.html#mlflow.onnx.log_model" title="mlflow.onnx.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.onnx.log_model()</span></code></a> to force the serialization of the model as a small file. Note that if the
model is in excess of 2GB, <strong>saving as a single file will not work</strong>.</p>
</div>
<div class="section" id="onnx-pyfunc-usage-example">
<h4>ONNX pyfunc usage example<a class="headerlink" href="#onnx-pyfunc-usage-example" title="Permalink to this headline"> </a></h4>
<p>For an ONNX model, an example configuration that uses pytorch to train a dummy model,
converts it to ONNX, logs to mlflow and makes a prediction using pyfunc predict() method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># define a torch model</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># run model training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># convert model to ONNX and load it</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>

<span class="c1"># log the model into a mlflow run</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>

<span class="c1"># load the logged model and make a prediction</span>
<span class="n">onnx_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">onnx_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="mxnet-gluon-gluon">
<h3><a class="toc-backref" href="#id51">MXNet Gluon (<code class="docutils literal notranslate"><span class="pre">gluon</span></code>)</a><a class="headerlink" href="#mxnet-gluon-gluon" title="Permalink to this headline"> </a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">gluon</span></code> model flavor is deprecated and will be removed in a future release.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">gluon</span></code> model flavor enables logging of <a class="reference external" href="https://mxnet.incubator.apache.org/api/python/docs/api/gluon/index.html">Gluon models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.gluon.html#mlflow.gluon.save_model" title="mlflow.gluon.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.gluon.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.gluon.html#mlflow.gluon.log_model" title="mlflow.gluon.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.gluon.log_model()</span></code></a> methods. These
methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can be scored with
both DataFrame input and numpy array input. You can also use the <a class="reference internal" href="python_api/mlflow.gluon.html#mlflow.gluon.load_model" title="mlflow.gluon.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.gluon.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">gluon</span></code> flavor in native Gluon format.</p>
<div class="section" id="gluon-pyfunc-usage">
<h4>Gluon pyfunc usage<a class="headerlink" href="#gluon-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a minimal gluon model, here is an example of the pyfunc predict() method with a logistic regression model :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="k">as</span> <span class="nn">mx</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span><span class="p">,</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">gluon</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">ArrayDataset</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># this example requires a compatible version of numpy : numpy == 1.23.1</span>
<span class="c1"># `pip uninstall numpy`  `python -m pip install numpy==1.23.1`</span>


<span class="k">def</span> <span class="nf">get_random_data</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


<span class="c1"># use cpu for this example, gpu could be used with ctx=gpu()</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">train_data_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">val_data_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">train_x</span><span class="p">,</span> <span class="n">train_ground_truth_class</span> <span class="o">=</span> <span class="n">get_random_data</span><span class="p">(</span><span class="n">train_data_size</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ArrayDataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_ground_truth_class</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">val_x</span><span class="p">,</span> <span class="n">val_ground_truth_class</span> <span class="o">=</span> <span class="n">get_random_data</span><span class="p">(</span><span class="n">val_data_size</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">ArrayDataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">val_ground_truth_class</span><span class="p">)</span>
<span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HybridSequential</span><span class="p">()</span>

<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>  <span class="c1"># input layer</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>  <span class="c1"># inner layer 1</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>  <span class="c1"># inner layer 2</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># output layer: must have only 1 neuron</span>

<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">())</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">SigmoidBinaryCrossEntropyLoss</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">,</span>
    <span class="n">optimizer_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">F1</span><span class="p">()</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span>


<span class="k">def</span> <span class="nf">train_model</span><span class="p">():</span>
    <span class="n">cumulative_train_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
            <span class="c1"># do forward pass on a batch of training data</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="c1"># calculate loss for the training data batch</span>
            <span class="n">loss_result</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="c1"># calculate gradients</span>
        <span class="n">loss_result</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># update parameters of the network</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="c1"># sum losses of every batch</span>
        <span class="n">cumulative_train_loss</span> <span class="o">+=</span> <span class="n">nd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">cumulative_train_loss</span>


<span class="k">def</span> <span class="nf">validate_model</span><span class="p">(</span><span class="n">threshold</span><span class="p">):</span>
    <span class="n">cumulative_val_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">val_data</span><span class="p">,</span> <span class="n">val_ground_truth_class</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">):</span>
        <span class="c1"># do forward pass on a batch of validation data</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span>
        <span class="c1"># calculate cumulative validation loss</span>
        <span class="n">cumulative_val_loss</span> <span class="o">+=</span> <span class="n">nd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">val_ground_truth_class</span><span class="p">))</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>
        <span class="c1"># prediction as a sigmoid</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
        <span class="c1"># converting neuron outputs to classes</span>
        <span class="n">predicted_classes</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="c1"># update validation accuracy</span>
        <span class="n">accuracy</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">val_ground_truth_class</span><span class="p">,</span> <span class="n">predicted_classes</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># calculate probabilities of belonging to different classes</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">f1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">val_ground_truth_class</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cumulative_val_loss</span>


<span class="c1"># train model and get metrics</span>
<span class="n">cumulative_train_loss</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">()</span>
<span class="n">cumulative_val_loss</span> <span class="o">=</span> <span class="n">validate_model</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">()</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">metrics_to_log</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;training_loss&quot;</span><span class="p">:</span> <span class="n">cumulative_train_loss</span><span class="p">,</span>
    <span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="n">cumulative_val_loss</span><span class="p">,</span>
    <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="n">f1</span><span class="o">.</span><span class="n">get</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>
    <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">get</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">params_to_log</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">trainer</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">}</span>

<span class="c1"># the model needs to be hybridized and run forward at least once before export is called</span>
<span class="n">net</span><span class="o">.</span><span class="n">hybridize</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">params_to_log</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics_to_log</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>

<span class="c1"># load the model</span>
<span class="n">pytorch_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># make a prediction</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pytorch_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.gluon.html#module-mlflow.gluon" title="mlflow.gluon"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.gluon</span></code></a>.</p>
</div>
</div>
<div class="section" id="xgboost-xgboost">
<h3><a class="toc-backref" href="#id52">XGBoost (<code class="docutils literal notranslate"><span class="pre">xgboost</span></code>)</a><a class="headerlink" href="#xgboost-xgboost" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">xgboost</span></code> model flavor enables logging of <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster">XGBoost models</a>
in MLflow format via the <a class="reference internal" href="python_api/mlflow.xgboost.html#mlflow.xgboost.save_model" title="mlflow.xgboost.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.xgboost.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.xgboost.html#mlflow.xgboost.log_model" title="mlflow.xgboost.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.xgboost.log_model()</span></code></a> methods in python and <a class="reference external" href="R-api.html#mlflow-save-model-crate">mlflow_save_model</a> and <a class="reference external" href="R-api.html#mlflow-log-model">mlflow_log_model</a> in R respectively.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can only be scored with DataFrame input.
You can also use the <a class="reference internal" href="python_api/mlflow.xgboost.html#mlflow.xgboost.load_model" title="mlflow.xgboost.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.xgboost.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">xgboost</span></code> model flavor in native XGBoost format.</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">xgboost</span></code> model flavor only supports an instance of <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster">xgboost.Booster</a>,
not models that implement the <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn">scikit-learn API</a>.</p>
<div class="section" id="xgboost-pyfunc-usage">
<h4><code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> pyfunc usage<a class="headerlink" href="#xgboost-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>The example below</p>
<ul class="simple">
<li><p>Loads the IRIS dataset from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></p></li>
<li><p>Trains an XGBoost Classifier</p></li>
<li><p>Logs the model and params using <code class="docutils literal notranslate"><span class="pre">mlflow</span></code></p></li>
<li><p>Loads the logged model and makes predictions</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="n">xgb_classifier</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary:logistic&quot;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># log fitted model and XGBClassifier parameters</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">clf_params</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">get_xgb_params</span><span class="p">()</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">clf_params</span><span class="p">)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">xgboost</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">xgb_classifier</span><span class="p">,</span> <span class="s2">&quot;iris-classifier&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span>

<span class="c1"># Load saved model and make predictions</span>
<span class="n">xgb_classifier_saved</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_classifier_saved</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.xgboost.html#module-mlflow.xgboost" title="mlflow.xgboost"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.xgboost</span></code></a>.</p>
</div>
</div>
<div class="section" id="lightgbm-lightgbm">
<h3><a class="toc-backref" href="#id53">LightGBM (<code class="docutils literal notranslate"><span class="pre">lightgbm</span></code>)</a><a class="headerlink" href="#lightgbm-lightgbm" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">lightgbm</span></code> model flavor enables logging of <a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm-booster">LightGBM models</a>
in MLflow format via the <a class="reference internal" href="python_api/mlflow.lightgbm.html#mlflow.lightgbm.save_model" title="mlflow.lightgbm.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.lightgbm.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.lightgbm.html#mlflow.lightgbm.log_model" title="mlflow.lightgbm.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.lightgbm.log_model()</span></code></a> methods.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. You can also use the <a class="reference internal" href="python_api/mlflow.lightgbm.html#mlflow.lightgbm.load_model" title="mlflow.lightgbm.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.lightgbm.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">lightgbm</span></code> model flavor in native LightGBM format.</p>
<p>Note that the scikit-learn API for LightGBM is now supported. For more information, see <a class="reference internal" href="python_api/mlflow.lightgbm.html#module-mlflow.lightgbm" title="mlflow.lightgbm"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.lightgbm</span></code></a>.</p>
<div class="section" id="lightgbm-pyfunc-usage">
<h4><code class="docutils literal notranslate"><span class="pre">LightGBM</span></code> pyfunc usage<a class="headerlink" href="#lightgbm-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>The example below</p>
<ul class="simple">
<li><p>Loads the IRIS dataset from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></p></li>
<li><p>Trains a LightGBM <code class="docutils literal notranslate"><span class="pre">LGBMClassifier</span></code></p></li>
<li><p>Logs the model and feature importance’s using <code class="docutils literal notranslate"><span class="pre">mlflow</span></code></p></li>
<li><p>Loads the logged model and makes predictions</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Remove special characters from feature names to be able to use them as keys for mlflow metrics</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;(&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;)&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;feature_names&quot;</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="c1"># create model instance</span>
<span class="n">lgb_classifier</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary:logistic&quot;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Fit and save model and LGBMClassifier feature importances as mlflow metrics</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">lgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">feature_importances</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">lgb_classifier</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">))</span>
    <span class="n">feature_importance_metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sa">f</span><span class="s2">&quot;feature_importance_</span><span class="si">{</span><span class="n">feature_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">imp_value</span>
        <span class="k">for</span> <span class="n">feature_name</span><span class="p">,</span> <span class="n">imp_value</span> <span class="ow">in</span> <span class="n">feature_importances</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">feature_importance_metrics</span><span class="p">)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">lgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">lightgbm</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">lgb_classifier</span><span class="p">,</span> <span class="s2">&quot;iris-classifier&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span>

<span class="c1"># Load saved model and make predictions</span>
<span class="n">lgb_classifier_saved</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lgb_classifier_saved</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="catboost-catboost">
<h3><a class="toc-backref" href="#id54">CatBoost (<code class="docutils literal notranslate"><span class="pre">catboost</span></code>)</a><a class="headerlink" href="#catboost-catboost" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">catboost</span></code> model flavor enables logging of <a class="reference external" href="https://catboost.ai/docs/concepts/python-reference_catboost.html">CatBoost models</a>
in MLflow format via the <a class="reference internal" href="python_api/mlflow.catboost.html#mlflow.catboost.save_model" title="mlflow.catboost.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.catboost.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.catboost.html#mlflow.catboost.log_model" title="mlflow.catboost.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.catboost.log_model()</span></code></a> methods.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. You can also use the <a class="reference internal" href="python_api/mlflow.catboost.html#mlflow.catboost.load_model" title="mlflow.catboost.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.catboost.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">catboost</span></code> model flavor in native CatBoost format.</p>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.catboost.html#module-mlflow.catboost" title="mlflow.catboost"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.catboost</span></code></a>.</p>
<div class="section" id="catboost-pyfunc-usage">
<h4><code class="docutils literal notranslate"><span class="pre">CatBoost</span></code> pyfunc usage<a class="headerlink" href="#catboost-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>For a CatBoost Classifier model, an example configuration for the pyfunc predict() method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># prepare data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_wine</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s2">&quot;MultiClass&quot;</span><span class="p">,</span>
    <span class="n">allow_writing_files</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># create model signature</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># log the model into a mlflow run</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">catboost</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>

<span class="c1"># load the logged model and make a prediction</span>
<span class="n">catboost_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">catboost_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="spacy-spacy">
<h3><a class="toc-backref" href="#id55">Spacy(<code class="docutils literal notranslate"><span class="pre">spaCy</span></code>)</a><a class="headerlink" href="#spacy-spacy" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> model flavor enables logging of <a class="reference external" href="https://spacy.io/models">spaCy models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.spacy.html#mlflow.spacy.save_model" title="mlflow.spacy.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spacy.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.spacy.html#mlflow.spacy.log_model" title="mlflow.spacy.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spacy.log_model()</span></code></a> methods. Additionally, these
methods add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the models to be
interpreted as generic Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can only be scored with DataFrame input. You can
also use the <a class="reference internal" href="python_api/mlflow.spacy.html#mlflow.spacy.load_model" title="mlflow.spacy.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.spacy.load_model()</span></code></a> method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">spacy</span></code> model flavor
in native spaCy format.</p>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.spacy.html#module-mlflow.spacy" title="mlflow.spacy"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.spacy</span></code></a>.</p>
<div class="section" id="spacy-pyfunc-usage">
<h4><code class="docutils literal notranslate"><span class="pre">Spacy</span></code> pyfunc usage<a class="headerlink" href="#spacy-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>The example below shows how to train a <code class="docutils literal notranslate"><span class="pre">Spacy</span></code> <code class="docutils literal notranslate"><span class="pre">TextCategorizer</span></code> model, log the model artifact and metrics to the
mlflow tracking server and then load the saved model to make predictions. For this example, we will be using the
<code class="docutils literal notranslate"><span class="pre">Polarity</span> <span class="pre">2.0</span></code> dataset available in the <code class="docutils literal notranslate"><span class="pre">nltk</span></code> package. This dataset consists of 10000 positive and 10000 negative
short movie reviews.</p>
<p>First we convert the texts and sentiment labels (“pos” or “neg”) from NLTK native format to <code class="docutils literal notranslate"><span class="pre">Spacy</span></code>’s <code class="docutils literal notranslate"><span class="pre">DocBin</span></code> format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">movie_reviews</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">Language</span>
<span class="kn">from</span> <span class="nn">spacy.tokens</span> <span class="kn">import</span> <span class="n">DocBin</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;movie_reviews&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_sentences</span><span class="p">(</span><span class="n">sentiment_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reconstruct the sentences from the word lists for each review record for a specific ``sentiment_type``</span>
<span class="sd">    as a pandas DataFrame with two columns: &#39;sentence&#39; and &#39;sentiment&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">file_ids</span> <span class="o">=</span> <span class="n">movie_reviews</span><span class="o">.</span><span class="n">fileids</span><span class="p">(</span><span class="n">sentiment_type</span><span class="p">)</span>
    <span class="n">sent_df</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="n">file_ids</span><span class="p">:</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">movie_reviews</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">file_id</span><span class="p">))</span>
        <span class="n">sent_df</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="n">sentence</span><span class="p">,</span> <span class="s2">&quot;sentiment&quot;</span><span class="p">:</span> <span class="n">sentiment_type</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sent_df</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">data_df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">target_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a DataFrame with &#39;sentence&#39; and &#39;sentiment&#39; columns to a</span>
<span class="sd">    spacy DocBin object and save it to &#39;target_file&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">blank</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">)</span>
    <span class="n">sentiment_labels</span> <span class="o">=</span> <span class="n">data_df</span><span class="o">.</span><span class="n">sentiment</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
    <span class="n">spacy_doc</span> <span class="o">=</span> <span class="n">DocBin</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">sent_tokens</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;sentence&quot;</span><span class="p">])</span>
        <span class="c1"># To train a Spacy TextCategorizer model, the label must be attached to the &quot;cats&quot; dictionary of the &quot;Doc&quot;</span>
        <span class="c1"># object, e.g. {&quot;pos&quot;: 1.0, &quot;neg&quot;: 0.0} for a &quot;pos&quot; label.</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">sentiment_labels</span><span class="p">:</span>
            <span class="n">sent_tokens</span><span class="o">.</span><span class="n">cats</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;sentiment&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="n">spacy_doc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">sent_tokens</span><span class="p">)</span>

    <span class="n">spacy_doc</span><span class="o">.</span><span class="n">to_disk</span><span class="p">(</span><span class="n">target_file</span><span class="p">)</span>


<span class="c1"># Build a single DataFrame with both positive and negative reviews, one row per review</span>
<span class="n">review_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_sentences</span><span class="p">(</span><span class="n">sentiment_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentiment_type</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="s2">&quot;neg&quot;</span><span class="p">)]</span>
<span class="n">review_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">review_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Split the DataFrame into a train and a dev set</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">review_data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;sentiment&quot;</span><span class="p">,</span> <span class="n">group_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">dev_df</span> <span class="o">=</span> <span class="n">review_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">review_data</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">train_df</span><span class="o">.</span><span class="n">index</span><span class="p">),</span> <span class="p">:]</span>

<span class="c1"># Save the train and dev data files to the current directory as &quot;corpora.train&quot; and &quot;corpora.dev&quot;, respectively</span>
<span class="n">convert</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="s2">&quot;corpora.train&quot;</span><span class="p">)</span>
<span class="n">convert</span><span class="p">(</span><span class="n">dev_df</span><span class="p">,</span> <span class="s2">&quot;corpora.dev&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To set up the training job, we first need to generate a configuration file as described in the <a class="reference external" href="https://spacy.io/usage/training#config">Spacy Documentation</a>
For simplicity, we will only use a <code class="docutils literal notranslate"><span class="pre">TextCategorizer</span></code> in the pipeline.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python -m spacy init config --pipeline textcat --lang en mlflow-textcat.cfg</span>
</pre></div>
</div>
<p>Change the default train and dev paths in the config file to the current directory:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span> [paths]
<span class="gd">- train = null</span>
<span class="gd">- dev = null</span>
<span class="gi">+ train = &quot;.&quot;</span>
<span class="gi">+ dev = &quot;.&quot;</span>
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">Spacy</span></code>, the training loop is defined internally in Spacy’s code. Spacy provides a “logging” extension point where
we can use <code class="docutils literal notranslate"><span class="pre">mlflow</span></code>. To do this,</p>
<ul class="simple">
<li><p>We have to define a function to write metrics / model input to <code class="docutils literal notranslate"><span class="pre">mlfow</span></code></p></li>
<li><p>Register it as a logger in <code class="docutils literal notranslate"><span class="pre">Spacy</span></code>’s component registry</p></li>
<li><p>Change the default console logger in the <code class="docutils literal notranslate"><span class="pre">Spacy</span></code>’s configuration file (<code class="docutils literal notranslate"><span class="pre">mlflow-textcat.cfg</span></code>)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">IO</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">Language</span>
<span class="kn">import</span> <span class="nn">mlflow</span>


<span class="nd">@spacy</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">loggers</span><span class="p">(</span><span class="s2">&quot;mlflow_logger.v1&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">mlflow_logger</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a function, ``setup_logger`` that returns two functions:</span>

<span class="sd">    * ``log_step`` is called internally by Spacy for every evaluation step. We can log the intermediate train and</span>
<span class="sd">    validation scores to the mlflow tracking server here.</span>
<span class="sd">    * ``finalize``: is called internally by Spacy after training is complete. We can log the model artifact to the</span>
<span class="sd">    mlflow tracking server here.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">setup_logger</span><span class="p">(</span>
        <span class="n">nlp</span><span class="p">:</span> <span class="n">Language</span><span class="p">,</span>
        <span class="n">stdout</span><span class="p">:</span> <span class="n">IO</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span>
        <span class="n">stderr</span><span class="p">:</span> <span class="n">IO</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
        <span class="k">def</span> <span class="nf">log_step</span><span class="p">(</span><span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
            <span class="k">if</span> <span class="n">info</span><span class="p">:</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>
                <span class="n">score</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

                <span class="k">for</span> <span class="n">pipe_name</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe_names</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;losses&quot;</span><span class="p">][</span><span class="n">pipe_name</span><span class="p">]</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pipe_name</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pipe_name</span><span class="si">}</span><span class="s2">_score&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">finalize</span><span class="p">():</span>
            <span class="n">uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">spacy</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">nlp</span><span class="p">,</span> <span class="s2">&quot;mlflow_textcat_example&quot;</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">end_run</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">log_step</span><span class="p">,</span> <span class="n">finalize</span>

    <span class="k">return</span> <span class="n">setup_logger</span>
</pre></div>
</div>
<p>Check the <cite>spacy-loggers library &lt;https://pypi.org/project/spacy-loggers/&gt;</cite> _ for a more complete implementation.</p>
<p>Point to our mlflow logger in <code class="docutils literal notranslate"><span class="pre">Spacy</span></code> configuration file. For this example, we will lower the number of training steps
and eval frequency:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span> [training.logger]
<span class="gd">- @loggers = &quot;spacy.ConsoleLogger.v1&quot;</span>
<span class="gd">- dev = null</span>
<span class="gi">+ @loggers = &quot;mlflow_logger.v1&quot;</span>

<span class="w"> </span> [training]
<span class="gd">- max_steps = 20000</span>
<span class="gd">- eval_frequency = 100</span>
<span class="gi">+ max_steps = 100</span>
<span class="gi">+ eval_frequency = 10</span>
</pre></div>
</div>
<p>Train our model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.cli.train</span> <span class="kn">import</span> <span class="n">train</span> <span class="k">as</span> <span class="n">spacy_train</span>

<span class="n">spacy_train</span><span class="p">(</span><span class="s2">&quot;mlflow-textcat.cfg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To make predictions, we load the saved model from the last run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">MlflowClient</span>

<span class="c1"># look up the last run info from mlflow</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">MlflowClient</span><span class="p">()</span>
<span class="n">last_run</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">search_runs</span><span class="p">(</span><span class="n">experiment_ids</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;0&quot;</span><span class="p">],</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># We need to append the spacy model directory name to the artifact uri</span>
<span class="n">spacy_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">last_run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">artifact_uri</span><span class="si">}</span><span class="s2">/mlflow_textcat_example&quot;</span>
<span class="p">)</span>
<span class="n">predictions_in</span> <span class="o">=</span> <span class="n">dev_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;sentence&quot;</span><span class="p">]]</span>
<span class="n">predictions_out</span> <span class="o">=</span> <span class="n">spacy_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predictions_in</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;pos&quot;</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;pos&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;neg&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;neg&quot;</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">predictions_out</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dev_df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">predicted_sentiment</span><span class="o">=</span><span class="n">predicted_labels</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fastai-fastai">
<h3><a class="toc-backref" href="#id56">Fastai(<code class="docutils literal notranslate"><span class="pre">fastai</span></code>)</a><a class="headerlink" href="#fastai-fastai" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">fastai</span></code> model flavor enables logging of <a class="reference external" href="https://docs.fast.ai/learner.html">fastai Learner models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.fastai.html#mlflow.fastai.save_model" title="mlflow.fastai.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.fastai.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.fastai.html#mlflow.fastai.log_model" title="mlflow.fastai.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.fastai.log_model()</span></code></a> methods. Additionally, these
methods add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the models to be
interpreted as generic Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can
only be scored with DataFrame input. You can also use the <a class="reference internal" href="python_api/mlflow.fastai.html#mlflow.fastai.load_model" title="mlflow.fastai.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.fastai.load_model()</span></code></a> method to
load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">fastai</span></code> model flavor in native fastai format.</p>
<p>The interface for utilizing a <code class="docutils literal notranslate"><span class="pre">fastai</span></code> model loaded as a pyfunc type for generating predictions uses a
Pandas DataFrame argument.</p>
<p>This example runs the <a class="reference external" href="https://docs.fast.ai/tutorial.tabular.html">fastai tabular tutorial</a>,
logs the experiments, saves the model in <code class="docutils literal notranslate"><span class="pre">fastai</span></code> format and loads the model to get predictions
using a <code class="docutils literal notranslate"><span class="pre">fastai</span></code> data loader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.data.external</span> <span class="kn">import</span> <span class="n">URLs</span><span class="p">,</span> <span class="n">untar_data</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.core</span> <span class="kn">import</span> <span class="n">Categorify</span><span class="p">,</span> <span class="n">FillMissing</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">,</span> <span class="n">TabularPandas</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.data</span> <span class="kn">import</span> <span class="n">TabularDataLoaders</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.learner</span> <span class="kn">import</span> <span class="n">tabular_learner</span>
<span class="kn">from</span> <span class="nn">fastai.data.transforms</span> <span class="kn">import</span> <span class="n">RandomSplitter</span>
<span class="kn">from</span> <span class="nn">fastai.metrics</span> <span class="kn">import</span> <span class="n">accuracy</span>
<span class="kn">from</span> <span class="nn">fastcore.basics</span> <span class="kn">import</span> <span class="n">range_of</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">mlflow.fastai</span>


<span class="k">def</span> <span class="nf">print_auto_logged_info</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="n">tags</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">r</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tags</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mlflow.&quot;</span><span class="p">)}</span>
    <span class="n">artifacts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">f</span><span class="o">.</span><span class="n">path</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">MlflowClient</span><span class="p">()</span><span class="o">.</span><span class="n">list_artifacts</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;run_id: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;artifacts: </span><span class="si">{</span><span class="n">artifacts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;params: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metrics: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;tags: </span><span class="si">{</span><span class="n">tags</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">ADULT_SAMPLE</span><span class="p">)</span>
    <span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;adult.csv&quot;</span><span class="p">)</span>

    <span class="n">dls</span> <span class="o">=</span> <span class="n">TabularDataLoaders</span><span class="o">.</span><span class="n">from_csv</span><span class="p">(</span>
        <span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;adult.csv&quot;</span><span class="p">,</span>
        <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
        <span class="n">y_names</span><span class="o">=</span><span class="s2">&quot;salary&quot;</span><span class="p">,</span>
        <span class="n">cat_names</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;workclass&quot;</span><span class="p">,</span>
            <span class="s2">&quot;education&quot;</span><span class="p">,</span>
            <span class="s2">&quot;marital-status&quot;</span><span class="p">,</span>
            <span class="s2">&quot;occupation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;relationship&quot;</span><span class="p">,</span>
            <span class="s2">&quot;race&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">cont_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;fnlwgt&quot;</span><span class="p">,</span> <span class="s2">&quot;education-num&quot;</span><span class="p">],</span>
        <span class="n">procs</span><span class="o">=</span><span class="p">[</span><span class="n">Categorify</span><span class="p">,</span> <span class="n">FillMissing</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">splits</span> <span class="o">=</span> <span class="n">RandomSplitter</span><span class="p">(</span><span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">range_of</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

    <span class="n">to</span> <span class="o">=</span> <span class="n">TabularPandas</span><span class="p">(</span>
        <span class="n">df</span><span class="p">,</span>
        <span class="n">procs</span><span class="o">=</span><span class="p">[</span><span class="n">Categorify</span><span class="p">,</span> <span class="n">FillMissing</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">],</span>
        <span class="n">cat_names</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;workclass&quot;</span><span class="p">,</span>
            <span class="s2">&quot;education&quot;</span><span class="p">,</span>
            <span class="s2">&quot;marital-status&quot;</span><span class="p">,</span>
            <span class="s2">&quot;occupation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;relationship&quot;</span><span class="p">,</span>
            <span class="s2">&quot;race&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">cont_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;fnlwgt&quot;</span><span class="p">,</span> <span class="s2">&quot;education-num&quot;</span><span class="p">],</span>
        <span class="n">y_names</span><span class="o">=</span><span class="s2">&quot;salary&quot;</span><span class="p">,</span>
        <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">dls</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tabular_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">fastai</span><span class="o">.</span><span class="n">autolog</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">fastai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>

    <span class="n">print_auto_logged_info</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">get_run</span><span class="p">(</span><span class="n">run_id</span><span class="o">=</span><span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="p">))</span>

    <span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;runs:/</span><span class="si">{</span><span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">/model&quot;</span>
    <span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">fastai</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>

    <span class="n">test_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;salary&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dl</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">test_dl</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>

    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">get_preds</span><span class="p">(</span><span class="n">dl</span><span class="o">=</span><span class="n">dl</span><span class="p">)</span>
    <span class="n">px</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
    <span class="n">px</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>


<span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>Output (<code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code>):</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 44%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Index</p></th>
<th class="head"><p>Probability of first class</p></th>
<th class="head"><p>Probability of second class</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.545088</p></td>
<td><p>0.454912</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.503172</p></td>
<td><p>0.496828</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.962663</p></td>
<td><p>0.037337</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.206107</p></td>
<td><p>0.793893</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0.807599</p></td>
<td><p>0.192401</p></td>
</tr>
</tbody>
</table>
<p>Alternatively, when using the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor, get predictions from a DataFrame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.data.external</span> <span class="kn">import</span> <span class="n">URLs</span><span class="p">,</span> <span class="n">untar_data</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.core</span> <span class="kn">import</span> <span class="n">Categorify</span><span class="p">,</span> <span class="n">FillMissing</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">,</span> <span class="n">TabularPandas</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.data</span> <span class="kn">import</span> <span class="n">TabularDataLoaders</span>
<span class="kn">from</span> <span class="nn">fastai.tabular.learner</span> <span class="kn">import</span> <span class="n">tabular_learner</span>
<span class="kn">from</span> <span class="nn">fastai.data.transforms</span> <span class="kn">import</span> <span class="n">RandomSplitter</span>
<span class="kn">from</span> <span class="nn">fastai.metrics</span> <span class="kn">import</span> <span class="n">accuracy</span>
<span class="kn">from</span> <span class="nn">fastcore.basics</span> <span class="kn">import</span> <span class="n">range_of</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">mlflow.fastai</span>

<span class="n">model_uri</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">ADULT_SAMPLE</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;adult.csv&quot;</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;salary&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
</pre></div>
</div>
<p>Output (<code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code>):</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Index</p></th>
<th class="head"><p>Probability of first class, Probability of second class</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>[0.5450878, 0.45491222]</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>[0.50317234, 0.49682766]</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>[0.9626626, 0.037337445]</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>[0.20610662, 0.7938934]</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>[0.8075987, 0.19240129]</p></td>
</tr>
</tbody>
</table>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.fastai.html#module-mlflow.fastai" title="mlflow.fastai"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.fastai</span></code></a>.</p>
</div>
<div class="section" id="statsmodels-statsmodels">
<h3><a class="toc-backref" href="#id57">Statsmodels (<code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>)</a><a class="headerlink" href="#statsmodels-statsmodels" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> model flavor enables logging of <a class="reference external" href="https://www.statsmodels.org/stable/api.html">Statsmodels models</a> in MLflow format via the <a class="reference internal" href="python_api/mlflow.statsmodels.html#mlflow.statsmodels.save_model" title="mlflow.statsmodels.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.statsmodels.save_model()</span></code></a>
and <a class="reference internal" href="python_api/mlflow.statsmodels.html#mlflow.statsmodels.log_model" title="mlflow.statsmodels.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.statsmodels.log_model()</span></code></a> methods.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can only be scored with DataFrame input.
You can also use the <a class="reference internal" href="python_api/mlflow.statsmodels.html#mlflow.statsmodels.load_model" title="mlflow.statsmodels.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.statsmodels.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> model flavor in native statsmodels format.</p>
<p>As for now, automatic logging is restricted to parameters, metrics and models generated by a call to <cite>fit</cite>
on a <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> model.</p>
<div class="section" id="statsmodels-pyfunc-usage">
<h4>Statsmodels pyfunc usage<a class="headerlink" href="#statsmodels-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>The following 2 examples illustrate usage of a basic regression model (OLS) and an ARIMA time series model
from the following statsmodels apis : statsmodels.formula.api and statsmodels.tsa.api</p>
<p>For a minimal statsmodels regression model, here is an example of the pyfunc predict() method :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1"># load the diabetes dataset from sklearn</span>
<span class="n">diabetes</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>

<span class="c1"># create X and y dataframes for the features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">])</span>

<span class="c1"># concatenate X and y dataframes</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># create the linear regression model (ordinary least squares)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
    <span class="n">formula</span><span class="o">=</span><span class="s2">&quot;target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span>
<span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">statsmodels</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">disable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">disable_for_unsupported_versions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;pinv&quot;</span><span class="p">,</span> <span class="n">use_t</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">statsmodels</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;OLS_model&quot;</span><span class="p">)</span>

<span class="c1"># load the pyfunc model</span>
<span class="n">statsmodels_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># generate predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">statsmodels_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
<p>For a minimal time series ARIMA model, here is an example of the pyfunc predict() method :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">statsmodels.tsa.arima.model</span> <span class="kn">import</span> <span class="n">ARIMA</span>

<span class="c1"># create a time series dataset with seasonality</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># generate a time index with a daily frequency</span>
<span class="n">dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s2">&quot;2022-12-01&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2023-12-01&quot;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s2">&quot;D&quot;</span><span class="p">)</span>

<span class="c1"># generate the seasonal component (weekly)</span>
<span class="n">seasonality</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">365.25</span><span class="p">)</span> <span class="o">*</span> <span class="mi">7</span><span class="p">)</span>

<span class="c1"># generate the trend component</span>
<span class="n">trend</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">365.25</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="p">)</span>

<span class="c1"># generate the residual component</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">))</span>

<span class="c1"># generate the final time series by adding the components</span>
<span class="n">time_series</span> <span class="o">=</span> <span class="n">seasonality</span> <span class="o">+</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">residuals</span>

<span class="c1"># create a dataframe from the time series</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;date&quot;</span><span class="p">:</span> <span class="n">dates</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">time_series</span><span class="p">})</span>
<span class="n">data</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">order</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># create the ARIMA model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ARIMA</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">statsmodels</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span>
    <span class="n">log_models</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">disable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">disable_for_unsupported_versions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;order&quot;</span><span class="p">:</span> <span class="n">order</span><span class="p">,</span>
            <span class="s2">&quot;trend&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">trend</span><span class="p">,</span>
            <span class="s2">&quot;seasonal_order&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">seasonal_order</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;aic&quot;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">aic</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;bic&quot;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">bic</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">statsmodels</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;ARIMA_model&quot;</span><span class="p">)</span>

<span class="c1"># load the pyfunc model</span>
<span class="n">statsmodels_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># prediction dataframes for a TimeSeriesModel must have exactly one row and include columns called start and end</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2024-01-01&quot;</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2024-01-07&quot;</span><span class="p">)</span>

<span class="c1"># generate predictions</span>
<span class="n">prediction_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="n">start</span><span class="p">,</span> <span class="s2">&quot;end&quot;</span><span class="p">:</span> <span class="n">end</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">statsmodels_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prediction_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.statsmodels.html#module-mlflow.statsmodels" title="mlflow.statsmodels"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.statsmodels</span></code></a>.</p>
</div>
</div>
<div class="section" id="prophet-prophet">
<h3><a class="toc-backref" href="#id58">Prophet (<code class="docutils literal notranslate"><span class="pre">prophet</span></code>)</a><a class="headerlink" href="#prophet-prophet" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">prophet</span></code> model flavor enables logging of <a class="reference external" href="https://facebook.github.io/prophet/">Prophet models</a> in MLflow format via the <a class="reference internal" href="python_api/mlflow.prophet.html#mlflow.prophet.save_model" title="mlflow.prophet.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.prophet.save_model()</span></code></a>
and <a class="reference internal" href="python_api/mlflow.prophet.html#mlflow.prophet.log_model" title="mlflow.prophet.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.prophet.log_model()</span></code></a> methods.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
models to be interpreted as generic Python functions for inference via
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>. This loaded PyFunc model can only be scored with DataFrame input.
You can also use the <a class="reference internal" href="python_api/mlflow.prophet.html#mlflow.prophet.load_model" title="mlflow.prophet.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.prophet.load_model()</span></code></a>
method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">prophet</span></code> model flavor in native prophet format.</p>
<div class="section" id="prophet-pyfunc-usage">
<h4>Prophet pyfunc usage<a class="headerlink" href="#prophet-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>This example uses a time series dataset from Prophet’s GitHub repository, containing log number of daily views to
Peyton Manning’s Wikipedia page for several years. A sample of the dataset is as follows:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 43%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ds</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2007-12-10</p></td>
<td><p>9.59076113897809</p></td>
</tr>
<tr class="row-odd"><td><p>2007-12-11</p></td>
<td><p>8.51959031601596</p></td>
</tr>
<tr class="row-even"><td><p>2007-12-12</p></td>
<td><p>8.18367658262066</p></td>
</tr>
<tr class="row-odd"><td><p>2007-12-13</p></td>
<td><p>8.07246736935477</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">prophet</span> <span class="kn">import</span> <span class="n">Prophet</span>
<span class="kn">from</span> <span class="nn">prophet.diagnostics</span> <span class="kn">import</span> <span class="n">cross_validation</span><span class="p">,</span> <span class="n">performance_metrics</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>

<span class="c1"># starts on 2007-12-10, ends on 2016-01-20</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv&quot;</span>
<span class="p">)</span>

<span class="c1"># Create a &quot;test&quot; DataFrame with the &quot;ds&quot; column containing 10 days after the end date in train_df</span>
<span class="n">test_dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s2">&quot;2016-01-21&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;2016-01-31&quot;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s2">&quot;D&quot;</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">test_dates</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ds&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>

<span class="n">prophet_model</span> <span class="o">=</span> <span class="n">Prophet</span><span class="p">(</span><span class="n">changepoint_prior_scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">uncertainty_samples</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">prophet_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>

    <span class="c1"># extract and log parameters such as changepoint_prior_scale in the mlflow run</span>
    <span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">(</span><span class="n">prophet_model</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">model_params</span><span class="p">)</span>

    <span class="c1"># cross validate with 900 days of data initially, predictions for next 30 days</span>
    <span class="c1"># walk forward by 30 days</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="p">(</span>
        <span class="n">prophet_model</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="s2">&quot;900 days&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="s2">&quot;30 days&quot;</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="s2">&quot;30 days&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Calculate metrics from cv_results, then average each metric across all backtesting windows and log to mlflow</span>
    <span class="n">cv_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="s2">&quot;rmse&quot;</span><span class="p">,</span> <span class="s2">&quot;mape&quot;</span><span class="p">]</span>
    <span class="n">metrics_results</span> <span class="o">=</span> <span class="n">performance_metrics</span><span class="p">(</span><span class="n">cv_results</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">cv_metrics</span><span class="p">)</span>
    <span class="n">average_metrics</span> <span class="o">=</span> <span class="n">metrics_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">cv_metrics</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">average_metrics</span><span class="p">)</span>

    <span class="c1"># Calculate model signature</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">prophet_model</span><span class="o">.</span><span class="n">history</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">prophet_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prophet_model</span><span class="o">.</span><span class="n">make_future_dataframe</span><span class="p">(</span><span class="mi">30</span><span class="p">))</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">prophet</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">prophet_model</span><span class="p">,</span> <span class="s2">&quot;prophet-model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span>

<span class="c1"># Load saved model</span>
<span class="n">prophet_model_saved</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">prophet_model_saved</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
</pre></div>
</div>
<p>Output (<code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code>):</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 21%" />
<col style="width: 23%" />
<col style="width: 25%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Index</p></th>
<th class="head"><p>ds</p></th>
<th class="head"><p>yhat</p></th>
<th class="head"><p>yhat_upper</p></th>
<th class="head"><p>yhat_lower</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>2016-01-21</p></td>
<td><p>8.526513</p></td>
<td><p>8.827397</p></td>
<td><p>8.328563</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>2016-01-22</p></td>
<td><p>8.541355</p></td>
<td><p>9.434994</p></td>
<td><p>8.112758</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>2016-01-23</p></td>
<td><p>8.308332</p></td>
<td><p>8.633746</p></td>
<td><p>8.201323</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>2016-01-24</p></td>
<td><p>8.676326</p></td>
<td><p>9.534593</p></td>
<td><p>8.020874</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>2016-01-25</p></td>
<td><p>8.983457</p></td>
<td><p>9.430136</p></td>
<td><p>8.121798</p></td>
</tr>
</tbody>
</table>
<p>For more information, see <a class="reference internal" href="python_api/mlflow.prophet.html#module-mlflow.prophet" title="mlflow.prophet"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.prophet</span></code></a>.</p>
</div>
</div>
<div class="section" id="pmdarima-pmdarima">
<span id="pmdarima-flavor"></span><h3><a class="toc-backref" href="#id59">Pmdarima (<code class="docutils literal notranslate"><span class="pre">pmdarima</span></code>)</a><a class="headerlink" href="#pmdarima-pmdarima" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> model flavor enables logging of <a class="reference external" href="http://alkaline-ml.com/pmdarima/">pmdarima models</a> in MLflow
format via the <a class="reference internal" href="python_api/mlflow.pmdarima.html#mlflow.pmdarima.save_model" title="mlflow.pmdarima.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pmdarima.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.pmdarima.html#mlflow.pmdarima.log_model" title="mlflow.pmdarima.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pmdarima.log_model()</span></code></a> methods.
These methods also add the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the
model to be interpreted as generic Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can only be scored with a DataFrame input.
You can also use the <a class="reference internal" href="python_api/mlflow.pmdarima.html#mlflow.pmdarima.load_model" title="mlflow.pmdarima.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pmdarima.load_model()</span></code></a> method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code>
model flavor in native pmdarima formats.</p>
<p>The interface for utilizing a <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> model loaded as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> type for generating forecast predictions uses
a <em>single-row</em> <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> configuration argument. The following columns in this configuration
<code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> are supported:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">n_periods</span></code> (required) - specifies the number of future periods to generate starting from the last datetime value</dt><dd><p>of the training dataset, utilizing the frequency of the input training series when the model was trained.
(for example, if the training data series elements represent one value per hour, in order to forecast 3 days of
future data, set the column <code class="docutils literal notranslate"><span class="pre">n_periods</span></code> to <code class="docutils literal notranslate"><span class="pre">72</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">X</span></code> (optional) - exogenous regressor values (<em>only supported in pmdarima version &gt;= 1.8.0</em>) a 2D array of values for</dt><dd><p>future time period events. For more information, read the underlying library
<a class="reference external" href="https://www.statsmodels.org/stable/endog_exog.html">explanation</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">return_conf_int</span></code> (optional) - a boolean (Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>) for whether to return confidence interval values.</dt><dd><p>See above note.</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> (optional) - the significance value for calculating confidence intervals. (Default: <code class="docutils literal notranslate"><span class="pre">0.05</span></code>)</p></li>
</ul>
<p>An example configuration for the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> predict of a <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> model is shown below, with a future period
prediction count of 100, a confidence interval calculation generation, no exogenous regressor elements, and a default
alpha of <code class="docutils literal notranslate"><span class="pre">0.05</span></code>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 30%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Index</p></th>
<th class="head"><p>n_periods</p></th>
<th class="head"><p>return_conf_int</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>100</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> passed to a <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> flavor must only contain 1 row.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When predicting a <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> flavor, the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method’s <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> configuration column
<code class="docutils literal notranslate"><span class="pre">return_conf_int</span></code>’s value controls the output format. When the column’s value is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>
(which is the default if this column is not supplied in the configuration <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>), the schema of the
returned <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> is a single column: <code class="docutils literal notranslate"><span class="pre">[&quot;yhat&quot;]</span></code>. When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the schema of the returned
<code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> is: <code class="docutils literal notranslate"><span class="pre">[&quot;yhat&quot;,</span> <span class="pre">&quot;yhat_lower&quot;,</span> <span class="pre">&quot;yhat_upper&quot;]</span></code> with the respective lower (<code class="docutils literal notranslate"><span class="pre">yhat_lower</span></code>) and
upper (<code class="docutils literal notranslate"><span class="pre">yhat_upper</span></code>) confidence intervals added to the forecast predictions (<code class="docutils literal notranslate"><span class="pre">yhat</span></code>).</p>
</div>
<p>Example usage of pmdarima artifact loaded as a pyfunc with confidence intervals calculated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pmdarima</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pmdarima</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_airpassengers</span><span class="p">()</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">pmdarima</span><span class="o">.</span><span class="n">auto_arima</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">seasonal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">pmdarima</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;/tmp/model.pmd&quot;</span><span class="p">)</span>

<span class="n">loaded_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;/tmp/model.pmd&quot;</span><span class="p">)</span>

<span class="n">prediction_conf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[{</span><span class="s2">&quot;n_periods&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;return_conf_int&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}]</span>
<span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">loaded_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prediction_conf</span><span class="p">)</span>
</pre></div>
</div>
<p>Output (<code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code>):</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 28%" />
<col style="width: 28%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Index</p></th>
<th class="head"><p>yhat</p></th>
<th class="head"><p>yhat_lower</p></th>
<th class="head"><p>yhat_upper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>467.573731</p></td>
<td><p>423.30995</p></td>
<td><p>511.83751</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>490.494467</p></td>
<td><p>416.17449</p></td>
<td><p>564.81444</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>509.138684</p></td>
<td><p>420.56255</p></td>
<td><p>597.71117</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>492.554714</p></td>
<td><p>397.30634</p></td>
<td><p>587.80309</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Signature logging for <code class="docutils literal notranslate"><span class="pre">pmdarima</span></code> will not function correctly if <code class="docutils literal notranslate"><span class="pre">return_conf_int</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> from
a non-pyfunc artifact. The output of the native <code class="docutils literal notranslate"><span class="pre">ARIMA.predict()</span></code> when returning confidence intervals is not
a recognized signature type.</p>
</div>
</div>
<div class="section" id="openai-openai-experimental">
<h3><a class="toc-backref" href="#id60">OpenAI (<code class="docutils literal notranslate"><span class="pre">openai</span></code>) (Experimental)</a><a class="headerlink" href="#openai-openai-experimental" title="Permalink to this headline"> </a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">openai</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">openi</span></code> model flavor enables logging of <a class="reference external" href="https://github.com/openai/openai-python">OpenAI models</a> in MLflow format via
the <a class="reference internal" href="python_api/openai/index.html#mlflow.openai.save_model" title="mlflow.openai.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.openai.save_model()</span></code></a> and <a class="reference internal" href="python_api/openai/index.html#mlflow.openai.log_model" title="mlflow.openai.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.openai.log_model()</span></code></a> functions. Use of these
functions also adds the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be
interpreted as a generic Python function for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="python_api/openai/index.html#mlflow.openai.load_model" title="mlflow.openai.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.openai.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">openai</span></code> flavor as a dictionary of the model’s attributes.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models.signature</span> <span class="kn">import</span> <span class="n">ModelSignature</span>
<span class="kn">from</span> <span class="nn">mlflow.types.schema</span> <span class="kn">import</span> <span class="n">ColSpec</span><span class="p">,</span> <span class="n">ParamSchema</span><span class="p">,</span> <span class="n">ParamSpec</span><span class="p">,</span> <span class="n">Schema</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;mlflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="c1"># Uncomment the following lines to run this script without using a real OpenAI API key.</span>
<span class="c1"># os.environ[&quot;MLFLOW_TESTING&quot;] = &quot;true&quot;</span>
<span class="c1"># os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;test&quot;</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Single variable</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;cats&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;dogs&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Multiple variables</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a </span><span class="si">{adjective}</span><span class="s2"> joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;funny&quot;</span><span class="p">,</span> <span class="s2">&quot;scary&quot;</span><span class="p">],</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cats&quot;</span><span class="p">,</span> <span class="s2">&quot;dogs&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>


<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="s2">&quot;funny&quot;</span><span class="p">,</span> <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;cats&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;adjective&quot;</span><span class="p">:</span> <span class="s2">&quot;scary&quot;</span><span class="p">,</span> <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="s2">&quot;dogs&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Multiple prompts</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are </span><span class="si">{person}</span><span class="s2">&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on </span><span class="si">{topic}</span><span class="s2">&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Elon Musk&quot;</span><span class="p">,</span> <span class="s2">&quot;Jeff Bezos&quot;</span><span class="p">],</span>
        <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;AI&quot;</span><span class="p">,</span> <span class="s2">&quot;ML&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="s2">&quot;Elon Musk&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;AI&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="s2">&quot;Jeff Bezos&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;ML&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># No input variables</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are Elon Musk&quot;</span><span class="p">}],</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>

<span class="n">list_of_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_dicts</span><span class="p">))</span>

<span class="n">list_of_strings</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Let me hear your thoughts on AI&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Let me hear your thoughts on ML&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">list_of_strings</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd"># Inference parameters with chat completions</span>
<span class="sd"># ******************************************************************************</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke about </span><span class="si">{animal}</span><span class="s2">.&quot;</span><span class="p">}],</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">ModelSignature</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)]),</span>
            <span class="n">outputs</span><span class="o">=</span><span class="n">Schema</span><span class="p">([</span><span class="n">ColSpec</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">)]),</span>
            <span class="n">params</span><span class="o">=</span><span class="n">ParamSchema</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">ParamSpec</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">),</span>
                <span class="p">]</span>
            <span class="p">),</span>
        <span class="p">),</span>
    <span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;animal&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;cats&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dogs&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="section" id="langchain-langchain-experimental">
<h3><a class="toc-backref" href="#id61">LangChain (<code class="docutils literal notranslate"><span class="pre">langchain</span></code>) (Experimental)</a><a class="headerlink" href="#langchain-langchain-experimental" title="Permalink to this headline"> </a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">langchain</span></code> model flavor enables logging of <a class="reference external" href="https://github.com/hwchase17/langchain">LangChain models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.langchain.html#mlflow.langchain.save_model" title="mlflow.langchain.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.langchain.html#mlflow.langchain.log_model" title="mlflow.langchain.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.log_model()</span></code></a> functions. Use of these
functions also adds the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be
interpreted as a generic Python function for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="python_api/mlflow.langchain.html#mlflow.langchain.load_model" title="mlflow.langchain.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.langchain.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor as a dictionary of the model’s attributes.</p>
<p>Example: Log a LangChain LLMChain</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;product&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;What is a good name for a company that makes </span><span class="si">{product}</span><span class="s2">?&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="s2">&quot;langchain_model&quot;</span><span class="p">)</span>

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([{</span><span class="s2">&quot;product&quot;</span><span class="p">:</span> <span class="s2">&quot;colorful socks&quot;</span><span class="p">}]))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id28">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id28" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Colorful Cozy Creations.&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Example: Log a LangChain Agent</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">AgentType</span><span class="p">,</span> <span class="n">initialize_agent</span><span class="p">,</span> <span class="n">load_tools</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>
<span class="k">assert</span> <span class="s2">&quot;SERPAPI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the SERPAPI_API_KEY environment variable.&quot;</span>

<span class="c1"># First, let&#39;s load the language model we&#39;re going to use to control the agent.</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Next, let&#39;s load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.</span>
<span class="n">tools</span> <span class="o">=</span> <span class="n">load_tools</span><span class="p">([</span><span class="s2">&quot;serpapi&quot;</span><span class="p">,</span> <span class="s2">&quot;llm-math&quot;</span><span class="p">],</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>

<span class="c1"># Finally, let&#39;s initialize an agent with the tools, the language model, and the type of agent we want to use.</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">initialize_agent</span><span class="p">(</span><span class="n">tools</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="n">AgentType</span><span class="o">.</span><span class="n">ZERO_SHOT_REACT_DESCRIPTION</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;langchain_model&quot;</span><span class="p">)</span>

<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?&quot;</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id29">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id29" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot;1.1044000282035853&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="logging-retrievalqa-chains">
<h4>Logging RetrievalQA Chains<a class="headerlink" href="#logging-retrievalqa-chains" title="Permalink to this headline"> </a></h4>
<p>In MLflow, you can use the <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor to save a <code class="docutils literal notranslate"><span class="pre">RetrievalQA</span></code> chain, including the retriever object.</p>
<p>Native LangChain requires the user to handle the serialization and deserialization of the retriever object, but MLflow’s <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor handles that for you.</p>
<p>Here are the two things you need to tell MLflow:</p>
<ol class="arabic simple">
<li><p>Where the retriever object is stored (<code class="docutils literal notranslate"><span class="pre">persist_dir</span></code>).</p></li>
<li><p>How to load the retriever object from that location (<code class="docutils literal notranslate"><span class="pre">loader_fn</span></code>).</p></li>
</ol>
<p>After you define these, MLflow takes care of the rest, saving both the content in the <code class="docutils literal notranslate"><span class="pre">persist_dir</span></code> and pickling the <code class="docutils literal notranslate"><span class="pre">loader_fn</span></code> function.</p>
<p>Example: Log a LangChain RetrievalQA Chain</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings.openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>

<span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
    <span class="n">persist_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">,</span> <span class="s2">&quot;faiss_index&quot;</span><span class="p">)</span>

    <span class="c1"># Create the vector db, persist the db to a local fs folder</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">TextLoader</span><span class="p">(</span><span class="s2">&quot;tests/langchain/state_of_the_union.txt&quot;</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
    <span class="n">db</span><span class="o">.</span><span class="n">save_local</span><span class="p">(</span><span class="n">persist_dir</span><span class="p">)</span>

    <span class="c1"># Create the RetrievalQA chain</span>
    <span class="n">retrievalQA</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">OpenAI</span><span class="p">(),</span> <span class="n">retriever</span><span class="o">=</span><span class="n">db</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">())</span>

    <span class="c1"># Log the retrievalQA chain</span>
    <span class="k">def</span> <span class="nf">load_retriever</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
        <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">load_local</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
        <span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
            <span class="n">retrievalQA</span><span class="p">,</span>
            <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;retrieval_qa&quot;</span><span class="p">,</span>
            <span class="n">loader_fn</span><span class="o">=</span><span class="n">load_retriever</span><span class="p">,</span>
            <span class="n">persist_dir</span><span class="o">=</span><span class="n">persist_dir</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># Load the retrievalQA chain</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;What did the president say about Ketanji Brown Jackson&quot;</span><span class="p">}]))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id30">
<div class="code-block-caption"><span class="caption-text">Output (truncated)</span><a class="headerlink" href="#id30" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s2">&quot; The president said...&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="logging-a-retriever-and-evaluate-it-individually">
<span id="log-retriever-chain"></span><h4>Logging a retriever and evaluate it individually<a class="headerlink" href="#logging-a-retriever-and-evaluate-it-individually" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor provides the functionality to log a retriever object and evaluate it individually. This is useful if
you want to evaluate the quality of the relevant documents returned by a retriever object without directing these documents
through a large language model (LLM) to yield a summarized response.</p>
<p>In order to log the retriever object in the <code class="docutils literal notranslate"><span class="pre">langchain</span></code> flavor, it is also required to specify <code class="docutils literal notranslate"><span class="pre">persist_dir</span></code>
and <code class="docutils literal notranslate"><span class="pre">loader_fn</span></code>, the same as logging the RetrievalQA chain. See the previous section for details about these parameters.</p>
<p>See the following example for more details.</p>
<p>Example: Log a LangChain Retriever</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings.openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="k">assert</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="s2">&quot;Please set the OPENAI_API_KEY environment variable.&quot;</span>

<span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
    <span class="n">persist_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">,</span> <span class="s2">&quot;faiss_index&quot;</span><span class="p">)</span>

    <span class="c1"># Create the vector db, persist the db to a local fs folder</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">TextLoader</span><span class="p">(</span><span class="s2">&quot;tests/langchain/state_of_the_union.txt&quot;</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
    <span class="n">db</span><span class="o">.</span><span class="n">save_local</span><span class="p">(</span><span class="n">persist_dir</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_retriever</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
        <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">load_local</span><span class="p">(</span><span class="n">persist_directory</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>

    <span class="c1"># Log the retriever</span>
    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
        <span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">langchain</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
            <span class="n">db</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span>
            <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;retriever&quot;</span><span class="p">,</span>
            <span class="n">loader_fn</span><span class="o">=</span><span class="n">load_retriever</span><span class="p">,</span>
            <span class="n">persist_dir</span><span class="o">=</span><span class="n">persist_dir</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># Load the retriever chain</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;What did the president say about Ketanji Brown Jackson&quot;</span><span class="p">}]))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id31">
<div class="code-block-caption"><span class="caption-text">Output (truncated)</span><a class="headerlink" href="#id31" title="Permalink to this code"> </a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;page_content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tonight. I call...&quot;</span><span class="p">,</span>
            <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;/state.txt&quot;</span><span class="p">},</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;page_content&quot;</span><span class="p">:</span> <span class="s2">&quot;A former top...&quot;</span><span class="p">,</span>
            <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;/state.txt&quot;</span><span class="p">},</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="john-snow-labs-johnsnowlabs-experimental">
<h3><a class="toc-backref" href="#id62">John Snow Labs (<code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code>) (Experimental)</a><a class="headerlink" href="#john-snow-labs-johnsnowlabs-experimental" title="Permalink to this headline"> </a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code> model flavor gives you access to <a class="reference external" href="https://nlp.johnsnowlabs.com/models">20.000+ state-of-the-art enterprise NLP models in 200+ languages</a> for medical, finance, legal and many more domains.</p>
<p>You can use <a class="reference internal" href="python_api/mlflow.johnsnowlabs.html#mlflow.johnsnowlabs.log_model" title="mlflow.johnsnowlabs.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.johnsnowlabs.log_model()</span></code></a> to log and export your model as
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel" title="mlflow.pyfunc.PyFuncModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlflow.pyfunc.PyFuncModel</span></code></a>.</p>
<p>This enables you to integrate <a class="reference external" href="https://nlp.johnsnowlabs.com/models">any John Snow Labs model</a>
into the MLflow framework. You can easily deploy your models for inference with MLflows serve functionalities.
Models are interpreted as a generic Python function for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="python_api/mlflow.johnsnowlabs.html#mlflow.johnsnowlabs.load_model" title="mlflow.johnsnowlabs.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.johnsnowlabs.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">johnsnowlabs</span></code> flavor from an stored artifact.</p>
<p>Features include: LLM’s, Text Summarization, Question Answering, Named Entity Recognition, Relation
Extraction, Sentiment Analysis, Spell Checking, Image Classification, Automatic Speech Recognition and much more,
powered by the latest Transformer Architectures. The models are provided by <a class="reference external" href="https://www.johnsnowlabs.com/">John Snow Labs</a> and requires a <a class="reference external" href="https://www.johnsnowlabs.com/">John Snow Labs</a>
Enterprise NLP License. <a class="reference external" href="https://www.johnsnowlabs.com/schedule-a-demo/">You can reach out to us</a>
for a research or industry license.</p>
<p>Example: Export a John Snow Labs to MLflow format</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">johnsnowlabs</span> <span class="kn">import</span> <span class="n">nlp</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.pyfunc</span> <span class="kn">import</span> <span class="n">spark_udf</span>

<span class="c1"># 1) Write your raw license.json string into the &#39;JOHNSNOWLABS_LICENSE_JSON&#39; env variable for MLflow</span>
<span class="n">creds</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;AWS_ACCESS_KEY_ID&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AWS_SECRET_ACCESS_KEY&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SPARK_NLP_LICENSE&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SECRET&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;JOHNSNOWLABS_LICENSE_JSON&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">creds</span><span class="p">)</span>

<span class="c1"># 2) Install enterprise libraries</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">install</span><span class="p">()</span>
<span class="c1"># 3) Start a Spark session with enterprise libraries</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># 4) Load a model and test it</span>
<span class="n">nlu_model</span> <span class="o">=</span> <span class="s2">&quot;en.classify.bert_sequence.covid_sentiment&quot;</span>
<span class="n">model_save_path</span> <span class="o">=</span> <span class="s2">&quot;my_model&quot;</span>
<span class="n">johnsnowlabs_model</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">nlu_model</span><span class="p">)</span>
<span class="n">johnsnowlabs_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="s2">&quot;I hate COVID,&quot;</span><span class="p">,</span> <span class="s2">&quot;I love COVID&quot;</span><span class="p">])</span>

<span class="c1"># 5) Export model with pyfunc and johnsnowlabs flavors</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">johnsnowlabs</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">johnsnowlabs_model</span><span class="p">,</span> <span class="n">model_save_path</span><span class="p">)</span>

<span class="c1"># 6) Load model with johnsnowlabs flavor</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">johnsnowlabs</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># 7) Load model with pyfunc flavor</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_save_path</span><span class="p">)</span>

<span class="n">pandas_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Hello World&quot;</span><span class="p">]})</span>
<span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">spark_udf</span><span class="p">(</span>
    <span class="n">spark</span><span class="o">=</span><span class="n">spark</span><span class="p">,</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="n">model_save_path</span><span class="p">,</span>
    <span class="n">env_manager</span><span class="o">=</span><span class="s2">&quot;virtualenv&quot;</span><span class="p">,</span>
    <span class="n">result_type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">(</span><span class="o">*</span><span class="n">pandas_df</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>

<span class="c1"># 9) You can now use the mlflow models serve command to serve the model see next section</span>

<span class="c1"># 10)  You can also use x command to deploy model inside of a container see next section</span>
</pre></div>
</div>
<div class="section" id="to-deploy-the-john-snow-labs-model-as-a-container">
<h4>To deploy the John Snow Labs model as a container<a class="headerlink" href="#to-deploy-the-john-snow-labs-model-as-a-container" title="Permalink to this headline"> </a></h4>
<ol class="arabic simple">
<li><p>Start the Docker Container</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span><span class="m">5001</span>:8080<span class="w"> </span>-e<span class="w"> </span><span class="nv">JOHNSNOWLABS_LICENSE_JSON</span><span class="o">=</span>your_json_string<span class="w"> </span><span class="s2">&quot;mlflow-pyfunc&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Query server</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://127.0.0.1:5001/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;dataframe_split&quot;: {</span>
<span class="s1">      &quot;columns&quot;: [&quot;text&quot;],</span>
<span class="s1">      &quot;data&quot;: [[&quot;I hate covid&quot;], [&quot;I love covid&quot;]]</span>
<span class="s1">  }</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="to-deploy-the-john-snow-labs-model-without-a-container">
<h4>To deploy the John Snow Labs model without a container<a class="headerlink" href="#to-deploy-the-john-snow-labs-model-without-a-container" title="Permalink to this headline"> </a></h4>
<ol class="arabic simple">
<li><p>Export env variable and start server</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">JOHNSNOWLABS_LICENSE_JSON</span><span class="o">=</span>your_json_string
mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>&lt;model_uri&gt;
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Query server</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;dataframe_split&quot;: {</span>
<span class="s1">      &quot;columns&quot;: [&quot;text&quot;],</span>
<span class="s1">      &quot;data&quot;: [[&quot;I hate covid&quot;], [&quot;I love covid&quot;]]</span>
<span class="s1">  }</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="diviner-diviner">
<h3><a class="toc-backref" href="#id63">Diviner (<code class="docutils literal notranslate"><span class="pre">diviner</span></code>)</a><a class="headerlink" href="#diviner-diviner" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">diviner</span></code> model flavor enables logging of
<a class="reference external" href="https://databricks-diviner.readthedocs.io/en/latest/index.html">diviner models</a> in MLflow format via the
<a class="reference internal" href="python_api/mlflow.diviner.html#mlflow.diviner.save_model" title="mlflow.diviner.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.diviner.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.diviner.html#mlflow.diviner.log_model" title="mlflow.diviner.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.diviner.log_model()</span></code></a> methods. These methods also add the
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be interpreted as generic
Python functions for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
This loaded PyFunc model can only be scored with a DataFrame input.
You can also use the <a class="reference internal" href="python_api/mlflow.diviner.html#mlflow.diviner.load_model" title="mlflow.diviner.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.diviner.load_model()</span></code></a> method to load MLflow Models with the <code class="docutils literal notranslate"><span class="pre">diviner</span></code>
model flavor in native diviner formats.</p>
<div class="section" id="diviner-types">
<h4>Diviner Types<a class="headerlink" href="#diviner-types" title="Permalink to this headline"> </a></h4>
<p>Diviner is a library that provides an orchestration framework for performing time series forecasting on groups of
related series. Forecasting in <code class="docutils literal notranslate"><span class="pre">diviner</span></code> is accomplished through wrapping popular open source libraries such as
<a class="reference external" href="https://facebook.github.io/prophet/">prophet</a> and <a class="reference external" href="http://alkaline-ml.com/pmdarima/">pmdarima</a>. The <code class="docutils literal notranslate"><span class="pre">diviner</span></code>
library offers a simplified set of APIs to simultaneously generate distinct time series forecasts for multiple data
groupings using a single input DataFrame and a unified high-level API.</p>
</div>
<div class="section" id="metrics-and-parameters-logging-for-diviner">
<h4>Metrics and Parameters logging for Diviner<a class="headerlink" href="#metrics-and-parameters-logging-for-diviner" title="Permalink to this headline"> </a></h4>
<p>Unlike other flavors that are supported in MLflow, Diviner has the concept of grouped models. As a collection of many
(perhaps thousands) of individual forecasting models, the burden to the tracking server to log individual metrics
and parameters for each of these models is significant. For this reason, metrics and parameters are exposed for
retrieval from Diviner’s APIs as <code class="docutils literal notranslate"><span class="pre">Pandas</span></code> <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>, rather than discrete primitive values.</p>
<p>To illustrate, let us assume we are forecasting hourly electricity consumption from major cities around the world.
A sample of our input data looks like this:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 23%" />
<col style="width: 44%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>country</p></th>
<th class="head"><p>city</p></th>
<th class="head"><p>datetime</p></th>
<th class="head"><p>watts</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>US</p></td>
<td><p>NewYork</p></td>
<td><p>2022-03-01 00:01:00</p></td>
<td><p>23568.9</p></td>
</tr>
<tr class="row-odd"><td><p>US</p></td>
<td><p>NewYork</p></td>
<td><p>2022-03-01 00:02:00</p></td>
<td><p>22331.7</p></td>
</tr>
<tr class="row-even"><td><p>US</p></td>
<td><p>Boston</p></td>
<td><p>2022-03-01 00:01:00</p></td>
<td><p>14220.1</p></td>
</tr>
<tr class="row-odd"><td><p>US</p></td>
<td><p>Boston</p></td>
<td><p>2022-03-01 00:02:00</p></td>
<td><p>14183.4</p></td>
</tr>
<tr class="row-even"><td><p>CA</p></td>
<td><p>Toronto</p></td>
<td><p>2022-03-01 00:01:00</p></td>
<td><p>18562.2</p></td>
</tr>
<tr class="row-odd"><td><p>CA</p></td>
<td><p>Toronto</p></td>
<td><p>2022-03-01 00:02:00</p></td>
<td><p>17681.6</p></td>
</tr>
<tr class="row-even"><td><p>MX</p></td>
<td><p>MexicoCity</p></td>
<td><p>2022-03-01 00:01:00</p></td>
<td><p>19946.8</p></td>
</tr>
<tr class="row-odd"><td><p>MX</p></td>
<td><p>MexicoCity</p></td>
<td><p>2022-03-01 00:02:00</p></td>
<td><p>19444.0</p></td>
</tr>
</tbody>
</table>
<p>If we were to <code class="docutils literal notranslate"><span class="pre">fit</span></code> a model on this data, supplying the grouping keys as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grouping_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">,</span> <span class="s2">&quot;city&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>We will have a model generated for each of the grouping keys that have been supplied:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="s2">&quot;US&quot;</span><span class="p">,</span> <span class="s2">&quot;NewYork&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;US&quot;</span><span class="p">,</span> <span class="s2">&quot;Boston&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;CA&quot;</span><span class="p">,</span> <span class="s2">&quot;Toronto&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;MX&quot;</span><span class="p">,</span> <span class="s2">&quot;MexicoCity&quot;</span><span class="p">)]</span>
</pre></div>
</div>
<p>With a model constructed for each of these, entering each of their metrics and parameters wouldn’t be an issue for the
MLflow tracking server. What would become a problem, however, is if we modeled each major city on the planet and ran
this forecasting scenario every day. If we were to adhere to the conditions of the World Bank, that would mean just
over 10,000 models as of 2022. After a mere few weeks of running this forecasting every day we would have a very large
metrics table.</p>
<p>To eliminate this issue for large-scale forecasting, the metrics and parameters for <code class="docutils literal notranslate"><span class="pre">diviner</span></code> are extracted as a
grouping key indexed <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code>, as shown below for example (float values truncated for visibility):</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 28%" />
<col style="width: 9%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 5%" />
<col style="width: 7%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>grouping_key_columns</p></th>
<th class="head"><p>country</p></th>
<th class="head"><p>city</p></th>
<th class="head"><p>mse</p></th>
<th class="head"><p>rmse</p></th>
<th class="head"><p>mae</p></th>
<th class="head"><p>mape</p></th>
<th class="head"><p>mdape</p></th>
<th class="head"><p>smape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“(‘country’, ‘city’)”</p></td>
<td><p>CA</p></td>
<td><p>Toronto</p></td>
<td><p>8276851.6</p></td>
<td><p>2801.7</p></td>
<td><p>2417.7</p></td>
<td><p>0.16</p></td>
<td><p>0.16</p></td>
<td><p>0.159</p></td>
</tr>
<tr class="row-odd"><td><p>“(‘country’, ‘city’)”</p></td>
<td><p>MX</p></td>
<td><p>MexicoCity</p></td>
<td><p>3548872.4</p></td>
<td><p>1833.8</p></td>
<td><p>1584.5</p></td>
<td><p>0.15</p></td>
<td><p>0.16</p></td>
<td><p>0.159</p></td>
</tr>
<tr class="row-even"><td><p>“(‘country’, ‘city’)”</p></td>
<td><p>US</p></td>
<td><p>NewYork</p></td>
<td><p>3167846.4</p></td>
<td><p>1732.4</p></td>
<td><p>1498.2</p></td>
<td><p>0.15</p></td>
<td><p>0.16</p></td>
<td><p>0.158</p></td>
</tr>
<tr class="row-odd"><td><p>“(‘country’, ‘city’)”</p></td>
<td><p>US</p></td>
<td><p>Boston</p></td>
<td><p>14082666.4</p></td>
<td><p>3653.2</p></td>
<td><p>3156.2</p></td>
<td><p>0.15</p></td>
<td><p>0.16</p></td>
<td><p>0.159</p></td>
</tr>
</tbody>
</table>
<p>There are two recommended means of logging the metrics and parameters from a <code class="docutils literal notranslate"><span class="pre">diviner</span></code> model :</p>
<ul class="simple">
<li><p>Writing the DataFrames to local storage and using <a class="reference internal" href="python_api/mlflow.html#mlflow.log_artifacts" title="mlflow.log_artifacts"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.log_artifacts()</span></code></a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">extract_model_params</span><span class="p">()</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cross_validate_and_score</span><span class="p">(</span>
        <span class="n">horizon</span><span class="o">=</span><span class="s2">&quot;72 hours&quot;</span><span class="p">,</span>
        <span class="n">period</span><span class="o">=</span><span class="s2">&quot;240 hours&quot;</span><span class="p">,</span>
        <span class="n">initial</span><span class="o">=</span><span class="s2">&quot;480 hours&quot;</span><span class="p">,</span>
        <span class="n">parallel</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">,</span>
        <span class="n">rolling_window</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">monthly</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tmpdir</span><span class="si">}</span><span class="s2">/params.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tmpdir</span><span class="si">}</span><span class="s2">/metrics.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifacts</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Writing directly as a JSON artifact using <a class="reference internal" href="python_api/mlflow.html#mlflow.log_dict" title="mlflow.log_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.log_dict()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The parameters extract from <code class="docutils literal notranslate"><span class="pre">diviner</span></code> models <em>may require</em> casting (or dropping of columns) if using the
<code class="docutils literal notranslate"><span class="pre">pd.DataFrame.to_dict()</span></code> approach due to the inability of this method to serialize objects.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">extract_model_params</span><span class="p">()</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cross_validate_and_score</span><span class="p">(</span>
    <span class="n">horizon</span><span class="o">=</span><span class="s2">&quot;72 hours&quot;</span><span class="p">,</span>
    <span class="n">period</span><span class="o">=</span><span class="s2">&quot;240 hours&quot;</span><span class="p">,</span>
    <span class="n">initial</span><span class="o">=</span><span class="s2">&quot;480 hours&quot;</span><span class="p">,</span>
    <span class="n">parallel</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">,</span>
    <span class="n">rolling_window</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">monthly</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">params</span><span class="p">[</span><span class="s2">&quot;t_scale&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;t_scale&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">params</span><span class="p">[</span><span class="s2">&quot;start&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;start&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;stan_backend&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span> <span class="s2">&quot;params.json&quot;</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(),</span> <span class="s2">&quot;metrics.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Logging of the model artifact is shown in the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> example below.</p>
</div>
<div class="section" id="diviner-pyfunc-usage">
<h4>Diviner pyfunc usage<a class="headerlink" href="#diviner-pyfunc-usage" title="Permalink to this headline"> </a></h4>
<p>The MLflow Diviner flavor includes an implementation of the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> interface for Diviner models. To control
prediction behavior, you can specify configuration arguments in the first row of a Pandas DataFrame input.</p>
<p>As this configuration is dependent upon the underlying model type (i.e., the <code class="docutils literal notranslate"><span class="pre">diviner.GroupedProphet.forecast()</span></code>
method has a different signature than does <code class="docutils literal notranslate"><span class="pre">diviner.GroupedPmdarima.predict()</span></code>), the Diviner pyfunc implementation
attempts to coerce arguments to the types expected by the underlying model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Diviner models support both “full group” and “partial group” forecasting. If a column named <code class="docutils literal notranslate"><span class="pre">&quot;groups&quot;</span></code> is present
in the configuration <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> submitted to the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> flavor, the grouping key values in the first row
will be used to generate a subset of forecast predictions. This functionality removes the need to filter a subset
from the full output of all groups forecasts if the results of only a few (or one) groups are needed.</p>
</div>
<p>For a <code class="docutils literal notranslate"><span class="pre">GroupedPmdarima</span></code> model, an example configuration for the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pmdarima.arima.auto</span> <span class="kn">import</span> <span class="n">AutoARIMA</span>
<span class="kn">from</span> <span class="nn">diviner</span> <span class="kn">import</span> <span class="n">GroupedPmdarima</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoARIMA</span><span class="p">(</span><span class="n">out_of_sample_size</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GroupedPmdarima</span><span class="p">(</span><span class="n">model_template</span><span class="o">=</span><span class="n">base_model</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
        <span class="n">group_key_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">,</span> <span class="s2">&quot;city&quot;</span><span class="p">],</span>
        <span class="n">y_col</span><span class="o">=</span><span class="s2">&quot;watts&quot;</span><span class="p">,</span>
        <span class="n">datetime_col</span><span class="o">=</span><span class="s2">&quot;datetime&quot;</span><span class="p">,</span>
        <span class="n">silence_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">diviner</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">diviner_model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s2">&quot;/tmp/diviner_model&quot;</span><span class="p">)</span>

<span class="n">diviner_pyfunc</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="s2">&quot;/tmp/diviner_model&quot;</span><span class="p">)</span>

<span class="n">predict_conf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;n_periods&quot;</span><span class="p">:</span> <span class="mi">120</span><span class="p">,</span>
        <span class="s2">&quot;groups&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">(</span><span class="s2">&quot;US&quot;</span><span class="p">,</span> <span class="s2">&quot;NewYork&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">&quot;CA&quot;</span><span class="p">,</span> <span class="s2">&quot;Toronto&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">&quot;MX&quot;</span><span class="p">,</span> <span class="s2">&quot;MexicoCity&quot;</span><span class="p">),</span>
        <span class="p">],</span>  <span class="c1"># NB: List of tuples required.</span>
        <span class="s2">&quot;predict_col&quot;</span><span class="p">:</span> <span class="s2">&quot;wattage_forecast&quot;</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s2">&quot;return_conf_int&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;on_error&quot;</span><span class="p">:</span> <span class="s2">&quot;warn&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">subset_forecasts</span> <span class="o">=</span> <span class="n">diviner_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predict_conf</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are several instances in which a configuration <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> submitted to the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method
will cause an <code class="docutils literal notranslate"><span class="pre">MlflowException</span></code> to be raised:</p>
<blockquote>
<div><ul class="simple">
<li><p>If neither <code class="docutils literal notranslate"><span class="pre">horizon</span></code> or <code class="docutils literal notranslate"><span class="pre">n_periods</span></code> are provided.</p></li>
<li><p>The value of <code class="docutils literal notranslate"><span class="pre">n_periods</span></code> or <code class="docutils literal notranslate"><span class="pre">horizon</span></code> is not an integer.</p></li>
<li><p>If the model is of type <code class="docutils literal notranslate"><span class="pre">GroupedProphet</span></code>, <code class="docutils literal notranslate"><span class="pre">frequency</span></code> as a string type must be provided.</p></li>
<li><p>If both <code class="docutils literal notranslate"><span class="pre">horizon</span></code> and <code class="docutils literal notranslate"><span class="pre">n_periods</span></code> are provided with different values.</p></li>
</ul>
</div></blockquote>
</div>
</div>
</div>
<div class="section" id="transformers-transformers-experimental">
<h3><a class="toc-backref" href="#id64">Transformers (<code class="docutils literal notranslate"><span class="pre">transformers</span></code>) (Experimental)</a><a class="headerlink" href="#transformers-transformers-experimental" title="Permalink to this headline"> </a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new features are
subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model flavor enables logging of
<a class="reference external" href="https://huggingface.co/docs/transformers/index">transformers models, components, and pipelines</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> functions. Use of these
functions also adds the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be
interpreted as a generic Python function for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor in the native transformers formats.</p>
<div class="section" id="input-and-output-types-for-pyfunc">
<h4>Input and Output types for PyFunc<a class="headerlink" href="#input-and-output-types-for-pyfunc" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> <a class="reference internal" href="#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> simplifies
and standardizes both the inputs and outputs of pipeline inference. This conformity allows for serving
and batch inference by coercing the data structures that are required for <code class="docutils literal notranslate"><span class="pre">transformers</span></code> inference pipelines
to formats that are compatible with json serialization and casting to Pandas DataFrames.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain <cite>TextGenerationPipeline</cite> types, particularly instructional-based ones, may return the original
prompt and included line-formatting carriage returns <cite>“n”</cite> in their outputs. For these pipeline types,
if you would like to disable the prompt return, you can set the following in the <cite>model_config</cite> dictionary when
saving or logging the model: <cite>“include_prompt”: False</cite>. To remove the newline characters from within the body
of the generated text output, you can add the <cite>“collapse_whitespace”: True</cite> option to the <cite>model_config</cite> dictionary.
If the pipeline type being saved does not inherit from <cite>TextGenerationPipeline</cite>, these options will not perform
any modification to the output returned from pipeline inference.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Not all <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipeline types are supported. See the table below for the list of currently supported Pipeline
types that can be loaded as <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>.</p>
<p>In the current version, audio and text-based large language
models are supported for use with <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>, while computer vision, multi-modal, timeseries,
reinforcement learning, and graph models are only supported for native type loading via <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a></p>
<p>Future releases of MLflow will introduce <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> support for these additional types.</p>
</div>
<p>The table below shows the mapping of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipeline types to the <a class="reference internal" href="#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a>
data type inputs and outputs.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The inputs and outputs of the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> implementation of these pipelines <em>are not guaranteed to match</em> the input types and output types that would
return from a native use of a given pipeline type. If your use case requires access to scores, top_k results, or other additional references within
the output from a pipeline inference call, please use the native implementation by loading via <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> to
receive the full output.</p>
<p>Similarly, if your use case requires the use of raw tensor outputs or processing of outputs through an external <code class="docutils literal notranslate"><span class="pre">processor</span></code> module, load the
model components directly as a <code class="docutils literal notranslate"><span class="pre">dict</span></code> by calling <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.load_model" title="mlflow.transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.load_model()</span></code></a> and specify the <code class="docutils literal notranslate"><span class="pre">return_type</span></code> argument as ‘components’.</p>
</div>
<div class="section" id="supported-transformers-pipeline-types-for-pyfunc">
<h5>Supported transformers Pipeline types for Pyfunc<a class="headerlink" href="#supported-transformers-pipeline-types-for-pyfunc" title="Permalink to this headline"> </a></h5>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 22%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Pipeline Type</p></th>
<th class="head"><p>Input Type</p></th>
<th class="head"><p>Output Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Instructional Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Conversational</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Summarization</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Text Classification</p></td>
<td><p>str or List[str]</p></td>
<td><p>pd.DataFrame (dtypes: {‘label’: str, ‘score’: double})</p></td>
</tr>
<tr class="row-even"><td><p>Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Text2Text Generation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Token Classification</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Translation</p></td>
<td><p>str or List[str]</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>ZeroShot Classification*</p></td>
<td><p>Dict[str, [List[str] | str]*</p></td>
<td><p>pd.DataFrame (dtypes: {‘sequence’: str, ‘labels’: str, ‘scores’: double})</p></td>
</tr>
<tr class="row-odd"><td><p>Table Question Answering**</p></td>
<td><p>Dict[str, [List[str] | str]**</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Question Answering***</p></td>
<td><p>Dict[str, str]***</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-odd"><td><p>Fill Mask****</p></td>
<td><p>str or List[str]****</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>Feature Extraction</p></td>
<td><p>str or List[str]</p></td>
<td><p>np.ndarray</p></td>
</tr>
<tr class="row-odd"><td><p>AutomaticSpeechRecognition</p></td>
<td><p>bytes*****, str, or np.ndarray</p></td>
<td><p>List[str]</p></td>
</tr>
<tr class="row-even"><td><p>AudioClassification</p></td>
<td><p>bytes*****, str, or np.ndarray</p></td>
<td><p>pd.DataFrame (dtypes: {‘label’: str, ‘score’: double})</p></td>
</tr>
</tbody>
</table>
<p>* A collection of these inputs can also be passed. The standard required key names are ‘sequences’ and ‘candidate_labels’, but these may vary.
Check the input requirments for the architecture that you’re using to ensure that the correct dictionary key names are provided.</p>
<p>** A collection of these inputs can also be passed. The reference table must be a json encoded dict (i.e. {‘query’: ‘what did we sell most of?’, ‘table’: json.dumps(table_as_dict)})</p>
<p>*** A collection of these inputs can also be passed. The standard required key names are ‘question’ and ‘context’. Verify the expected input key names match the
expected input to the model to ensure your inference request can be read properly.</p>
<p>**** The mask syntax for the model that you’ve chosen is going to be specific to that model’s implementation. Some are ‘[MASK]’, while others are ‘&lt;mask&gt;’. Verify the expected syntax to
avoid failed inference requests.</p>
<p>***** If using <cite>pyfunc</cite> in MLflow Model Serving for realtime inference, the raw audio in bytes format must be base64 encoded prior to submitting to the endpoint. String inputs will be interpreted as uri locations.</p>
</div>
<div class="section" id="using-model-config-and-model-signature-params-for-transformers-inference">
<h5>Using model_config and model signature params for <cite>transformers</cite> inference<a class="headerlink" href="#using-model-config-and-model-signature-params-for-transformers-inference" title="Permalink to this headline"> </a></h5>
<p>For <cite>transformers</cite> inference, there are two ways to pass in additional arguments to the pipeline.</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">model_config</span></code> when saving/logging the model. Optionally, specify <code class="docutils literal notranslate"><span class="pre">model_config</span></code> when calling <code class="docutils literal notranslate"><span class="pre">load_model</span></code>.</p></li>
<li><p>Specify params at inference time when calling <code class="docutils literal notranslate"><span class="pre">predict()</span></code></p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">model_config</span></code> to control how the model is loaded and inference performed for all input samples. Configuration in
<code class="docutils literal notranslate"><span class="pre">model_config</span></code> is not overridable at <code class="docutils literal notranslate"><span class="pre">predict()</span></code> time unless a <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> is indicated with the same parameters.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with params schema, on the other hand, to allow downstream consumers to provide additional inference
params that may be needed to compute the predictions for their specific samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If both <code class="docutils literal notranslate"><span class="pre">model_config</span></code> and <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with parameters are saved when logging model, both of them
will be used for inference. The default parameters in <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> will override the params in <code class="docutils literal notranslate"><span class="pre">model_config</span></code>.
If extra <code class="docutils literal notranslate"><span class="pre">params</span></code> are provided at inference time, they take precedence over all params. We recommend using
<code class="docutils literal notranslate"><span class="pre">model_config</span></code> for those parameters needed to run the model in general for all the samples. Then, add
<code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> with parameters for those extra parameters that you want downstream consumers to indicated at
per each of the samples.</p>
</div>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">model_config</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;mrm8488/t5-base-finetuned-common_gen&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;pencil draw paper&quot;</span>

<span class="c1"># Infer the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Define an model_config</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;remove_invalid_values&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Saving model_config with the model</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;text2text&quot;</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;text2text&quot;</span><span class="p">)</span>
<span class="c1"># model_config will be applied</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pyfunc_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># overriding some inference configuration with diferent values</span>
<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="s2">&quot;text2text&quot;</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that in the previous example, the user can’t override the configuration <code class="docutils literal notranslate"><span class="pre">do_sample</span></code>
when calling <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p>
</div>
<ul class="simple">
<li><p>Specifying params at inference time</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;mrm8488/t5-base-finetuned-common_gen&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text2text-generation&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;pencil draw paper&quot;</span>

<span class="c1"># Define an model_config</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;remove_invalid_values&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Define the inference parameters params</span>
<span class="n">inference_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Infer the signature including params</span>
<span class="n">signature_with_params</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">),</span>
    <span class="n">params</span><span class="o">=</span><span class="n">inference_params</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Saving model with signature and model config</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;text2text&quot;</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="n">signature_with_params</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pyfunc_loaded</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;text2text&quot;</span><span class="p">)</span>

<span class="c1"># Pass params at inference time</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># In this case we only override max_length and do_sample,</span>
<span class="c1"># other params will use the default one saved on ModelSignature</span>
<span class="c1"># or in the model configuration.</span>
<span class="c1"># The final params used for prediction is as follows:</span>
<span class="c1"># {</span>
<span class="c1">#    &quot;num_beams&quot;: 5,</span>
<span class="c1">#    &quot;max_length&quot;: 20,</span>
<span class="c1">#    &quot;do_sample&quot;: False,</span>
<span class="c1">#    &quot;remove_invalid_values&quot;: True,</span>
<span class="c1"># }</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pyfunc_loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="example-of-loading-a-transformers-model-as-a-python-function">
<h5>Example of loading a transformers model as a python function<a class="headerlink" href="#example-of-loading-a-transformers-model-as-a-python-function" title="Permalink to this headline"> </a></h5>
<p>In the below example, a simple pre-trained model is used within a pipeline. After logging to MLflow, the pipeline is
loaded as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> and used to generate a response from a passed-in list of strings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="c1"># Read a pre-trained conversation pipeline from HuggingFace hub</span>
<span class="n">conversational_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;microsoft/DialoGPT-medium&quot;</span><span class="p">)</span>

<span class="c1"># Define the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="s2">&quot;Hi there, chatbot!&quot;</span><span class="p">,</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generate_signature_output</span><span class="p">(</span>
        <span class="n">conversational_pipeline</span><span class="p">,</span> <span class="s2">&quot;Hi there, chatbot!&quot;</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Log the pipeline</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">conversational_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s2">&quot;conversational&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="s2">&quot;A clever and witty question&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the saved pipeline as pyfunc</span>
<span class="n">chatbot</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Ask the chatbot a question</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;What is machine learning?&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [It&#39;s a new thing that&#39;s been around for a while.]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="save-and-load-options-for-transformers">
<h4>Save and Load options for transformers<a class="headerlink" href="#save-and-load-options-for-transformers" title="Permalink to this headline"> </a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor for MLflow provides support for saving either components of a model or a pipeline object that contains the customized components in
an easy to use interface that is optimized for inference.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLflow by default uses a 500 MB <cite>max_shard_size</cite> to save the model object in <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> or <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> APIs. You can use the environment variable <cite>MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE</cite> to override the value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For component-based logging, the only requirement that must be met in the submitted <code class="docutils literal notranslate"><span class="pre">dict</span></code> is that a model is provided. All other elements of the <code class="docutils literal notranslate"><span class="pre">dict</span></code> are optional.</p>
</div>
<div class="section" id="logging-a-components-based-model">
<h5>Logging a components-based model<a class="headerlink" href="#logging-a-components-based-model" title="Permalink to this headline"> </a></h5>
<p>The example below shows logging components of a <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model via a dictionary mapping of specific named components. The names of the keys within the submitted dictionary
must be in the set: <code class="docutils literal notranslate"><span class="pre">{&quot;model&quot;,</span> <span class="pre">&quot;tokenizer&quot;,</span> <span class="pre">&quot;feature_extractor&quot;,</span> <span class="pre">&quot;image_processor&quot;}</span></code>. Processor type objects (some image processors, audio processors, and multi-modal processors)
must be saved explicitly with the <code class="docutils literal notranslate"><span class="pre">processor</span></code> argument in the <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.save_model" title="mlflow.transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.save_model()</span></code></a> or <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.log_model" title="mlflow.transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.log_model()</span></code></a> APIs.</p>
<p>After logging, the components are automatically inserted into the appropriate <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> type for the task being performed and are returned, ready for inference.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The components that are logged can be retrieved in their original structure (a dictionary) by setting the attribute <code class="docutils literal notranslate"><span class="pre">return_type</span></code> to “components” in the <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> API.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Not all model types are compatible with the pipeline API constructor via component elements. Incompatible models will raise an
<code class="docutils literal notranslate"><span class="pre">MLflowException</span></code> error stating that the model is missing the <cite>name_or_path</cite> attribute. In
the event that this occurs, please construct the model directly via the <code class="docutils literal notranslate"><span class="pre">transformers.pipeline(&lt;repo</span> <span class="pre">name&gt;)</span></code> API and save the pipeline object directly.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;text-classification&quot;</span>
<span class="n">architecture</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span>

<span class="c1"># Define the components of the model in a dictionary</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">}</span>

<span class="c1"># Log the model components</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;text_classifier&quot;</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load the components as a pipeline</span>
<span class="n">loaded_pipeline</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">loaded_pipeline</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="c1"># &gt;&gt; TextClassificationPipeline</span>

<span class="n">loaded_pipeline</span><span class="p">([</span><span class="s2">&quot;MLflow is awesome!&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformers is a great library!&quot;</span><span class="p">])</span>

<span class="c1"># &gt;&gt; [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998478889465332},</span>
<span class="c1"># &gt;&gt;  {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998030066490173}]</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-a-pipeline-and-loading-components">
<h5>Saving a pipeline and loading components<a class="headerlink" href="#saving-a-pipeline-and-loading-components" title="Permalink to this headline"> </a></h5>
<p>Some use cases can benefit from the simplicity of defining a solution as a pipeline, but need the component-level access for performing a micro-services based deployment strategy
where pre / post-processing is performed on containers that do not house the model itself. For this paradigm, a pipeline can be loaded as its constituent parts, as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">translation_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;translation_en_to_fr&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">translation_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;french_translator&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">translation_components</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;components&quot;</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">translation_components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># &gt;&gt; task -&gt; str</span>
<span class="c1"># &gt;&gt; model -&gt; T5ForConditionalGeneration</span>
<span class="c1"># &gt;&gt; tokenizer -&gt; T5TokenizerFast</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">translation_pipeline</span><span class="p">(</span><span class="s2">&quot;MLflow is great!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [{&#39;translation_text&#39;: &#39;MLflow est formidable!&#39;}]</span>

<span class="n">reconstructed_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="o">**</span><span class="n">translation_components</span><span class="p">)</span>

<span class="n">reconstructed_response</span> <span class="o">=</span> <span class="n">reconstructed_pipeline</span><span class="p">(</span>
    <span class="s2">&quot;transformers makes using Deep Learning models easy and fun!&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">reconstructed_response</span><span class="p">)</span>

<span class="c1"># &gt;&gt; [{&#39;translation_text&#39;: &quot;Les transformateurs rendent l&#39;utilisation de modèles Deep Learning facile et amusante!&quot;}]</span>
</pre></div>
</div>
</div>
<div class="section" id="automatic-metadata-and-modelcard-logging">
<h5>Automatic Metadata and ModelCard logging<a class="headerlink" href="#automatic-metadata-and-modelcard-logging" title="Permalink to this headline"> </a></h5>
<p>In order to provide as much information as possible for saved models, the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor will automatically fetch the <code class="docutils literal notranslate"><span class="pre">ModelCard</span></code> for any model or pipeline that
is saved that has a stored card on the HuggingFace hub. This card will be logged as part of the model artifact, viewable at the same directory level as the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file and
the stored model object.</p>
<p>In addition to the <code class="docutils literal notranslate"><span class="pre">ModelCard</span></code>, the components that comprise any Pipeline (or the individual components if saving a dictionary of named components) will have their source types
stored. The model type, pipeline type, task, and classes of any supplementary component (such as a <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">ImageProcessor</span></code>) will be stored in the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file as well.</p>
</div>
<div class="section" id="automatic-signature-inference">
<h5>Automatic Signature inference<a class="headerlink" href="#automatic-signature-inference" title="Permalink to this headline"> </a></h5>
<p>For pipelines that support <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code>, there are 3 means of attaching a model signature to the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file.</p>
<ul class="simple">
<li><p>Provide a model signature explicitly via setting a valid <code class="docutils literal notranslate"><span class="pre">ModelSignature</span></code> to the <code class="docutils literal notranslate"><span class="pre">signature</span></code> attribute. This can be generated via the helper utility <a class="reference internal" href="python_api/mlflow.transformers.html#mlflow.transformers.generate_signature_output" title="mlflow.transformers.generate_signature_output"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.transformers.generate_signature_output()</span></code></a></p></li>
<li><p>Provide an <code class="docutils literal notranslate"><span class="pre">input_example</span></code>. The signature will be inferred and validated that it matches the appropriate input type. The output type will be validated by performing inference automatically (if the model is a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> supported type).</p></li>
<li><p>Do nothing. The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> flavor will automatically apply the appropriate general signature that the pipeline type supports (only for a single-entity; collections will not be inferred).</p></li>
</ul>
</div>
<div class="section" id="scalability-for-inference">
<h5>Scalability for inference<a class="headerlink" href="#scalability-for-inference" title="Permalink to this headline"> </a></h5>
<p>A common configuration for lowering the total memory pressure for pytorch models within <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipelines is to modify the
processing data type. This is achieved through setting the <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> argument when creating a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>.
For a full reference of these tunable arguments for configuration of pipelines, see the <a class="reference external" href="https://huggingface.co/docs/transformers/v4.28.1/en/perf_train_gpu_one#floating-data-types">training docs</a> .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature does not exist in versions of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> &lt; 4.26.x</p>
</div>
<p>In order to apply these configurations to a saved or logged run, there are two options:</p>
<ul class="simple">
<li><p>Save a pipeline with the <cite>torch_dtype</cite> argument set to the encoding type of your choice.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;translation_en_to_fr&quot;</span>

<span class="n">my_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
    <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_pipeline&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Illustrate that the torch data type is recorded in the flavor configuration</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_info</span><span class="o">.</span><span class="n">flavors</span><span class="p">[</span><span class="s2">&quot;transformers&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span><span class="s1">&#39;transformers_version&#39;</span>:<span class="w"> </span><span class="s1">&#39;4.28.1&#39;</span>,
<span class="w"> </span><span class="s1">&#39;code&#39;</span>:<span class="w"> </span>None,
<span class="w"> </span><span class="s1">&#39;task&#39;</span>:<span class="w"> </span><span class="s1">&#39;translation_en_to_fr&#39;</span>,
<span class="w"> </span><span class="s1">&#39;instance_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;TranslationPipeline&#39;</span>,
<span class="w"> </span><span class="s1">&#39;source_model_name&#39;</span>:<span class="w"> </span><span class="s1">&#39;t5-small&#39;</span>,
<span class="w"> </span><span class="s1">&#39;pipeline_model_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;T5ForConditionalGeneration&#39;</span>,
<span class="w"> </span><span class="s1">&#39;framework&#39;</span>:<span class="w"> </span><span class="s1">&#39;pt&#39;</span>,
<span class="w"> </span><span class="s1">&#39;torch_dtype&#39;</span>:<span class="w"> </span><span class="s1">&#39;torch.bfloat16&#39;</span>,
<span class="w"> </span><span class="s1">&#39;tokenizer_type&#39;</span>:<span class="w"> </span><span class="s1">&#39;T5TokenizerFast&#39;</span>,
<span class="w"> </span><span class="s1">&#39;components&#39;</span>:<span class="w"> </span><span class="o">[</span><span class="s1">&#39;tokenizer&#39;</span><span class="o">]</span>,
<span class="w"> </span><span class="s1">&#39;pipeline&#39;</span>:<span class="w"> </span><span class="s1">&#39;pipeline&#39;</span><span class="o">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Specify the <cite>torch_dtype</cite> argument when loading the model to override any values set during logging or saving.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;translation_en_to_fr&quot;</span>

<span class="n">my_pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">T5TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;t5-small&quot;</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">),</span>
    <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_pipeline&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">loaded_pipeline</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="n">model_info</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;pipeline&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loaded_pipeline</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torch.float64
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Logging or saving a model in ‘components’ mode (using a dictionary to declare components) does not support setting the data type for a constructed pipeline.
If you need to override the default behavior of how data is encoded, please save or log a <cite>pipeline</cite> object.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Overriding the data type for a pipeline when loading as a <a class="reference internal" href="#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> is not supported.
The value set for <code class="docutils literal notranslate"><span class="pre">torch_dtype</span></code> during <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> or <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> will persist when loading as <cite>pyfunc</cite>.</p>
</div>
</div>
<div class="section" id="input-data-types-for-audio-pipelines">
<h5>Input data types for audio pipelines<a class="headerlink" href="#input-data-types-for-audio-pipelines" title="Permalink to this headline"> </a></h5>
<p>Note that passing raw data to an audio pipeline (raw bytes) requires two separate elements of the same effective library.
In order to use the bitrate transposition and conversion of the audio bytes data into numpy nd.array format, the library <cite>ffmpeg</cite> is required.
Installing this package directly from pypi (<cite>pip install ffmpeg</cite>) does not install the underlying <cite>c</cite> dll’s that are required to make <cite>ffmpeg</cite> function.
Please consult with the documentation at <a class="reference external" href="https://ffmpeg.org/download.html">the ffmpeg website</a> for guidance on your given operating system.</p>
<p>The Audio Pipeline types, when loaded as a <a class="reference internal" href="#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> have three input types available:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
</ul>
<p>The string input type is meant for blob references (uri locations) that are accessible to the instance of the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model.
This input mode is useful when doing large batch processing of audio inference in Spark due to the inherent limitations of handling large <code class="docutils literal notranslate"><span class="pre">bytes</span></code>
data in <code class="docutils literal notranslate"><span class="pre">Spark</span></code> <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>. Ensure that you have <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> installed in the environment that the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> model is running in order
to use <code class="docutils literal notranslate"><span class="pre">str</span></code> input uri-based inference. If this package is not properly installed (both from <code class="docutils literal notranslate"><span class="pre">pypi</span></code> and from the <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> binaries), an Exception
will be thrown at inference time.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If using a uri (<cite>str</cite>) as an input type for a <cite>pyfunc</cite> model that you are intending to host for realtime inference through the <cite>MLflow Model Server</cite>,
you <em>must</em> specify a custom model signature when logging or saving the model.
The default signature input value type of <code class="docutils literal notranslate"><span class="pre">bytes</span></code> will, in <cite>MLflow Model serving</cite>, force the conversion of the uri string to <code class="docutils literal notranslate"><span class="pre">bytes</span></code>, which will cause an Exception
to be thrown from the serving process stating that the soundfile is corrupt.</p>
</div>
<p>An example of specifying an appropriate uri-based input model signature for an audio model is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.transformers</span> <span class="kn">import</span> <span class="n">generate_signature_output</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.mywebsite.com/sound/files/for/transcription/file111.mp3&quot;</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">generate_signature_output</span><span class="p">(</span><span class="n">my_audio_pipeline</span><span class="p">,</span> <span class="n">url</span><span class="p">))</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">transformers_model</span><span class="o">=</span><span class="n">my_audio_pipeline</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;my_transcriber&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">bytes</span></code></p></li>
</ul>
<p>This is the default serialization format of audio files. It is the easiest format to utilize due to the fact that
Pipeline implementations will automatically convert the audio bitrate from the file with the use of <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> (a required dependency if using this format) to the bitrate required by the underlying model within the <cite>Pipeline</cite>.
When using the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> representation of the pipeline directly (not through serving), the sound file can be passed directly as <code class="docutils literal notranslate"><span class="pre">bytes</span></code> without any
modification. When used through serving, the <code class="docutils literal notranslate"><span class="pre">bytes</span></code> data <em>must be</em> base64 encoded.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code></p></li>
</ul>
<p>This input format requires that both the bitrate has been set prior to conversion to <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> (i.e., through the use of a package like
<code class="docutils literal notranslate"><span class="pre">librosa</span></code> or <code class="docutils literal notranslate"><span class="pre">pydub</span></code>) and that the model has been saved with a signature that uses the <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format for the input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Audio models being used for serving that intend to utilize pre-formatted audio in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format
must have the model saved with a signature configuration that reflects this schema. Failure to do so will result in type casting errors due to the default signature for
audio transformers pipelines being set as expecting <code class="docutils literal notranslate"><span class="pre">binary</span></code> (<code class="docutils literal notranslate"><span class="pre">bytes</span></code>) data. The serving endpoint cannot accept a union of types, so a particular model instance must choose one
or the other as an allowed input type.</p>
</div>
</div>
</div>
</div>
<div class="section" id="sentencetransformers-sentence-transformers-experimental">
<h3><a class="toc-backref" href="#id65">SentenceTransformers (<code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code>) (Experimental)</a><a class="headerlink" href="#sentencetransformers-sentence-transformers-experimental" title="Permalink to this headline"> </a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> flavor is in active development and is marked as Experimental. Public APIs may change and new
features are subject to be added as additional functionality is brought to the flavor.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> model flavor enables logging of
<a class="reference external" href="https://www.sbert.net/docs/pretrained_models.html">sentence-transformers models</a> in MLflow format via
the <a class="reference internal" href="python_api/mlflow.sentence_transformers.html#mlflow.sentence_transformers.save_model" title="mlflow.sentence_transformers.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sentence_transformers.save_model()</span></code></a> and <a class="reference internal" href="python_api/mlflow.sentence_transformers.html#mlflow.sentence_transformers.log_model" title="mlflow.sentence_transformers.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sentence_transformers.log_model()</span></code></a> functions.
Use of these functions also adds the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor to the MLflow Models that they produce, allowing the model to be
interpreted as a generic Python function for inference via <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a>.
You can also use the <a class="reference internal" href="python_api/mlflow.sentence_transformers.html#mlflow.sentence_transformers.load_model" title="mlflow.sentence_transformers.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.sentence_transformers.load_model()</span></code></a> function to load a saved or logged MLflow
Model with the <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> flavor as a native <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> model.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">mlflow.sentence_transformers</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>

<span class="n">example_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This is a sentence.&quot;</span><span class="p">,</span> <span class="s2">&quot;This is another sentence.&quot;</span><span class="p">]</span>

<span class="c1"># Define the signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_signature</span><span class="p">(</span>
    <span class="n">model_input</span><span class="o">=</span><span class="n">example_sentences</span><span class="p">,</span>
    <span class="n">model_output</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">example_sentences</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Log the model using mlflow</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sentence_transformers</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;sbert_model&quot;</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">example_sentences</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Load option 1: mlflow.pyfunc.load_model returns a PyFuncModel</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">embeddings1</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="s2">&quot;hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;i am mlflow&quot;</span><span class="p">])</span>

<span class="c1"># Load option 2: mlflow.sentence_transformers.load_model returns a SentenceTransformer</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sentence_transformers</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">embeddings2</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">encode</span><span class="p">([</span><span class="s2">&quot;hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;i am mlflow&quot;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">embeddings1</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&gt;&gt; [[-3.44772562e-02  3.10232025e-02  6.73496164e-03  2.61089969e-02</span>
<span class="sd">  ...</span>
<span class="sd">  2.37922110e-02 -2.28897743e-02  3.89375277e-02  3.02067865e-02]</span>
<span class="sd"> [ 4.81191138e-03 -9.33756605e-02  6.95968643e-02  8.09735525e-03</span>
<span class="sd">  ...</span>
<span class="sd">   6.57437667e-02 -2.72239652e-02  4.02687863e-02 -1.05599344e-01]]</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-evaluation">
<span id="id13"></span><h2><a class="toc-backref" href="#id36">Model Evaluation</a><a class="headerlink" href="#model-evaluation" title="Permalink to this headline"> </a></h2>
<p>After building and training your MLflow Model, you can use the <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> API to
evaluate its performance on one or more datasets of your choosing. <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>
currently supports evaluation of MLflow Models with the
<a class="reference internal" href="#pyfunc-model-flavor"><span class="std std-ref">python_function (pyfunc) model flavor</span></a> for classification, regression, and numerous language modeling tasks (see <a class="reference internal" href="#model-evaluation-llms"><span class="std std-ref">Evaluating with LLMs</span></a>), computing a variety of
task-specific performance metrics, model performance plots, and
model explanations. Evaluation results are logged to <a class="reference internal" href="tracking.html#tracking"><span class="std std-ref">MLflow Tracking</span></a>.</p>
<p>The following <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_on_binary_classifier.py">example from the MLflow GitHub Repository</a>
uses <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to evaluate the performance of a classifier
on the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/adult">UCI Adult Data Set</a>, logging a
comprehensive collection of MLflow Metrics and Artifacts that provide insight into model performance
and behavior:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span>
<span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Load the UCI Adult Dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">adult</span><span class="p">()</span>

<span class="c1"># Split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Fit an XGBoost binary classifier on the training data split</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Create a model signature</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="c1"># Build the Evaluation Dataset from the test set</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">X_test</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="c1"># Log the baseline model to MLflow</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">model_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">get_artifact_uri</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>

    <span class="c1"># Evaluate the logged model</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model_uri</span><span class="p">,</span>
        <span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classifier&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="_images/model_evaluation_metrics.png"><img alt="eval_metrics_img" src="_images/model_evaluation_metrics.png" style="width: 15%;" /></a> <a class="reference internal" href="_images/model_evaluation_feature_importance.png"><img alt="eval_importance_img" src="_images/model_evaluation_feature_importance.png" style="width: 84%;" /></a></p>
<div class="section" id="evaluating-with-llms">
<span id="model-evaluation-llms"></span><h3>Evaluating with LLMs<a class="headerlink" href="#evaluating-with-llms" title="Permalink to this headline"> </a></h3>
<p>As of MLflow 2.4.0, <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> has built-in support for a variety of tasks with
LLMs, including text summarization, text classification, question answering, and text generation.
The following example uses <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to evaluate a model that answers
questions about MLflow (note that you must have the <code class="docutils literal notranslate"><span class="pre">OPENAI_API_TOKEN</span></code> environment variable set
in your current system environment in order to run the example):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">openai</span>

<span class="c1"># Create a question answering model using prompt engineering with OpenAI. Log the</span>
<span class="c1"># prompt and the model to MLflow Tracking</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;Your job is to answer questions about MLflow. When you are asked a question about MLflow,&quot;</span>
    <span class="s2">&quot; respond to it. Make sure to include code examples. If the question is not related to&quot;</span>
    <span class="s2">&quot; MLflow, refuse to answer and say that the question is unrelated.&quot;</span>
<span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;system_prompt&quot;</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">)</span>
<span class="n">logged_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span>
    <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">},</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Evaluate the model on some example questions</span>
<span class="n">questions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;How do you create a run with MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;How do you log a model with MLflow?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">logged_model</span><span class="o">.</span><span class="n">model_uri</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">questions</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load and inspect the evaluation results</span>
<span class="n">results</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">load_table</span><span class="p">(</span>
    <span class="s2">&quot;eval_results_table.json&quot;</span><span class="p">,</span> <span class="n">extra_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;run_id&quot;</span><span class="p">,</span> <span class="s2">&quot;params.system_prompt&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluation results:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<p>MLflow also provides an Artifact View UI for comparing inputs and outputs across multiple models
built with LLMs. For example, after evaluating multiple prompts for question answering
(see the
<a class="reference external" href="https://github.com/mlflow/mlflow/tree/master/examples/llms/question_answering/question_answering.py">MLflow OpenAI question answering full example</a>), you can navigate to the Artifact View to view the questions and compare the answers for
each model:</p>
<img alt="_images/artifact-view-ui.png" src="_images/artifact-view-ui.png" />
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<p>For additional examples demonstrating the use of <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> with LLMs, check out the
<a class="reference external" href="https://github.com/mlflow/mlflow/tree/master/examples/llms">MLflow LLMs example repository</a>.</p>
</div>
<div class="section" id="evaluating-with-extra-metrics">
<h3>Evaluating with Extra Metrics<a class="headerlink" href="#evaluating-with-extra-metrics" title="Permalink to this headline"> </a></h3>
<p>If the default set of metrics is insufficient, you can supply <code class="docutils literal notranslate"><span class="pre">extra_metrics</span></code> and <code class="docutils literal notranslate"><span class="pre">custom_artifacts</span></code>
to <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to produce extra metrics and artifacts for the model(s) that you’re evaluating.</p>
<p>To define an extra metric, you should define an <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> function that takes in <code class="docutils literal notranslate"><span class="pre">predictions</span></code>, <code class="docutils literal notranslate"><span class="pre">targets</span></code>,
and <code class="docutils literal notranslate"><span class="pre">metrics</span></code> as inputs and outputs a <code class="docutils literal notranslate"><span class="pre">MetricValue</span></code> object. <code class="docutils literal notranslate"><span class="pre">predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">targets</span></code> are <code class="docutils literal notranslate"><span class="pre">pandas.Series</span></code>
objects. If <code class="docutils literal notranslate"><span class="pre">predictions</span></code> or <code class="docutils literal notranslate"><span class="pre">targets</span></code> specified in <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> is either <code class="docutils literal notranslate"><span class="pre">numpy.array</span></code> or <code class="docutils literal notranslate"><span class="pre">List</span></code>,
they will be converted to <code class="docutils literal notranslate"><span class="pre">pandas.Series</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">metrics</span></code> is a dictionary mapping a metric name <code class="docutils literal notranslate"><span class="pre">string</span></code> to a <code class="docutils literal notranslate"><span class="pre">MetricValue</span></code> object. It contains the values
from built-in metrics and can be used to compute your custom metric. The built-in metrics are available when
<code class="docutils literal notranslate"><span class="pre">model_type</span></code> is defined for <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate(...</span> <span class="pre">model_type=&quot;classifier&quot;)</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;accuracy_score&quot;</span><span class="p">:</span> <span class="n">MetricValue</span><span class="p">(</span>
        <span class="n">scores</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">justifications</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aggregate_results</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;accuracy_score&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span>
    <span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">MetricValue</span></code> class has three attributes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scores</span></code>: a list that contains per-row metrics.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aggregate_results</span></code>: a dictionary that maps the aggregation method names to the corresponding aggregated values. This is intended to be used to aggregate <code class="docutils literal notranslate"><span class="pre">scores</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">justification</span></code>: a per-row justification of the values in <code class="docutils literal notranslate"><span class="pre">scores</span></code>. This is optional, and is usually used with genai metrics.</p></li>
</ul>
<p>The code block below demonstrates how to define a custom metric evaluation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.metrics</span> <span class="kn">import</span> <span class="n">MetricValue</span>


<span class="k">def</span> <span class="nf">my_metric_eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span>
        <span class="n">scores</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
        <span class="n">aggregate_results</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
            <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
            <span class="s2">&quot;median&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Once you have defined an <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code>, you then use <code class="docutils literal notranslate"><span class="pre">make_metric()</span></code> to wrap this <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> function into a metric.
In addition to <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code>, <code class="docutils literal notranslate"><span class="pre">make_metric()</span></code> requires an additional parameter , <code class="docutils literal notranslate"><span class="pre">greater_is_better</span></code>, for optimization purposes. This parameter
indicates whether this is a metric we want to maximize or minimize.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlflow.metrics</span> <span class="kn">import</span> <span class="n">make_metric</span>

<span class="n">mymetric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span><span class="n">eval_fn</span><span class="o">=</span><span class="n">my_metric_eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The extra metric allows you to either evaluate a model directly, or to evaluate an output dataframe.</p>
<p>To evaluate the model directly, you will have to provide <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> either a pyfunc model
instance, a URI referring to a pyfunc model, or a callable function that takes in the data as input
and outputs the predictions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span>


<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mymetric</span><span class="p">])</span>
</pre></div>
</div>
<dl class="simple">
<dt>To directly evaluate an output dataframe, you can <strong>omit</strong> the <code class="docutils literal notranslate"><span class="pre">model</span></code> parameter. However, you will need</dt><dd><p>to set the <code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code> in order to evaluate an inference output dataframe.</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
        <span class="s2">&quot;predictions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;targets&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mymetric</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When your model has multiple outputs, the model must return a pandas DataFrame with multiple columns. You must
specify one column among the model output columns as the predictions column using the <code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter,
and other output columns of the model will be accessible from the <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> based on their column names. For example, if
your model has two outputs <code class="docutils literal notranslate"><span class="pre">retrieved_context</span></code> and <code class="docutils literal notranslate"><span class="pre">answer</span></code>, you can specify <code class="docutils literal notranslate"><span class="pre">answer</span></code> as the predictions
column, and <code class="docutils literal notranslate"><span class="pre">retrieved_context</span></code> column will be accessible as the <code class="docutils literal notranslate"><span class="pre">context</span></code> parameter from <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> via <code class="docutils literal notranslate"><span class="pre">col_mapping</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span> <span class="o">+</span> <span class="n">context</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span>
        <span class="n">scores</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
        <span class="n">aggregate_results</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)},</span>
    <span class="p">)</span>


<span class="n">mymetric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span><span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mymetric&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;retrieved_context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]})</span>


<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;col_mapping&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;retrieved_context&quot;</span><span class="p">}}</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;targets&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mymetric</span><span class="p">],</span>
    <span class="n">evaluator_config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>However, you can also avoid using <code class="docutils literal notranslate"><span class="pre">col_mapping</span></code> if the parameter of <code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> is the same as the output column name of the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">retrieved_context</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span> <span class="o">+</span> <span class="n">retrieved_context</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span>
        <span class="n">scores</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span>
        <span class="n">aggregate_results</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)},</span>
    <span class="p">)</span>


<span class="n">mymetric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span><span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mymetric&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;retrieved_context&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]})</span>


<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="p">,</span>
    <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;answer&quot;</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;targets&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">mymetric</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">col_mapping</span></code> also allows you to pass additional parameters to the extra metric function, in this case passing a value <code class="docutils literal notranslate"><span class="pre">k</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MetricValue</span><span class="p">(</span><span class="n">scores</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">aggregate_results</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)})</span>


<span class="n">weighted_mymetric</span> <span class="o">=</span> <span class="n">make_metric</span><span class="p">(</span><span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span>


<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;targets&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;col_mapping&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}}</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;targets&quot;</span><span class="p">,</span>
    <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">weighted_mymetric</span><span class="p">],</span>
    <span class="n">evaluator_config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The following <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_with_custom_metrics.py">short example from the MLflow GitHub Repository</a>
uses <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> with an extra metric function to evaluate the performance of a regressor on the
<a class="reference external" href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html">California Housing Dataset</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span><span class="p">,</span> <span class="n">make_metric</span>

<span class="c1"># loading the California housing dataset</span>
<span class="n">cali_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># split the dataset into train and test partitions</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cali_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cali_housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>

<span class="c1"># train the model</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Infer model signature</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># creating the evaluation dataframe</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>


<span class="k">def</span> <span class="nf">squared_diff_plus_one</span><span class="p">(</span><span class="n">eval_df</span><span class="p">,</span> <span class="n">_builtin_metrics</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This example custom metric function creates a metric based on the ``prediction`` and</span>
<span class="sd">    ``target`` columns in ``eval_df`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eval_df</span><span class="p">[</span><span class="s2">&quot;prediction&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">eval_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sum_on_target_divided_by_two</span><span class="p">(</span><span class="n">_eval_df</span><span class="p">,</span> <span class="n">builtin_metrics</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This example custom metric function creates a metric derived from existing metrics in</span>
<span class="sd">    ``builtin_metrics``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">builtin_metrics</span><span class="p">[</span><span class="s2">&quot;sum_on_target&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">prediction_target_scatter</span><span class="p">(</span><span class="n">eval_df</span><span class="p">,</span> <span class="n">_builtin_metrics</span><span class="p">,</span> <span class="n">artifacts_dir</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This example custom artifact generates and saves a scatter plot to ``artifacts_dir`` that</span>
<span class="sd">    visualizes the relationship between the predictions and targets for the given model to a</span>
<span class="sd">    file as an image artifact.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">eval_df</span><span class="p">[</span><span class="s2">&quot;prediction&quot;</span><span class="p">],</span> <span class="n">eval_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Targets&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Predictions&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Targets vs. Predictions&quot;</span><span class="p">)</span>
    <span class="n">plot_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">artifacts_dir</span><span class="p">,</span> <span class="s2">&quot;example_scatter_plot.png&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">plot_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;example_scatter_plot_artifact&quot;</span><span class="p">:</span> <span class="n">plot_path</span><span class="p">}</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">lin_reg</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">)</span>
    <span class="n">model_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">get_artifact_uri</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;regressor&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">],</span>
        <span class="n">extra_metrics</span><span class="o">=</span><span class="p">[</span>
            <span class="n">make_metric</span><span class="p">(</span>
                <span class="n">eval_fn</span><span class="o">=</span><span class="n">squared_diff_plus_one</span><span class="p">,</span>
                <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">make_metric</span><span class="p">(</span>
                <span class="n">eval_fn</span><span class="o">=</span><span class="n">sum_on_target_divided_by_two</span><span class="p">,</span>
                <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">],</span>
        <span class="n">custom_artifacts</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_target_scatter</span><span class="p">],</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metrics:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;artifacts:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">artifacts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For a more comprehensive extra metrics usage example, refer to <a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_with_custom_metrics_comprehensive.py">this example from the MLflow GitHub Repository</a>.</p>
</div>
<div class="section" id="evaluating-with-a-function">
<h3>Evaluating with a Function<a class="headerlink" href="#evaluating-with-a-function" title="Permalink to this headline"> </a></h3>
<p>As of MLflow 2.8.0, <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> supports evaluating a python function without requiring
logging the model to MLflow. This is useful when you don’t want to log the model and just want to evaluate
it. The requirements for the function’s input and output are the same as the requirements for a model’s input and
output.</p>
<p>The following example uses <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to evaluate a function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">import</span> <span class="nn">xgboost</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Load the UCI Adult Dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">adult</span><span class="p">()</span>

<span class="c1"># Split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit an XGBoost binary classifier on the training data split</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Build the Evaluation Dataset from the test set</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">X_test</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>


<span class="c1"># Define a function that calls the model&#39;s predict method</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="c1"># Evaluate the function without logging the model</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">fn</span><span class="p">,</span>
        <span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classifier&quot;</span><span class="p">,</span>
        <span class="n">evaluators</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">],</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metrics:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;artifacts:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">artifacts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="evaluating-with-a-static-dataset">
<h3>Evaluating with a Static Dataset<a class="headerlink" href="#evaluating-with-a-static-dataset" title="Permalink to this headline"> </a></h3>
<p>As of MLflow 2.8.0, <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> supports evaluating a static dataset without specifying a model.
This is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and
want to evaluate the static dataset without re-running the model.</p>
<p>If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the
top-level <code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter in <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume that the model output is saved to the pandas_df[&quot;model_output&quot;] column</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">pandas_df</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;model_output&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>If you are using an MLflow PandasDataset, you must specify the column name that contains the model output using
the <code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter in <a class="reference internal" href="python_api/mlflow.data.html#mlflow.data.from_pandas" title="mlflow.data.from_pandas"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.data.from_pandas()</span></code></a>, and specify <code class="docutils literal notranslate"><span class="pre">None</span></code> for the
<code class="docutils literal notranslate"><span class="pre">predictions</span></code> parameter in <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume that the model output is saved to the pandas_df[&quot;model_output&quot;] column</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;model_output&quot;</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">pandas_df</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>When your model has multiple outputs, you must specify one column among the model output columns as the predictions
column. The other output columns of the model will be treated as “input” columns. For example, if your model
has two outputs named <code class="docutils literal notranslate"><span class="pre">retrieved_context</span></code> and <code class="docutils literal notranslate"><span class="pre">answer</span></code>, you can specify <code class="docutils literal notranslate"><span class="pre">answer</span></code> as the predictions column. The
<code class="docutils literal notranslate"><span class="pre">retrieved_context</span></code> column will be treated as an “input” column when calculating the metrics.</p>
<p>The following example uses <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to evaluate a static dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">import</span> <span class="nn">xgboost</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="c1"># Load the UCI Adult Dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">adult</span><span class="p">()</span>

<span class="c1"># Split the data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit an XGBoost binary classifier on the training data split</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Build the Evaluation Dataset from the test set</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">X_test</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;predictions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test_pred</span>


<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="c1"># Evaluate the static dataset without providing a model</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
        <span class="n">predictions</span><span class="o">=</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classifier&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;metrics:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;artifacts:</span><span class="se">\n</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">artifacts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="performing-model-validation">
<span id="model-validation"></span><h3>Performing Model Validation<a class="headerlink" href="#performing-model-validation" title="Permalink to this headline"> </a></h3>
<p>You can also use the <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> API to perform some checks on the metrics
generated during model evaluation to validate the quality of your model. By specifying a
<code class="docutils literal notranslate"><span class="pre">validation_thresholds</span></code> dictionary mapping metric names to <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.MetricThreshold" title="mlflow.models.MetricThreshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlflow.models.MetricThreshold</span></code></a>
objects, you can specify value thresholds that your model’s evaluation metrics must exceed as well
as absolute and relative gains your model must have in comparison to a specified
<code class="docutils literal notranslate"><span class="pre">baseline_model</span></code>. If your model fails to clear specified thresholds, <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a>
will throw a <code class="docutils literal notranslate"><span class="pre">ModelValidationFailedException</span></code> detailing the validation failure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span>
<span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">MetricThreshold</span>

<span class="c1"># load UCI Adult Data Set; segment it into training and test sets</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">adult</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># train a candidate XGBoost model</span>
<span class="n">candidate_model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># train a baseline dummy model</span>
<span class="n">baseline_model</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># create signature that is shared by the two models</span>
<span class="n">signature</span> <span class="o">=</span> <span class="n">infer_signature</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># construct an evaluation dataset from the test set</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">X_test</span>
<span class="n">eval_data</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># Define criteria for model to be validated against</span>
<span class="n">thresholds</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;accuracy_score&quot;</span><span class="p">:</span> <span class="n">MetricThreshold</span><span class="p">(</span>
        <span class="n">threshold</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># accuracy should be &gt;=0.8</span>
        <span class="n">min_absolute_change</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># accuracy should be at least 0.05 greater than baseline model accuracy</span>
        <span class="n">min_relative_change</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># accuracy should be at least 5 percent greater than baseline model accuracy</span>
        <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">candidate_model_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">candidate_model</span><span class="p">,</span> <span class="s2">&quot;candidate_model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span><span class="o">.</span><span class="n">model_uri</span>
    <span class="n">baseline_model_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">baseline_model</span><span class="p">,</span> <span class="s2">&quot;baseline_model&quot;</span><span class="p">,</span> <span class="n">signature</span><span class="o">=</span><span class="n">signature</span>
    <span class="p">)</span><span class="o">.</span><span class="n">model_uri</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
        <span class="n">candidate_model_uri</span><span class="p">,</span>
        <span class="n">eval_data</span><span class="p">,</span>
        <span class="n">targets</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classifier&quot;</span><span class="p">,</span>
        <span class="n">validation_thresholds</span><span class="o">=</span><span class="n">thresholds</span><span class="p">,</span>
        <span class="n">baseline_model</span><span class="o">=</span><span class="n">baseline_model_uri</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Refer to <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.MetricThreshold" title="mlflow.models.MetricThreshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlflow.models.MetricThreshold</span></code></a> to see details on how the thresholds are specified
and checked. For a more comprehensive demonstration on how to use <a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> to perform model validation, refer to
<a class="reference external" href="https://github.com/mlflow/mlflow/blob/master/examples/evaluation/evaluate_with_model_validation.py">the Model Validation example from the MLflow GitHub Repository</a>.</p>
<p>The logged output within the MLflow UI for the comprehensive example is shown below. Note the two model artifacts that have
been logged: ‘baseline_model’ and ‘candidate_model’ for comparison purposes in the example.</p>
<p><a class="reference internal" href="_images/model_evaluation_compare_feature_importance.png"><img alt="eval_importance_compare_img" src="_images/model_evaluation_compare_feature_importance.png" style="width: 99%;" /></a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Limitations (when the default evaluator is used):</p>
<ul class="simple">
<li><p>Model validation results are not included in the active MLflow run.</p></li>
<li><p>No metrics are logged nor artifacts produced for the baseline model in the active MLflow run.</p></li>
</ul>
</div>
<p>Additional information about model evaluation behaviors and outputs is available in the
<a class="reference internal" href="python_api/mlflow.html#mlflow.evaluate" title="mlflow.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.evaluate()</span></code></a> API docs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are plugins that support in-depth model validation with features that are not supported
directly in MLflow. To learn more, see:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#giskard-plugin"><span class="std std-ref">Model Validation with Giskard’s plugin</span></a></p></li>
<li><p><a class="reference internal" href="#trubrics-plugin"><span class="std std-ref">Model Validation with Trubrics’ plugin</span></a>.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Differences in the computation of Area under Curve Precision Recall score (metric name
<code class="docutils literal notranslate"><span class="pre">precision_recall_auc</span></code>) between multi and binary classifiers:</p>
<p>Multiclass classifier models, when evaluated, utilize the standard scoring metric from sklearn:
<code class="docutils literal notranslate"><span class="pre">sklearn.metrics.roc_auc_score</span></code> to calculate the area under the precision recall curve. This
algorithm performs a linear interpolation calculation utilizing the trapezoidal rule to estimate
the area under the precision recall curve. It is well-suited for use in evaluating multi-class
classification models to provide a single numeric value of the quality of fit.</p>
<p>Binary classifier models, on the other hand, use the <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.average_precision_score</span></code> to
avoid the shortcomings of the <code class="docutils literal notranslate"><span class="pre">roc_auc_score</span></code> implementation when applied to heavily
imbalanced classes in binary classification. Usage of the <code class="docutils literal notranslate"><span class="pre">roc_auc_score</span></code> for imbalanced
datasets can give a misleading result (optimistically better than the model’s actual ability
to accurately predict the minority class membership).</p>
<p>For additional information on the topic of why different algorithms are employed for this, as
well as links to the papers that informed the implementation of these metrics within the
<code class="docutils literal notranslate"><span class="pre">sklearn.metrics</span></code> module, refer to
<a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">the documentation</a>.</p>
<p>For simplicity purposes, both methodologies evaluation metric results (whether for multi-class
or binary classification) are unified in the single metric: <code class="docutils literal notranslate"><span class="pre">precision_recall_auc</span></code>.</p>
</div>
</div>
<div class="section" id="model-validation-with-giskard-s-plugin">
<span id="giskard-plugin"></span><h3>Model Validation with Giskard’s plugin<a class="headerlink" href="#model-validation-with-giskard-s-plugin" title="Permalink to this headline"> </a></h3>
<p>To extend the validation capabilities of MLflow and anticipate issues before they go to production, a plugin has been built by <a class="reference external" href="https://docs.giskard.ai/en/latest/integrations/mlflow/index.html">Giskard</a> allowing users to:</p>
<blockquote>
<div><ul class="simple">
<li><p>scan a model in order to detect hidden vulnerabilities such as
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/performance_bias/index.html">Performance bias</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/robustness/index.html">Unrobustness</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/overconfidence/index.html">Overconfidence</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/underconfidence/index.html">Underconfidence</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/ethics/index.html">Ethical bias</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/data_leakage/index.html">Data leakage</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/stochasticity/index.html">Stochasticity</a>,
<a class="reference external" href="https://docs.giskard.ai/en/latest/getting-started/key_vulnerabilities/spurious/index.html">Spurious correlation</a>, and others</p></li>
<li><p>explore samples in the data that highlight the vulnerabilities found</p></li>
<li><p>log the vulnerabilities as well-defined and quantified metrics</p></li>
<li><p>compare the metrics across different models</p></li>
</ul>
</div></blockquote>
<p>See the following plugin example notebooks for a demo:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-tabular-example.html">Tabular ML models</a></p></li>
<li><p><a class="reference external" href="https://docs.giskard.ai/en/latest/integrations/mlflow/mlflow-llm-example.html">Text ML models (LLMs)</a></p></li>
</ul>
</div></blockquote>
<p>For more information on the plugin, see the <a class="reference external" href="https://docs.giskard.ai/en/latest/integrations/mlflow/index.html">giskard-mlflow docs</a>.</p>
</div>
<div class="section" id="model-validation-with-trubrics-plugin">
<span id="trubrics-plugin"></span><h3>Model Validation with Trubrics’ plugin<a class="headerlink" href="#model-validation-with-trubrics-plugin" title="Permalink to this headline"> </a></h3>
<p>To extend the validation capabilities of MLflow, a plugin has been built by <a class="reference external" href="https://github.com/trubrics/trubrics-sdk">Trubrics</a> allowing users:</p>
<blockquote>
<div><ul class="simple">
<li><p>to use a large number of out-of-the-box validations</p></li>
<li><p>to validate a run with any custom python functions</p></li>
<li><p>to view all validation results in a .json file, for diagnosis of why an MLflow run could have failed</p></li>
</ul>
</div></blockquote>
<p>See the <a class="reference external" href="https://github.com/trubrics/trubrics-sdk/blob/main/examples/mlflow/mlflow-trubrics.ipynb">plugin example notebook</a> for a demo.</p>
<p>For more information on the plugin, see the <a class="reference external" href="https://trubrics.github.io/trubrics-sdk/mlflow/">trubrics-mlflow docs</a>.</p>
</div>
</div>
<div class="section" id="model-customization">
<h2><a class="toc-backref" href="#id37">Model Customization</a><a class="headerlink" href="#model-customization" title="Permalink to this headline"> </a></h2>
<p>While MLflow’s built-in model persistence utilities are convenient for packaging models from various
popular ML libraries in MLflow Model format, they do not cover every use case. For example, you may
want to use a model from an ML library that is not explicitly supported by MLflow’s built-in
flavors. Alternatively, you may want to package custom inference code and data to create an
MLflow Model. Fortunately, MLflow provides two solutions that can be used to accomplish these
tasks: <a class="reference internal" href="#custom-python-models"><span class="std std-ref">Custom Python Models</span></a> and <a class="reference internal" href="#custom-flavors"><span class="std std-ref">Custom Flavors</span></a>.</p>
<div class="contents local topic" id="in-this-section">
<p class="topic-title">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#custom-python-models" id="id66">Custom Python Models</a></p>
<ul>
<li><p><a class="reference internal" href="#example-creating-a-custom-add-n-model" id="id67">Example: Creating a custom “add n” model</a></p></li>
<li><p><a class="reference internal" href="#example-saving-an-xgboost-model-in-mlflow-format" id="id68">Example: Saving an XGBoost model in MLflow format</a></p></li>
<li><p><a class="reference internal" href="#example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files" id="id69">Example: Logging a transformers model with hf:/ schema to avoid copying large files</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#custom-flavors" id="id70">Custom Flavors</a></p>
<ul>
<li><p><a class="reference internal" href="#example-creating-a-custom-sktime-flavor" id="id71">Example: Creating a custom “sktime” flavor</a></p></li>
<li><p><a class="reference internal" href="#example-using-the-custom-sktime-flavor" id="id72">Example: Using the custom “sktime” flavor</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="custom-python-models">
<span id="id14"></span><h3><a class="toc-backref" href="#id66">Custom Python Models</a><a class="headerlink" href="#custom-python-models" title="Permalink to this headline"> </a></h3>
<p>The <a class="reference internal" href="python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" title="mlflow.pyfunc"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pyfunc</span></code></a> module provides <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.save_model" title="mlflow.pyfunc.save_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_model()</span></code></a> and
<a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.log_model" title="mlflow.pyfunc.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_model()</span></code></a> utilities for creating MLflow Models with the
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor that contain user-specified code and <em>artifact</em> (file) dependencies.
These artifact dependencies may include serialized models produced by any Python ML library.</p>
<p>Because these custom models contain the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor, they can be deployed
to any of MLflow’s supported production environments, such as SageMaker, AzureML, or local
REST endpoints.</p>
<p>The following examples demonstrate how you can use the <a class="reference internal" href="python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" title="mlflow.pyfunc"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pyfunc</span></code></a> module to create
custom Python models. For additional information about model customization with MLflow’s
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> utilities, see the
<a class="reference internal" href="python_api/mlflow.pyfunc.html#pyfunc-create-custom"><span class="std std-ref">python_function custom models documentation</span></a>.</p>
<div class="section" id="example-creating-a-custom-add-n-model">
<h4><a class="toc-backref" href="#id67">Example: Creating a custom “add n” model</a><a class="headerlink" href="#example-creating-a-custom-add-n-model" title="Permalink to this headline"> </a></h4>
<p>This example defines a class for a custom model that adds a specified numeric value, <code class="docutils literal notranslate"><span class="pre">n</span></code>, to all
columns of a Pandas DataFrame input. Then, it uses the <a class="reference internal" href="python_api/mlflow.pyfunc.html#module-mlflow.pyfunc" title="mlflow.pyfunc"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.pyfunc</span></code></a> APIs to save an
instance of this model with <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">5</span></code> in MLflow Model format. Finally, it loads the model in
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> format and uses it to evaluate a sample input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow.pyfunc</span>


<span class="c1"># Define the model class</span>
<span class="k">class</span> <span class="nc">AddN</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">model_input</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">column</span><span class="p">:</span> <span class="n">column</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>


<span class="c1"># Construct and save the model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;add_n_model&quot;</span>
<span class="n">add5_model</span> <span class="o">=</span> <span class="n">AddN</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">python_model</span><span class="o">=</span><span class="n">add5_model</span><span class="p">)</span>

<span class="c1"># Load the model in `python_function` format</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">model_input</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">model_output</span><span class="o">.</span><span class="n">equals</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="section" id="example-saving-an-xgboost-model-in-mlflow-format">
<h4><a class="toc-backref" href="#id68">Example: Saving an XGBoost model in MLflow format</a><a class="headerlink" href="#example-saving-an-xgboost-model-in-mlflow-format" title="Permalink to this headline"> </a></h4>
<p>This example begins by training and saving a gradient boosted tree model using the XGBoost
library. Next, it defines a wrapper class around the XGBoost model that conforms to MLflow’s
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> <a class="reference internal" href="python_api/mlflow.pyfunc.html#pyfunc-inference-api"><span class="std std-ref">inference API</span></a>. Then, it uses the wrapper class and
the saved XGBoost model to construct an MLflow Model that performs inference using the gradient
boosted tree. Finally, it loads the MLflow Model in <code class="docutils literal notranslate"><span class="pre">python_function</span></code> format and uses it to
evaluate test data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load training and test datasets</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">version_info</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">PYTHON_VERSION</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">version_info</span><span class="o">.</span><span class="n">major</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">version_info</span><span class="o">.</span><span class="n">minor</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">version_info</span><span class="o">.</span><span class="n">micro</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Train and save an XGBoost model</span>
<span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> <span class="n">dtrain</span><span class="o">=</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">xgb_model_path</span> <span class="o">=</span> <span class="s2">&quot;xgb_model.pth&quot;</span>
<span class="n">xgb_model</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">xgb_model_path</span><span class="p">)</span>

<span class="c1"># Create an `artifacts` dictionary that assigns a unique name to the saved XGBoost model file.</span>
<span class="c1"># This dictionary will be passed to `mlflow.pyfunc.save_model`, which will copy the model file</span>
<span class="c1"># into the new MLflow Model&#39;s directory.</span>
<span class="n">artifacts</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;xgb_model&quot;</span><span class="p">:</span> <span class="n">xgb_model_path</span><span class="p">}</span>

<span class="c1"># Define the model class</span>
<span class="kn">import</span> <span class="nn">mlflow.pyfunc</span>


<span class="k">class</span> <span class="nc">XGBWrapper</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">load_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xgb_model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">artifacts</span><span class="p">[</span><span class="s2">&quot;xgb_model&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">input_matrix</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">xgb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">)</span>


<span class="c1"># Create a Conda environment for the new MLflow Model that contains all necessary dependencies.</span>
<span class="kn">import</span> <span class="nn">cloudpickle</span>

<span class="n">conda_env</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;channels&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;defaults&quot;</span><span class="p">],</span>
    <span class="s2">&quot;dependencies&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;python=</span><span class="si">{</span><span class="n">PYTHON_VERSION</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;pip&quot;</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="s2">&quot;pip&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="sa">f</span><span class="s2">&quot;mlflow==</span><span class="si">{</span><span class="n">mlflow</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;xgboost==</span><span class="si">{</span><span class="n">xgb</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;cloudpickle==</span><span class="si">{</span><span class="n">cloudpickle</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">],</span>
        <span class="p">},</span>
    <span class="p">],</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;xgb_env&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Save the MLflow Model</span>
<span class="n">mlflow_pyfunc_model_path</span> <span class="o">=</span> <span class="s2">&quot;xgb_mlflow_pyfunc&quot;</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="n">mlflow_pyfunc_model_path</span><span class="p">,</span>
    <span class="n">python_model</span><span class="o">=</span><span class="n">XGBWrapper</span><span class="p">(),</span>
    <span class="n">artifacts</span><span class="o">=</span><span class="n">artifacts</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load the model in `python_function` format</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">mlflow_pyfunc_model_path</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files">
<h4><a class="toc-backref" href="#id69">Example: Logging a transformers model with hf:/ schema to avoid copying large files</a><a class="headerlink" href="#example-logging-a-transformers-model-with-hf-schema-to-avoid-copying-large-files" title="Permalink to this headline"> </a></h4>
<p>This example shows how to use a special schema <code class="docutils literal notranslate"><span class="pre">hf:/</span></code> to log a transformers model from huggingface
hub directly. This is useful when the model is too large and especially when you want to serve the
model directly, but it doesn’t save extra space if you want to download and test the model locally.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">infer_signature</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">transformers</span>


<span class="c1"># Define a custom PythonModel</span>
<span class="k">class</span> <span class="nc">QAModel</span><span class="p">(</span><span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">PythonModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">load_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method initializes the tokenizer and language model</span>
<span class="sd">        using the specified snapshot location from model context.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">snapshot_location</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">artifacts</span><span class="p">[</span><span class="s2">&quot;bert-tiny-model&quot;</span><span class="p">]</span>
        <span class="c1"># Initialize tokenizer and language model</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">snapshot_location</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">BertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">snapshot_location</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
            <span class="n">task</span><span class="o">=</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">model_input</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">question</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">model_input</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">ctx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>


<span class="c1"># Log the model</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Who&#39;s house?&quot;</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;The house is owned by Run.&quot;</span><span class="p">}</span>
<span class="n">pyfunc_artifact_path</span> <span class="o">=</span> <span class="s2">&quot;question_answering_model&quot;</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">pyfunc_artifact_path</span><span class="p">,</span>
        <span class="n">python_model</span><span class="o">=</span><span class="n">QAModel</span><span class="p">(),</span>
        <span class="n">artifacts</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;bert-tiny-model&quot;</span><span class="p">:</span> <span class="s2">&quot;hf:/prajjwal1/bert-tiny&quot;</span><span class="p">},</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">infer_signature</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Run&quot;</span><span class="p">]),</span>
        <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">,</span> <span class="s2">&quot;transformers&quot;</span><span class="p">,</span> <span class="s2">&quot;numpy&quot;</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="custom-flavors">
<span id="id15"></span><h3><a class="toc-backref" href="#id70">Custom Flavors</a><a class="headerlink" href="#custom-flavors" title="Permalink to this headline"> </a></h3>
<p>You can also create custom MLflow Models by writing a custom <em>flavor</em>.</p>
<p>As discussed in the <a class="reference internal" href="#model-api"><span class="std std-ref">Model API</span></a> and <a class="reference internal" href="#model-storage-format"><span class="std std-ref">Storage Format</span></a> sections, an MLflow Model
is defined by a directory of files that contains an <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> configuration file. This <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code>
file describes various model attributes, including the flavors in which the model can be
interpreted. The <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file contains an entry for each flavor name; each entry is
a YAML-formatted collection of flavor-specific attributes.</p>
<p>To create a new flavor to support a custom model, you define the set of flavor-specific attributes
to include in the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> configuration file, as well as the code that can interpret the
contents of the model directory and the flavor’s attributes. A detailed example of constructing a
custom model flavor and its usage is shown below. New custom flavors not considered for official
inclusion into MLflow should be introduced as separate GitHub repositories with documentation
provided in the
<a class="reference external" href="community-model-flavors.html">Community Model Flavors</a>
page.</p>
<div class="section" id="example-creating-a-custom-sktime-flavor">
<h4><a class="toc-backref" href="#id71">Example: Creating a custom “sktime” flavor</a><a class="headerlink" href="#example-creating-a-custom-sktime-flavor" title="Permalink to this headline"> </a></h4>
<p>This example illustrates the creation of a custom flavor for
<a class="reference external" href="https://github.com/sktime/sktime">sktime</a> time series library. The library provides a unified
interface for multiple learning  tasks including time series forecasting. While the custom flavor in
this example is specific in terms of the <code class="docutils literal notranslate"><span class="pre">sktime</span></code> inference API and model serialization format,
its interface design is similar to many of the existing built-in flavors. Particularly, the
interface for utilizing the custom model loaded as a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor for generating
predictions uses a <em>single-row</em> <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> configuration argument to expose the paramters
of the <code class="docutils literal notranslate"><span class="pre">sktime</span></code> inference API. The complete code for this example is included in the
<a class="reference external" href="https://github.com/mlflow/mlflow/tree/master/examples/sktime/flavor.py">flavor.py</a> module of the
<code class="docutils literal notranslate"><span class="pre">sktime</span></code> example directory.</p>
<p>Let’s examine the custom flavor module in more detail. The first step is to import several modules
inluding <code class="docutils literal notranslate"><span class="pre">sktime</span></code> library, various MLflow utilities as well as the MLflow <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> module which
is required to add the <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> specification to the MLflow model configuration. Note also the
import of the <code class="docutils literal notranslate"><span class="pre">flavor</span></code> module itself. This will be passed to the
<a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.log" title="mlflow.models.Model.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.Model.log()</span></code></a> method to log the model as an artifact to the current MLflow
run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">import</span> <span class="nn">flavor</span>
<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sktime</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">pyfunc</span>
<span class="kn">from</span> <span class="nn">mlflow.exceptions</span> <span class="kn">import</span> <span class="n">MlflowException</span>
<span class="kn">from</span> <span class="nn">mlflow.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mlflow.models.model</span> <span class="kn">import</span> <span class="n">MLMODEL_FILE_NAME</span>
<span class="kn">from</span> <span class="nn">mlflow.models.utils</span> <span class="kn">import</span> <span class="n">_save_example</span>
<span class="kn">from</span> <span class="nn">mlflow.protos.databricks_pb2</span> <span class="kn">import</span> <span class="n">INTERNAL_ERROR</span><span class="p">,</span> <span class="n">INVALID_PARAMETER_VALUE</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking._model_registry</span> <span class="kn">import</span> <span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span>
<span class="kn">from</span> <span class="nn">mlflow.tracking.artifact_utils</span> <span class="kn">import</span> <span class="n">_download_artifact_from_uri</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.environment</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
    <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">,</span>
    <span class="n">_mlflow_conda_env</span><span class="p">,</span>
    <span class="n">_process_conda_env</span><span class="p">,</span>
    <span class="n">_process_pip_requirements</span><span class="p">,</span>
    <span class="n">_PythonEnv</span><span class="p">,</span>
    <span class="n">_validate_env_arguments</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.file_utils</span> <span class="kn">import</span> <span class="n">write_to</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.model_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">,</span>
    <span class="n">_get_flavor_configuration</span><span class="p">,</span>
    <span class="n">_validate_and_copy_code_paths</span><span class="p">,</span>
    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mlflow.utils.requirements_utils</span> <span class="kn">import</span> <span class="n">_get_pinned_requirement</span>
<span class="kn">from</span> <span class="nn">sktime.utils.multiindex</span> <span class="kn">import</span> <span class="n">flatten_multiindex</span>

<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</pre></div>
</div>
<p>We continue by defining a set of important variables used throughout the code that follows.</p>
<p>The flavor name needs to be provided for every custom flavor and should reflect the name of the
library to be supported. It is saved as part of the flavor-specific attributes to the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code>
configuration file. This example also defines some <code class="docutils literal notranslate"><span class="pre">sktime</span></code> specific variables. For illustration
purposes, only a subset of the available predict methods to be exposed via the
<code class="docutils literal notranslate"><span class="pre">_SktimeModelWrapper</span></code> class is included when loading the model in its <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor
(additional methods could be added in a similar fashion). Additionaly, the model serialization
formats, namely <code class="docutils literal notranslate"><span class="pre">pickle</span></code> (default) and <code class="docutils literal notranslate"><span class="pre">cloudpickle</span></code>, are defined. Note that both serialization
modules require using the same python environment (version) in whatever environment this model is
used for inference to ensure that the model will load with the appropriate version of
pickle (cloudpickle).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FLAVOR_NAME</span> <span class="o">=</span> <span class="s2">&quot;sktime&quot;</span>

<span class="n">SKTIME_PREDICT</span> <span class="o">=</span> <span class="s2">&quot;predict&quot;</span>
<span class="n">SKTIME_PREDICT_INTERVAL</span> <span class="o">=</span> <span class="s2">&quot;predict_interval&quot;</span>
<span class="n">SKTIME_PREDICT_QUANTILES</span> <span class="o">=</span> <span class="s2">&quot;predict_quantiles&quot;</span>
<span class="n">SKTIME_PREDICT_VAR</span> <span class="o">=</span> <span class="s2">&quot;predict_var&quot;</span>
<span class="n">SUPPORTED_SKTIME_PREDICT_METHODS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SKTIME_PREDICT</span><span class="p">,</span>
    <span class="n">SKTIME_PREDICT_INTERVAL</span><span class="p">,</span>
    <span class="n">SKTIME_PREDICT_QUANTILES</span><span class="p">,</span>
    <span class="n">SKTIME_PREDICT_VAR</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">SERIALIZATION_FORMAT_PICKLE</span> <span class="o">=</span> <span class="s2">&quot;pickle&quot;</span>
<span class="n">SERIALIZATION_FORMAT_CLOUDPICKLE</span> <span class="o">=</span> <span class="s2">&quot;cloudpickle&quot;</span>
<span class="n">SUPPORTED_SERIALIZATION_FORMATS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SERIALIZATION_FORMAT_PICKLE</span><span class="p">,</span>
    <span class="n">SERIALIZATION_FORMAT_CLOUDPICKLE</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Similar to the MLflow built-in flavors, a custom flavor logs the model in MLflow format via the
<code class="docutils literal notranslate"><span class="pre">save_model()</span></code> and <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> functions. In the <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function, the <code class="docutils literal notranslate"><span class="pre">sktime</span></code>
model is saved to a specified output directory. Additionally, <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> leverages the
<a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.add_flavor" title="mlflow.models.Model.add_flavor"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.Model.add_flavor()</span></code></a> and <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.save" title="mlflow.models.Model.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.Model.save()</span></code></a> methods to
produce the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> configuration containing the <code class="docutils literal notranslate"><span class="pre">sktime</span></code> and the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor.
The resulting configuration has several flavor-specific attributes, such as the flavor name and
<code class="docutils literal notranslate"><span class="pre">sktime_version</span></code>, which denotes the version of the <code class="docutils literal notranslate"><span class="pre">sktime</span></code> library that was used to train the
model. An example of the output directoy for the custom <code class="docutils literal notranslate"><span class="pre">sktime</span></code> model is shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Directory written by flavor.save_model(model, &quot;my_model&quot;)
my_model/
├── MLmodel
├── conda.yaml
├── model.pkl
├── python_env.yaml
└── requirements.txt
</pre></div>
</div>
<p>And its YAML-formatted <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> file describes the two flavors:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">flavors</span><span class="p">:</span>
<span class="w">  </span><span class="nt">python_function</span><span class="p">:</span>
<span class="w">    </span><span class="nt">env</span><span class="p">:</span>
<span class="w">      </span><span class="nt">conda</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">conda.yaml</span>
<span class="w">      </span><span class="nt">virtualenv</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python_env.yaml</span>
<span class="w">    </span><span class="nt">loader_module</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">flavor</span>
<span class="w">    </span><span class="nt">model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model.pkl</span>
<span class="w">    </span><span class="nt">python_version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3.8.15</span>
<span class="w">  </span><span class="nt">sktime</span><span class="p">:</span>
<span class="w">    </span><span class="nt">code</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">pickled_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model.pkl</span>
<span class="w">    </span><span class="nt">serialization_format</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pickle</span>
<span class="w">    </span><span class="nt">sktime_version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.16.0</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function also provides flexibility to add additional paramters which can be
added as flavor-specific attributes to the model configuration. In this example there is only one
flavor-specific parameter for specifying the model serialization format. All other paramters are
non-flavor specific (for a detailed description of these parameters take a look at
<a class="reference external" href="https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.save_model">mlflow.sklearn.save_model</a>).
Note: When creating your own custom flavor, be sure rename the <code class="docutils literal notranslate"><span class="pre">sktime_model</span></code> parameter in both the
<code class="docutils literal notranslate"><span class="pre">save_model()</span></code> and <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> functions to reflect the name of your custom model flavor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span>
    <span class="n">sktime_model</span><span class="p">,</span>
    <span class="n">path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlflow_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">serialization_format</span><span class="o">=</span><span class="n">SERIALIZATION_FORMAT_PICKLE</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">_validate_env_arguments</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">serialization_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_SERIALIZATION_FORMATS</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unrecognized serialization format: </span><span class="si">{</span><span class="n">serialization_format</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;Please specify one of the following supported formats: &quot;</span>
                <span class="s2">&quot;</span><span class="si">{SUPPORTED_SERIALIZATION_FORMATS}</span><span class="s2">.&quot;</span>
            <span class="p">),</span>
            <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">_validate_and_prepare_target_save_path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">code_dir_subpath</span> <span class="o">=</span> <span class="n">_validate_and_copy_code_paths</span><span class="p">(</span><span class="n">code_paths</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mlflow_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow_model</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span>
    <span class="k">if</span> <span class="n">input_example</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_save_example</span><span class="p">(</span><span class="n">mlflow_model</span><span class="p">,</span> <span class="n">input_example</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="n">model_data_subpath</span> <span class="o">=</span> <span class="s2">&quot;model.pkl&quot;</span>
    <span class="n">model_data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">model_data_subpath</span><span class="p">)</span>
    <span class="n">_save_model</span><span class="p">(</span>
        <span class="n">sktime_model</span><span class="p">,</span> <span class="n">model_data_path</span><span class="p">,</span> <span class="n">serialization_format</span><span class="o">=</span><span class="n">serialization_format</span>
    <span class="p">)</span>

    <span class="n">pyfunc</span><span class="o">.</span><span class="n">add_to_model</span><span class="p">(</span>
        <span class="n">mlflow_model</span><span class="p">,</span>
        <span class="n">loader_module</span><span class="o">=</span><span class="s2">&quot;flavor&quot;</span><span class="p">,</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">model_data_subpath</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">python_env</span><span class="o">=</span><span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">add_flavor</span><span class="p">(</span>
        <span class="n">FLAVOR_NAME</span><span class="p">,</span>
        <span class="n">pickled_model</span><span class="o">=</span><span class="n">model_data_subpath</span><span class="p">,</span>
        <span class="n">sktime_version</span><span class="o">=</span><span class="n">sktime</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="n">serialization_format</span><span class="o">=</span><span class="n">serialization_format</span><span class="p">,</span>
        <span class="n">code</span><span class="o">=</span><span class="n">code_dir_subpath</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mlflow_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">MLMODEL_FILE_NAME</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">conda_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pip_requirements</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">include_cloudpickle</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">serialization_format</span> <span class="o">==</span> <span class="n">SERIALIZATION_FORMAT_CLOUDPICKLE</span>
            <span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="n">get_default_pip_requirements</span><span class="p">(</span><span class="n">include_cloudpickle</span><span class="p">)</span>
            <span class="n">inferred_reqs</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">infer_pip_requirements</span><span class="p">(</span>
                <span class="n">path</span><span class="p">,</span> <span class="n">FLAVOR_NAME</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="n">default_reqs</span>
            <span class="p">)</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">inferred_reqs</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">default_reqs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_reqs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_pip_requirements</span><span class="p">(</span>
            <span class="n">default_reqs</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">extra_pip_requirements</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conda_env</span><span class="p">,</span> <span class="n">pip_requirements</span><span class="p">,</span> <span class="n">pip_constraints</span> <span class="o">=</span> <span class="n">_process_conda_env</span><span class="p">(</span><span class="n">conda_env</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONDA_ENV_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">safe_dump</span><span class="p">(</span><span class="n">conda_env</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pip_constraints</span><span class="p">:</span>
        <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_CONSTRAINTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_constraints</span><span class="p">))</span>

    <span class="n">write_to</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_REQUIREMENTS_FILE_NAME</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pip_requirements</span><span class="p">))</span>

    <span class="n">_PythonEnv</span><span class="o">.</span><span class="n">current</span><span class="p">()</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">_PYTHON_ENV_FILE_NAME</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">serialization_format</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">serialization_format</span> <span class="o">==</span> <span class="n">SERIALIZATION_FORMAT_PICKLE</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">cloudpickle</span>

            <span class="n">cloudpickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function also writes the model dependencies to a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> and
<code class="docutils literal notranslate"><span class="pre">conda.yaml</span></code> file in the model output directory. For this purpose the set of <code class="docutils literal notranslate"><span class="pre">pip</span></code> dependecies
produced by this flavor need to be added to the <code class="docutils literal notranslate"><span class="pre">get_default_pip_requirements()</span></code> function. In this
example only the minimum required dependencies are provided. In practice, additional requirements needed for
preprocessing or post-processing steps could be included. Note that for any custom flavor, the
<a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.infer_pip_requirements" title="mlflow.models.infer_pip_requirements"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.infer_pip_requirements()</span></code></a> method in the <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function will
return the default requirements defined in <code class="docutils literal notranslate"><span class="pre">get_default_pip_requirements()</span></code> as package imports are
only inferred for built-in flavors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_default_pip_requirements</span><span class="p">(</span><span class="n">include_cloudpickle</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">pip_deps</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_pinned_requirement</span><span class="p">(</span><span class="s2">&quot;sktime&quot;</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">include_cloudpickle</span><span class="p">:</span>
        <span class="n">pip_deps</span> <span class="o">+=</span> <span class="p">[</span><span class="n">_get_pinned_requirement</span><span class="p">(</span><span class="s2">&quot;cloudpickle&quot;</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">pip_deps</span>


<span class="k">def</span> <span class="nf">get_default_conda_env</span><span class="p">(</span><span class="n">include_cloudpickle</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_mlflow_conda_env</span><span class="p">(</span>
        <span class="n">additional_pip_deps</span><span class="o">=</span><span class="n">get_default_pip_requirements</span><span class="p">(</span><span class="n">include_cloudpickle</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Next, we add the <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> function. This function is little more than a wrapper around the
<a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.log" title="mlflow.models.Model.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.Model.log()</span></code></a> method to enable logging our custom model as an artifact to the
curren MLflow run. Any flavor-specific parameters (e.g. <code class="docutils literal notranslate"><span class="pre">serialization_format</span></code>) introduced in the
<code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function also need to be added in the <code class="docutils literal notranslate"><span class="pre">log_model()</span></code> function. We also need to
pass the <code class="docutils literal notranslate"><span class="pre">flavor</span></code> module to the <a class="reference internal" href="python_api/mlflow.models.html#mlflow.models.Model.log" title="mlflow.models.Model.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.Model.log()</span></code></a> method which internally calls
the <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> function from above to persist the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_model</span><span class="p">(</span>
    <span class="n">sktime_model</span><span class="p">,</span>
    <span class="n">artifact_path</span><span class="p">,</span>
    <span class="n">conda_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">code_paths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">registered_model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_example</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">await_registration_for</span><span class="o">=</span><span class="n">DEFAULT_AWAIT_MAX_SLEEP_SECONDS</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">serialization_format</span><span class="o">=</span><span class="n">SERIALIZATION_FORMAT_PICKLE</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">Model</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">artifact_path</span><span class="p">,</span>
        <span class="n">flavor</span><span class="o">=</span><span class="n">flavor</span><span class="p">,</span>
        <span class="n">registered_model_name</span><span class="o">=</span><span class="n">registered_model_name</span><span class="p">,</span>
        <span class="n">sktime_model</span><span class="o">=</span><span class="n">sktime_model</span><span class="p">,</span>
        <span class="n">conda_env</span><span class="o">=</span><span class="n">conda_env</span><span class="p">,</span>
        <span class="n">code_paths</span><span class="o">=</span><span class="n">code_paths</span><span class="p">,</span>
        <span class="n">signature</span><span class="o">=</span><span class="n">signature</span><span class="p">,</span>
        <span class="n">input_example</span><span class="o">=</span><span class="n">input_example</span><span class="p">,</span>
        <span class="n">await_registration_for</span><span class="o">=</span><span class="n">await_registration_for</span><span class="p">,</span>
        <span class="n">pip_requirements</span><span class="o">=</span><span class="n">pip_requirements</span><span class="p">,</span>
        <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="n">extra_pip_requirements</span><span class="p">,</span>
        <span class="n">serialization_format</span><span class="o">=</span><span class="n">serialization_format</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>To interpret model directories produced by <code class="docutils literal notranslate"><span class="pre">save_model()</span></code>, the custom flavor must also define a
<code class="docutils literal notranslate"><span class="pre">load_model()</span></code> function. The <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> function reads the <code class="docutils literal notranslate"><span class="pre">MLmodel</span></code> configuration from
the specified model directory and uses the configuration attributes to load and return the
<code class="docutils literal notranslate"><span class="pre">sktime</span></code> model from its serialized representation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">dst_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">local_model_path</span> <span class="o">=</span> <span class="n">_download_artifact_from_uri</span><span class="p">(</span>
        <span class="n">artifact_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">dst_path</span>
    <span class="p">)</span>
    <span class="n">flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span>
    <span class="p">)</span>
    <span class="n">_add_code_from_conf_to_system_path</span><span class="p">(</span><span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">)</span>
    <span class="n">sktime_model_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">local_model_path</span><span class="p">,</span> <span class="n">flavor_conf</span><span class="p">[</span><span class="s2">&quot;pickled_model&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">serialization_format</span> <span class="o">=</span> <span class="n">flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;serialization_format&quot;</span><span class="p">,</span> <span class="n">SERIALIZATION_FORMAT_PICKLE</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_load_model</span><span class="p">(</span>
        <span class="n">path</span><span class="o">=</span><span class="n">sktime_model_file_path</span><span class="p">,</span> <span class="n">serialization_format</span><span class="o">=</span><span class="n">serialization_format</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_load_model</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">serialization_format</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pickled_model</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">serialization_format</span> <span class="o">==</span> <span class="n">SERIALIZATION_FORMAT_PICKLE</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">pickled_model</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">serialization_format</span> <span class="o">==</span> <span class="n">SERIALIZATION_FORMAT_CLOUDPICKLE</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">cloudpickle</span>

            <span class="k">return</span> <span class="n">cloudpickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">pickled_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">_load_pyfunc()</span></code> function will be called by the <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model" title="mlflow.pyfunc.load_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.load_model()</span></code></a> method
to load the custom model flavor as a <code class="docutils literal notranslate"><span class="pre">pyfunc</span></code> type. The MLmodel flavor configuration is used to
pass any flavor-specific attributes to the <code class="docutils literal notranslate"><span class="pre">_load_model()</span></code> function (i.e., the path to the
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor in the model directory and the model serialization format).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_load_pyfunc</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sktime_flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span>
            <span class="n">model_path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">FLAVOR_NAME</span>
        <span class="p">)</span>
        <span class="n">serialization_format</span> <span class="o">=</span> <span class="n">sktime_flavor_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;serialization_format&quot;</span><span class="p">,</span> <span class="n">SERIALIZATION_FORMAT_PICKLE</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="n">MlflowException</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Could not find sktime flavor configuration during model &quot;</span>
            <span class="s2">&quot;loading process. Assuming &#39;pickle&#39; serialization format.&quot;</span>
        <span class="p">)</span>
        <span class="n">serialization_format</span> <span class="o">=</span> <span class="n">SERIALIZATION_FORMAT_PICKLE</span>

    <span class="n">pyfunc_flavor_conf</span> <span class="o">=</span> <span class="n">_get_flavor_configuration</span><span class="p">(</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">flavor_name</span><span class="o">=</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">FLAVOR_NAME</span>
    <span class="p">)</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">pyfunc_flavor_conf</span><span class="p">[</span><span class="s2">&quot;model_path&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">_SktimeModelWrapper</span><span class="p">(</span>
        <span class="n">_load_model</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">serialization_format</span><span class="o">=</span><span class="n">serialization_format</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The final step is to create the model wrapper class defining the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor. The
design of the wrapper class determines how the flavor’s inference API is exposed when making
predictions using the <code class="docutils literal notranslate"><span class="pre">python_function</span></code> flavor. Just like the built-in flavors, the <code class="docutils literal notranslate"><span class="pre">predict()</span></code>
method of the <code class="docutils literal notranslate"><span class="pre">sktime</span></code> wrapper class accepts a <em>single-row</em> <code class="docutils literal notranslate"><span class="pre">Pandas</span> <span class="pre">DataFrame</span></code> configuration
argument. For an example of how to construct this configuration DataFrame refer to the usage example
in the next section. A detailed description of the supported paramaters and input formats is
provided in the
<a class="reference external" href="https://github.com/mlflow/mlflow/tree/master/examples/sktime/flavor.py">flavor.py</a> module
docstrings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">_SktimeModelWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sktime_model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sktime_model</span> <span class="o">=</span> <span class="n">sktime_model</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
        <span class="n">df_schema</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided prediction pd.DataFrame contains </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span><span class="si">}</span><span class="s2"> rows. &quot;</span>
                <span class="s2">&quot;Only 1 row should be supplied.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Convert the configuration dataframe into a dictionary to simplify the</span>
        <span class="c1"># extraction of parameters passed to the sktime predcition methods.</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">predict_method</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;predict_method&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">predict_method</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided prediction configuration pd.DataFrame columns (</span><span class="si">{</span><span class="n">df_schema</span><span class="si">}</span><span class="s2">) do not &quot;</span>
                <span class="s2">&quot;contain the required column `predict_method` for specifying the prediction method.&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">predict_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_SKTIME_PREDICT_METHODS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MlflowException</span><span class="p">(</span>
                <span class="s2">&quot;Invalid `predict_method` value.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The supported prediction methods are </span><span class="si">{</span><span class="n">SUPPORTED_SKTIME_PREDICT_METHODS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">error_code</span><span class="o">=</span><span class="n">INVALID_PARAMETER_VALUE</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># For inference parameters &#39;fh&#39;, &#39;X&#39;, &#39;coverage&#39;, &#39;alpha&#39;, and &#39;cov&#39;</span>
        <span class="c1"># the respective sktime default value is used if the value was not</span>
        <span class="c1"># provided in the configuration dataframe.</span>
        <span class="n">fh</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fh&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Any model that is trained with exogenous regressor elements will need</span>
        <span class="c1"># to provide `X` entries as a numpy ndarray to the predict method.</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># When the model is served via REST API the exogenous regressor must be</span>
        <span class="c1"># provided as a list to the configuration DataFrame to be JSON serializable.</span>
        <span class="c1"># Below we convert the list back to ndarray type as required by sktime</span>
        <span class="c1"># predict methods.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># For illustration purposes only a subset of the available sktime prediction</span>
        <span class="c1"># methods is exposed. Additional methods (e.g. predict_proba) could be added</span>
        <span class="c1"># in a similar fashion.</span>
        <span class="k">if</span> <span class="n">predict_method</span> <span class="o">==</span> <span class="n">SKTIME_PREDICT</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sktime_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">fh</span><span class="o">=</span><span class="n">fh</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">predict_method</span> <span class="o">==</span> <span class="n">SKTIME_PREDICT_INTERVAL</span><span class="p">:</span>
            <span class="n">coverage</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;coverage&quot;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sktime_model</span><span class="o">.</span><span class="n">predict_interval</span><span class="p">(</span>
                <span class="n">fh</span><span class="o">=</span><span class="n">fh</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">coverage</span><span class="o">=</span><span class="n">coverage</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">predict_method</span> <span class="o">==</span> <span class="n">SKTIME_PREDICT_QUANTILES</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sktime_model</span><span class="o">.</span><span class="n">predict_quantiles</span><span class="p">(</span><span class="n">fh</span><span class="o">=</span><span class="n">fh</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">predict_method</span> <span class="o">==</span> <span class="n">SKTIME_PREDICT_VAR</span><span class="p">:</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cov&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sktime_model</span><span class="o">.</span><span class="n">predict_var</span><span class="p">(</span><span class="n">fh</span><span class="o">=</span><span class="n">fh</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">)</span>

        <span class="c1"># Methods predict_interval() and predict_quantiles() return a pandas</span>
        <span class="c1"># MultiIndex column structure. As MLflow signature inference does not</span>
        <span class="c1"># support MultiIndex column structure the columns must be flattened.</span>
        <span class="k">if</span> <span class="n">predict_method</span> <span class="ow">in</span> <span class="p">[</span><span class="n">SKTIME_PREDICT_INTERVAL</span><span class="p">,</span> <span class="n">SKTIME_PREDICT_QUANTILES</span><span class="p">]:</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">flatten_multiindex</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="section" id="example-using-the-custom-sktime-flavor">
<h4><a class="toc-backref" href="#id72">Example: Using the custom “sktime” flavor</a><a class="headerlink" href="#example-using-the-custom-sktime-flavor" title="Permalink to this headline"> </a></h4>
<p>This example trains a <code class="docutils literal notranslate"><span class="pre">sktime</span></code> NaiveForecaster model using the Longley dataset for forecasting
with exogenous variables. It shows a custom model type implementation that logs the training
hyper-parameters, evaluation metrics and the trained model as an artifact. The <em>single-row</em>
configuration DataFrame for this example defines an interval forecast with nominal coverage values
<code class="docutils literal notranslate"><span class="pre">[0.9,0.95]</span></code>, a future forecast horizon of four periods, and an exogenous regressor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">import</span> <span class="nn">flavor</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sktime.datasets</span> <span class="kn">import</span> <span class="n">load_longley</span>
<span class="kn">from</span> <span class="nn">sktime.forecasting.model_selection</span> <span class="kn">import</span> <span class="n">temporal_train_test_split</span>
<span class="kn">from</span> <span class="nn">sktime.forecasting.naive</span> <span class="kn">import</span> <span class="n">NaiveForecaster</span>
<span class="kn">from</span> <span class="nn">sktime.performance_metrics.forecasting</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">mean_absolute_error</span><span class="p">,</span>
    <span class="n">mean_absolute_percentage_error</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">ARTIFACT_PATH</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">load_longley</span><span class="p">()</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">temporal_train_test_split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="n">forecaster</span> <span class="o">=</span> <span class="n">NaiveForecaster</span><span class="p">()</span>
    <span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">y_train</span><span class="p">,</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Extract parameters</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>

    <span class="c1"># Evaluate model</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">fh</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span>
        <span class="s2">&quot;mape&quot;</span><span class="p">:</span> <span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: </span><span class="se">\n</span><span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Metrics: </span><span class="se">\n</span><span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Log parameters and metrics</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

    <span class="c1"># Log model using custom model flavor with pickle serialization (default).</span>
    <span class="n">flavor</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
        <span class="n">sktime_model</span><span class="o">=</span><span class="n">forecaster</span><span class="p">,</span>
        <span class="n">artifact_path</span><span class="o">=</span><span class="n">ARTIFACT_PATH</span><span class="p">,</span>
        <span class="n">serialization_format</span><span class="o">=</span><span class="s2">&quot;pickle&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model_uri</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">get_artifact_uri</span><span class="p">(</span><span class="n">ARTIFACT_PATH</span><span class="p">)</span>

<span class="c1"># Load model in native sktime flavor and pyfunc flavor</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">flavor</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">)</span>
<span class="n">loaded_pyfunc</span> <span class="o">=</span> <span class="n">flavor</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="o">=</span><span class="n">model_uri</span><span class="p">)</span>

<span class="c1"># Convert test data to 2D numpy array so it can be passed to pyfunc predict using</span>
<span class="c1"># a single-row Pandas DataFrame configuration argument</span>
<span class="n">X_test_array</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Create configuration DataFrame</span>
<span class="n">predict_conf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;fh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
            <span class="s2">&quot;predict_method&quot;</span><span class="p">:</span> <span class="s2">&quot;predict_interval&quot;</span><span class="p">,</span>
            <span class="s2">&quot;coverage&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span>
            <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">X_test_array</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Generate interval forecasts with native sktime flavor and pyfunc flavor</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Native sktime &#39;predict_interval&#39;:</span><span class="se">\n</span><span class="s2">$</span><span class="si">{</span><span class="n">loaded_model</span><span class="o">.</span><span class="n">predict_interval</span><span class="p">(</span><span class="n">fh</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">coverage</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.95</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Pyfunc &#39;predict_interval&#39;:</span><span class="se">\n</span><span class="s2">$</span><span class="si">{</span><span class="n">loaded_pyfunc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">predict_conf</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print the run id wich is used for serving the model to a local REST API endpoint</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MLflow run id:</span><span class="se">\n</span><span class="si">{</span><span class="n">run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>When opening the MLflow runs detail page the serialized model artifact  will show up, such as:</p>
<blockquote>
<div><div class="figure align-default">
<img alt="_images/tracking_artifact_ui_custom_flavor.png" src="_images/tracking_artifact_ui_custom_flavor.png" />
</div>
</div></blockquote>
<p>To serve the model to a local REST API endpoint run the following MLflow CLI command substituting
the run id printed during execution of the previous block (for more details refer to the
<a class="reference external" href="https://mlflow.org/docs/latest/models.html#deploy-mlflow-models">Deploy MLflow models</a> section):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>runs:/&lt;run_id&gt;/model<span class="w"> </span>--env-manager<span class="w"> </span><span class="nb">local</span><span class="w"> </span>--host<span class="w"> </span><span class="m">127</span>.0.0.1
</pre></div>
</div>
<p>An example of requesting a prediction from the served model is shown below. The exogenous regressor
needs to be provided as a list to be JSON serializable. The wrapper instance will convert the list
back to <code class="docutils literal notranslate"><span class="pre">numpy</span> <span class="pre">ndarray</span></code> type as required by <code class="docutils literal notranslate"><span class="pre">sktime</span></code> inference API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="kn">from</span> <span class="nn">sktime.datasets</span> <span class="kn">import</span> <span class="n">load_longley</span>
<span class="kn">from</span> <span class="nn">sktime.forecasting.model_selection</span> <span class="kn">import</span> <span class="n">temporal_train_test_split</span>

<span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">load_longley</span><span class="p">()</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">temporal_train_test_split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="c1"># Define local host and endpoint url</span>
<span class="n">host</span> <span class="o">=</span> <span class="s2">&quot;127.0.0.1&quot;</span>
<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s2">:5000/invocations&quot;</span>

<span class="c1"># Create configuration DataFrame</span>
<span class="n">X_test_list</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">predict_conf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;fh&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
            <span class="s2">&quot;predict_method&quot;</span><span class="p">:</span> <span class="s2">&quot;predict_interval&quot;</span><span class="p">,</span>
            <span class="s2">&quot;coverage&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span>
            <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">X_test_list</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Create dictionary with pandas DataFrame in the split orientation</span>
<span class="n">json_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dataframe_split&quot;</span><span class="p">:</span> <span class="n">predict_conf</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;split&quot;</span><span class="p">)}</span>

<span class="c1"># Score model</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">json_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Pyfunc &#39;predict_interval&#39;:</span><span class="se">\n</span><span class="s2">$</span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="built-in-deployment-tools">
<span id="built-in-deployment"></span><h2><a class="toc-backref" href="#id38">Built-In Deployment Tools</a><a class="headerlink" href="#built-in-deployment-tools" title="Permalink to this headline"> </a></h2>
<p>MLflow provides tools for deploying MLflow models on a local machine and to several production environments.
Not all deployment methods are available for all model flavors.</p>
<div class="contents local topic" id="id17">
<p class="topic-title">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#local-model-deployment" id="id73">Deploy MLflow models</a></p></li>
<li><p><a class="reference internal" href="#deploy-a-python-function-model-on-microsoft-azure-ml" id="id74">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Microsoft Azure ML</a></p></li>
<li><p><a class="reference internal" href="#deploy-a-python-function-model-on-amazon-sagemaker" id="id75">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Amazon SageMaker</a></p></li>
<li><p><a class="reference internal" href="#export-a-python-function-model-as-an-apache-spark-udf" id="id76">Export a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model as an Apache Spark UDF</a></p></li>
</ul>
</div>
<div class="section" id="local-model-deployment">
<span id="id18"></span><h3><a class="toc-backref" href="#id73">Deploy MLflow models</a><a class="headerlink" href="#local-model-deployment" title="Permalink to this headline"> </a></h3>
<p>MLflow can deploy models locally as local REST API endpoints or to directly score files. In addition,
MLflow can package models as self-contained Docker images with the REST API endpoint. The image can
be used to safely deploy the model to various environments such as Kubernetes.</p>
<p>You deploy MLflow model locally or generate a Docker image using the CLI interface to the
<a class="reference internal" href="python_api/mlflow.models.html#module-mlflow.models" title="mlflow.models"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.models</span></code></a> module.</p>
<p>The REST API defines 4 endpoints:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/ping</span></code> used for health check</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/health</span></code> (same as /ping)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/version</span></code> used for getting the mlflow version</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/invocations</span></code> used for scoring</p></li>
</ul>
<p>The REST API server accepts csv or json input. The input format must be specified in
<code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> header. The value of the header must be either <code class="docutils literal notranslate"><span class="pre">application/json</span></code> or
<code class="docutils literal notranslate"><span class="pre">application/csv</span></code>.</p>
<p>The csv input must be a valid pandas.DataFrame csv representation. For example,
<code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">pandas_df.to_csv()</span></code>.</p>
<p>The json input must be a dictionary with exactly one of the following fields that further specify
the type and encoding of the input data</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataframe_split</span></code> field with pandas DataFrames in the <code class="docutils literal notranslate"><span class="pre">split</span></code> orientation. For example,
<code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">{&quot;dataframe_split&quot;:</span> <span class="pre">pandas_df.to_dict(orient='split')</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dataframe_records</span></code> field with pandas DataFrame in the <code class="docutils literal notranslate"><span class="pre">records</span></code> orientation. For example,
<code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">{&quot;dataframe_records&quot;:</span> <span class="pre">pandas_df.to_dict(orient='records')</span></code>.*We do not
recommend using this format because it is not guaranteed to preserve column ordering.*</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">instances</span></code> field with tensor input formatted as described in <a class="reference external" href="https://www.tensorflow.org/tfx/serving/api_rest#request_format_2">TF Serving’s API docs</a> where the provided inputs
will be cast to Numpy arrays.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inputs</span></code> field with tensor input formatted as described in <a class="reference external" href="https://www.tensorflow.org/tfx/serving/api_rest#request_format_2">TF Serving’s API docs</a> where the provided inputs
will be cast to Numpy arrays.</p></li>
</ul>
<p>The json input also has an optional field <code class="docutils literal notranslate"><span class="pre">params</span></code> that can be used to pass additional parameters.
Valid parameters types are <code class="docutils literal notranslate"><span class="pre">Union[DataType,</span> <span class="pre">List[DataType],</span> <span class="pre">None]</span></code> where DataType is
<a class="reference internal" href="python_api/mlflow.types.html#mlflow.types.DataType" title="mlflow.types.DataType"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">data</span> <span class="pre">types</span></code></a>. In order to pass params, a valid
<a class="reference internal" href="#model-signature"><span class="std std-ref">Model Signature</span></a> with <code class="docutils literal notranslate"><span class="pre">params</span></code> must be defined.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since JSON loses type information, MLflow will cast the JSON input to the input type specified
in the model’s schema if available. If your model is sensitive to input types, it is recommended that
a schema is provided for the model to ensure that type mismatch errors do not occur at inference time.
In particular, DL models are typically strict about input types and will need model schema in order
for the model to score correctly. For complex data types, see <a class="reference internal" href="#encoding-complex-data"><span class="std std-ref">Encoding complex data</span></a> below.</p>
</div>
<p>Example requests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># split-oriented DataFrame input</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;dataframe_split&quot;: {</span>
<span class="s1">      &quot;columns&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],</span>
<span class="s1">      &quot;data&quot;: [[1, 2, 3], [4, 5, 6]]</span>
<span class="s1">  }</span>
<span class="s1">}&#39;</span>

<span class="c1"># record-oriented DataFrame input (fine for vector rows, loses ordering for JSON records)</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;dataframe_records&quot;: [</span>
<span class="s1">    {&quot;a&quot;: 1,&quot;b&quot;: 2,&quot;c&quot;: 3},</span>
<span class="s1">    {&quot;a&quot;: 4,&quot;b&quot;: 5,&quot;c&quot;: 6}</span>
<span class="s1">  ]</span>
<span class="s1">}&#39;</span>

<span class="c1"># numpy/tensor input using TF serving&#39;s &quot;instances&quot; format</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;instances&quot;: [</span>
<span class="s1">        {&quot;a&quot;: &quot;s1&quot;, &quot;b&quot;: 1, &quot;c&quot;: [1, 2, 3]},</span>
<span class="s1">        {&quot;a&quot;: &quot;s2&quot;, &quot;b&quot;: 2, &quot;c&quot;: [4, 5, 6]},</span>
<span class="s1">        {&quot;a&quot;: &quot;s3&quot;, &quot;b&quot;: 3, &quot;c&quot;: [7, 8, 9]}</span>
<span class="s1">    ]</span>
<span class="s1">}&#39;</span>

<span class="c1"># numpy/tensor input using TF serving&#39;s &quot;inputs&quot; format</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;inputs&quot;: {&quot;a&quot;: [&quot;s1&quot;, &quot;s2&quot;, &quot;s3&quot;], &quot;b&quot;: [1, 2, 3], &quot;c&quot;: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]}</span>
<span class="s1">}&#39;</span>

<span class="c1"># inference with params</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;inputs&quot;: {&quot;question&quot;: [&quot;What color is it?&quot;],</span>
<span class="s1">               &quot;context&quot;: [&quot;Some people said it was green but I know that it is pink.&quot;]},</span>
<span class="s1">    &quot;params&quot;: {&quot;max_answer_len&quot;: 10}</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>For more information about serializing pandas DataFrames, see
<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html">pandas.DataFrame.to_json</a>.</p>
<p>For more information about serializing tensor inputs using the TF serving format, see
<a class="reference external" href="https://www.tensorflow.org/tfx/serving/api_rest#request_format_2">TF serving’s request format docs</a>.</p>
<div class="section" id="serving-with-mlserver">
<span id="id20"></span><h4>Serving with MLServer<a class="headerlink" href="#serving-with-mlserver" title="Permalink to this headline"> </a></h4>
<p>Python models can be deployed using <a class="reference external" href="https://mlserver.readthedocs.io/en/latest/">Seldon’s MLServer</a> as alternative inference server.
MLServer is integrated with two leading open source model deployment tools,
<a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol">Seldon Core</a>
and <a class="reference external" href="https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/">KServe (formerly known as KFServing)</a>, and can
be used to test and deploy models using these frameworks.
This is especially powerful when building docker images since the docker image
built with MLServer can be deployed directly with both of these frameworks.</p>
<p>MLServer exposes the same scoring API through the <code class="docutils literal notranslate"><span class="pre">/invocations</span></code> endpoint.
In addition, it supports the standard <a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html">V2 Inference Protocol</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use MLServer with MLflow, please install <code class="docutils literal notranslate"><span class="pre">mlflow</span></code> as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mlflow<span class="o">[</span>extras<span class="o">]</span>
</pre></div>
</div>
</div>
<p>To serve a MLflow model using MLServer, you can use the <code class="docutils literal notranslate"><span class="pre">--enable-mlserver</span></code> flag,
such as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>my_model<span class="w"> </span>--enable-mlserver
</pre></div>
</div>
<p>Similarly, to build a Docker image built with MLServer you can use the
<code class="docutils literal notranslate"><span class="pre">--enable-mlserver</span></code> flag, such as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>build-docker<span class="w"> </span>-m<span class="w"> </span>my_model<span class="w"> </span>--enable-mlserver<span class="w"> </span>-n<span class="w"> </span>my-model
</pre></div>
</div>
<p>To read more about the integration between MLflow and MLServer, please check
the <a class="reference external" href="https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html">end-to-end example in the MLServer documentation</a> or
visit the <a class="reference external" href="https://mlserver.readthedocs.io/en/latest/">MLServer docs</a>.</p>
</div>
<div class="section" id="encoding-complex-data">
<span id="id21"></span><h4>Encoding complex data<a class="headerlink" href="#encoding-complex-data" title="Permalink to this headline"> </a></h4>
<p>Complex data types, such as dates or binary, do not have a native JSON representation. If you include a model
signature, MLflow can automatically decode supported data types from JSON. The following data type conversions
are supported:</p>
<ul class="simple">
<li><p>binary: data is expected to be base64 encoded, MLflow will automatically base64 decode.</p></li>
<li><p>datetime: data is expected as string according to
<a class="reference external" href="https://www.iso.org/iso-8601-date-and-time-format.html">ISO 8601 specification</a>.
MLflow will parse this into the appropriate datetime representation on the given platform.</p></li>
</ul>
<p>Example requests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># record-oriented DataFrame input with binary column &quot;b&quot;</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;[</span>
<span class="s1">    {&quot;a&quot;: 0, &quot;b&quot;: &quot;dGVzdCBiaW5hcnkgZGF0YSAw&quot;},</span>
<span class="s1">    {&quot;a&quot;: 1, &quot;b&quot;: &quot;dGVzdCBiaW5hcnkgZGF0YSAx&quot;},</span>
<span class="s1">    {&quot;a&quot;: 2, &quot;b&quot;: &quot;dGVzdCBiaW5hcnkgZGF0YSAy&quot;}</span>
<span class="s1">]&#39;</span>

<span class="c1"># record-oriented DataFrame input with datetime column &quot;b&quot;</span>
curl<span class="w"> </span>http://127.0.0.1:5000/invocations<span class="w"> </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;[</span>
<span class="s1">    {&quot;a&quot;: 0, &quot;b&quot;: &quot;2020-01-01T00:00:00Z&quot;},</span>
<span class="s1">    {&quot;a&quot;: 1, &quot;b&quot;: &quot;2020-02-01T12:34:56Z&quot;},</span>
<span class="s1">    {&quot;a&quot;: 2, &quot;b&quot;: &quot;2021-03-01T00:00:00Z&quot;}</span>
<span class="s1">]&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="command-line-interface">
<h4>Command Line Interface<a class="headerlink" href="#command-line-interface" title="Permalink to this headline"> </a></h4>
<p>MLflow also has a CLI that supports the following commands:</p>
<ul class="simple">
<li><p><a class="reference external" href="cli.html#mlflow-models-serve">serve</a> deploys the model as a local REST API server.</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-models-build-docker">build_docker</a> packages a REST API endpoint serving the
model as a docker image.</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-models-predict">predict</a> uses the model to generate a prediction for a local
CSV or JSON file. Note that this method only supports DataFrame input.</p></li>
</ul>
<p>For more info, see:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>--help
mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>--help
mlflow<span class="w"> </span>models<span class="w"> </span>predict<span class="w"> </span>--help
mlflow<span class="w"> </span>models<span class="w"> </span>build-docker<span class="w"> </span>--help
</pre></div>
</div>
</div>
<div class="section" id="environment-management-tools">
<span id="model-enviroment-management"></span><h4>Environment Management Tools<a class="headerlink" href="#environment-management-tools" title="Permalink to this headline"> </a></h4>
<p>MLflow currently supports the following environment management tools to restore model environments:</p>
<dl>
<dt>local</dt><dd><p>Use the local environment. No extra tools are required.</p>
</dd>
<dt>virtualenv (preferred)</dt><dd><p>Create environments using virtualenv and pyenv (for python version management). Virtualenv and
pyenv (for Linux and macOS) or pyenv-win (for Windows) must be installed for this mode of environment reconstruction.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://virtualenv.pypa.io/en/latest/installation.html">virtualenv installation instructions</a></p></li>
<li><p><a class="reference external" href="https://github.com/pyenv/pyenv#installation">pyenv installation instructions</a></p></li>
<li><p><a class="reference external" href="https://github.com/pyenv-win/pyenv-win#installation">pyenv-win installation instructions</a></p></li>
</ul>
</dd>
<dt>conda</dt><dd><p>Create environments using conda. Conda must be installed for this mode of environment reconstruction.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>By using conda, you’re responsible for adhering to <a class="reference external" href="https://legal.anaconda.com/policies/en/?name=terms-of-service">Anaconda’s terms of service</a>.</p>
</div>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">conda installation instructions</a></p></li>
</ul>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">models</span></code> CLI commands provide an optional <code class="docutils literal notranslate"><span class="pre">--env-manager</span></code> argument that selects a specific environment management configuration to be used, as shown below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use virtualenv</span>
mlflow<span class="w"> </span>models<span class="w"> </span>predict<span class="w"> </span>...<span class="w"> </span>--env-manager<span class="o">=</span>virtualenv
<span class="c1"># Use conda</span>
mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>...<span class="w"> </span>--env-manager<span class="o">=</span>conda
</pre></div>
</div>
</div>
</div>
<div class="section" id="deploy-a-python-function-model-on-microsoft-azure-ml">
<span id="azureml-deployment"></span><h3><a class="toc-backref" href="#id74">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Microsoft Azure ML</a><a class="headerlink" href="#deploy-a-python-function-model-on-microsoft-azure-ml" title="Permalink to this headline"> </a></h3>
<p>The MLflow plugin <a class="reference external" href="https://pypi.org/project/azureml-mlflow/">azureml-mlflow</a> can deploy models to Azure ML, either to Azure Kubernetes Service (AKS) or Azure Container Instances (ACI) for real-time serving.</p>
<p>The resulting deployment accepts the following data formats as input:</p>
<ul class="simple">
<li><p>JSON-serialized pandas DataFrames in the <code class="docutils literal notranslate"><span class="pre">split</span></code> orientation. For example, <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">pandas_df.to_json(orient='split')</span></code>. This format is specified using a <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> request header value of <code class="docutils literal notranslate"><span class="pre">application/json</span></code>.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code> input format is not fully supported for deployments on Azure Machine Learning at the moment. Be aware that many <code class="docutils literal notranslate"><span class="pre">autolog()</span></code> implementations may use <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code> for model’s signatures when logging models and hence those deployments will fail in Azure ML.</p>
</div>
<p>Deployments can be generated using both the Python API or MLflow CLI. In both cases, a <code class="docutils literal notranslate"><span class="pre">JSON</span></code> configuration file can be indicated with the details of the deployment you want to achieve. If not indicated, then a default deployment is done using Azure Container Instances (ACI) and a minimal configuration. The full specification of this configuration file can be checked at <a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli#deployment-configuration-schema">Deployment configuration schema</a>. Also, you will also need the Azure ML MLflow Tracking URI of your particular Azure ML Workspace where you want to deploy your model. You can obtain this URI in several ways:</p>
<ul class="simple">
<li><p>Through the <a class="reference external" href="https://ml.azure.com">Azure ML Studio</a>:</p>
<ul>
<li><p>Navigate to <a class="reference external" href="https://ml.azure.com">Azure ML Studio</a> and select the workspace you are working on.</p></li>
<li><p>Click on the name of the workspace at the upper right corner of the page.</p></li>
<li><p>Click “View all properties in Azure Portal” on the pane popup.</p></li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">MLflow</span> <span class="pre">tracking</span> <span class="pre">URI</span></code> value from the properties section.</p></li>
</ul>
</li>
<li><p>Programmatically, using Azure ML SDK with the method <a class="reference external" href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-get-mlflow-tracking-uri">Workspace.get_mlflow_tracking_uri()</a>. If you are running inside Azure ML Compute, like for instance a Compute Instance, you can get this value also from the environment variable <code class="docutils literal notranslate"><span class="pre">os.environ[&quot;MLFLOW_TRACKING_URI&quot;]</span></code>.</p></li>
<li><p>Manually, for a given Subscription ID, Resource Group and Azure ML Workspace, the URI is as follows: <code class="docutils literal notranslate"><span class="pre">azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/&lt;SUBSCRIPTION_ID&gt;/resourceGroups/&lt;RESOURCE_GROUP_NAME&gt;/providers/Microsoft.MachineLearningServices/workspaces/&lt;WORKSPACE_NAME&gt;</span></code></p></li>
</ul>
<p class="rubric">Configuration example for ACI deployment</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;computeType&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;aci&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;containerResourceRequirements&quot;</span><span class="p">:</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;cpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;memoryInGB&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;location&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;eastus2&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Remarks:</dt><dd><ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">containerResourceRequirements</span></code> is not indicated, a deployment with minimal compute configuration is applied (<code class="docutils literal notranslate"><span class="pre">cpu:</span> <span class="pre">0.1</span></code> and <code class="docutils literal notranslate"><span class="pre">memory:</span> <span class="pre">0.5</span></code>).</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">location</span></code> is not indicated, it defaults to the location of the workspace.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Configuration example for an AKS deployment</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;computeType&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;aks&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;computeTargetName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;aks-mlflow&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Remarks:</dt><dd><ul class="simple">
<li><p>In above example, <code class="docutils literal notranslate"><span class="pre">aks-mlflow</span></code> is the name of an Azure Kubernetes Cluster registered/created in Azure Machine Learning.</p></li>
</ul>
</dd>
</dl>
<p>The following examples show how to create a deployment in ACI. Please, ensure you have <a class="reference external" href="https://pypi.org/project/azureml-mlflow/">azureml-mlflow</a> installed before continuing.</p>
<p class="rubric">Example: Workflow using the Python API</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">mlflow.deployments</span> <span class="kn">import</span> <span class="n">get_deploy_client</span>

<span class="c1"># Create the deployment configuration.</span>
<span class="c1"># If no deployment configuration is provided, then the deployment happens on ACI.</span>
<span class="n">deploy_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;computeType&quot;</span><span class="p">:</span> <span class="s2">&quot;aci&quot;</span><span class="p">}</span>

<span class="c1"># Write the deployment configuration into a file.</span>
<span class="n">deployment_config_path</span> <span class="o">=</span> <span class="s2">&quot;deployment_config.json&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">deployment_config_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">outfile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">deploy_config</span><span class="p">))</span>

<span class="c1"># Set the tracking uri in the deployment client.</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">get_deploy_client</span><span class="p">(</span><span class="s2">&quot;&lt;azureml-mlflow-tracking-url&gt;&quot;</span><span class="p">)</span>

<span class="c1"># MLflow requires the deployment configuration to be passed as a dictionary.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;deploy-config-file&quot;</span><span class="p">:</span> <span class="n">deployment_config_path</span><span class="p">}</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;mymodel&quot;</span>
<span class="n">model_version</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># define the model path and the name is the service name</span>
<span class="c1"># if model is not registered, it gets registered automatically and a name is autogenerated using the &quot;name&quot; parameter below</span>
<span class="n">client</span><span class="o">.</span><span class="n">create_deployment</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;models:/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mymodel-aci-deployment&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># After the model deployment completes, requests can be posted via HTTP to the new ACI</span>
<span class="c1"># webservice&#39;s scoring URI.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Scoring URI is: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">webservice</span><span class="o">.</span><span class="n">scoring_uri</span><span class="p">)</span>

<span class="c1"># The following example posts a sample input from the wine dataset</span>
<span class="c1"># used in the MLflow ElasticNet example:</span>
<span class="c1"># https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine</span>

<span class="c1"># `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="c1"># `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;columns&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;alcohol&quot;</span><span class="p">,</span>
        <span class="s2">&quot;chlorides&quot;</span><span class="p">,</span>
        <span class="s2">&quot;citric acid&quot;</span><span class="p">,</span>
        <span class="s2">&quot;density&quot;</span><span class="p">,</span>
        <span class="s2">&quot;fixed acidity&quot;</span><span class="p">,</span>
        <span class="s2">&quot;free sulfur dioxide&quot;</span><span class="p">,</span>
        <span class="s2">&quot;pH&quot;</span><span class="p">,</span>
        <span class="s2">&quot;residual sugar&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sulphates&quot;</span><span class="p">,</span>
        <span class="s2">&quot;total sulfur dioxide&quot;</span><span class="p">,</span>
        <span class="s2">&quot;volatile acidity&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[[</span><span class="mf">8.8</span><span class="p">,</span> <span class="mf">0.045</span><span class="p">,</span> <span class="mf">0.36</span><span class="p">,</span> <span class="mf">1.001</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">20.7</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mf">0.27</span><span class="p">]],</span>
<span class="p">}</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
    <span class="n">url</span><span class="o">=</span><span class="n">webservice</span><span class="o">.</span><span class="n">scoring_uri</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">sample_input</span><span class="p">),</span>
    <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Content-type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">response_json</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_json</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Example: Workflow using the MLflow CLI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;{ computeType: aci }&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>deployment_config.json
mlflow<span class="w"> </span>deployments<span class="w"> </span>create<span class="w"> </span>--name<span class="w"> </span>&lt;deployment-name&gt;<span class="w"> </span>-m<span class="w"> </span>models:/&lt;model-name&gt;/&lt;model-version&gt;<span class="w"> </span>-t<span class="w"> </span>&lt;azureml-mlflow-tracking-url&gt;<span class="w"> </span>--deploy-config-file<span class="w"> </span>deployment_config.json

<span class="c1"># After the deployment completes, requests can be posted via HTTP to the new ACI</span>
<span class="c1"># webservice&#39;s scoring URI.</span>

<span class="nv">scoring_uri</span><span class="o">=</span><span class="k">$(</span>az<span class="w"> </span>ml<span class="w"> </span>service<span class="w"> </span>show<span class="w"> </span>--name<span class="w"> </span>&lt;deployment-name&gt;<span class="w"> </span>-v<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;.scoringUri&quot;</span><span class="k">)</span>

<span class="c1"># The following example posts a sample input from the wine dataset</span>
<span class="c1"># used in the MLflow ElasticNet example:</span>
<span class="c1"># https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine</span>

<span class="c1"># `sample_input` is a JSON-serialized pandas DataFrame with the `split` orientation</span>
<span class="nv">sample_input</span><span class="o">=</span><span class="s1">&#39;</span>
<span class="s1">{</span>
<span class="s1">    &quot;columns&quot;: [</span>
<span class="s1">        &quot;alcohol&quot;,</span>
<span class="s1">        &quot;chlorides&quot;,</span>
<span class="s1">        &quot;citric acid&quot;,</span>
<span class="s1">        &quot;density&quot;,</span>
<span class="s1">        &quot;fixed acidity&quot;,</span>
<span class="s1">        &quot;free sulfur dioxide&quot;,</span>
<span class="s1">        &quot;pH&quot;,</span>
<span class="s1">        &quot;residual sugar&quot;,</span>
<span class="s1">        &quot;sulphates&quot;,</span>
<span class="s1">        &quot;total sulfur dioxide&quot;,</span>
<span class="s1">        &quot;volatile acidity&quot;</span>
<span class="s1">    ],</span>
<span class="s1">    &quot;data&quot;: [</span>
<span class="s1">        [8.8, 0.045, 0.36, 1.001, 7, 45, 3, 20.7, 0.45, 170, 0.27]</span>
<span class="s1">    ]</span>
<span class="s1">}&#39;</span>

<span class="nb">echo</span><span class="w"> </span><span class="nv">$sample_input</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>curl<span class="w"> </span>-s<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="nv">$scoring_uri</span><span class="se">\</span>
-H<span class="w"> </span><span class="s1">&#39;Cache-Control: no-cache&#39;</span><span class="se">\</span>
-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="se">\</span>
-d<span class="w"> </span>@-
</pre></div>
</div>
<p>You can also test your deployments locally first using the option <cite>run-local</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>deployments<span class="w"> </span>run-local<span class="w"> </span>--name<span class="w"> </span>&lt;deployment-name&gt;<span class="w"> </span>-m<span class="w"> </span>models:/&lt;model-name&gt;/&lt;model-version&gt;<span class="w"> </span>-t<span class="w"> </span>&lt;azureml-mlflow-tracking-url&gt;
</pre></div>
</div>
<p>For more info, see:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>deployments<span class="w"> </span><span class="nb">help</span><span class="w"> </span>-t<span class="w"> </span>azureml
</pre></div>
</div>
</div>
<div class="section" id="deploy-a-python-function-model-on-amazon-sagemaker">
<span id="sagemaker-deployment"></span><h3><a class="toc-backref" href="#id75">Deploy a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model on Amazon SageMaker</a><a class="headerlink" href="#deploy-a-python-function-model-on-amazon-sagemaker" title="Permalink to this headline"> </a></h3>
<p>The <a class="reference internal" href="python_api/mlflow.deployments.html#module-mlflow.deployments" title="mlflow.deployments"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.deployments</span></code></a> and <a class="reference internal" href="python_api/mlflow.sagemaker.html#module-mlflow.sagemaker" title="mlflow.sagemaker"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.sagemaker</span></code></a> modules can deploy
<code class="docutils literal notranslate"><span class="pre">python_function</span></code> models locally in a Docker container with SageMaker compatible environment and
remotely on SageMaker. To deploy remotely to SageMaker you need to set up your environment and user
accounts. To export a custom model to SageMaker, you need a MLflow-compatible Docker image to be
available on Amazon ECR. MLflow provides a default Docker image definition; however, it is up to you
to build the image and upload it to ECR. MLflow includes the utility function
<code class="docutils literal notranslate"><span class="pre">build_and_push_container</span></code> to perform this step. Once built and uploaded, you can use the MLflow
container for all MLflow Models. Model webservers deployed using the <a class="reference internal" href="python_api/mlflow.deployments.html#module-mlflow.deployments" title="mlflow.deployments"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mlflow.deployments</span></code></a>
module accept the following data formats as input, depending on the deployment flavor:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python_function</span></code>: For this deployment flavor, the endpoint accepts the same formats described
in the <a class="reference internal" href="#local-model-deployment"><span class="std std-ref">local model deployment documentation</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mleap</span></code>: For this deployment flavor, the endpoint accepts <cite>only</cite>
JSON-serialized pandas DataFrames in the <code class="docutils literal notranslate"><span class="pre">split</span></code> orientation. For example,
<code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">pandas_df.to_json(orient='split')</span></code>. This format is specified using a <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code>
request header value of <code class="docutils literal notranslate"><span class="pre">application/json</span></code>.</p></li>
</ul>
<div class="section" id="commands">
<h4>Commands<a class="headerlink" href="#commands" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="python_api/mlflow.sagemaker.html#mlflow.sagemaker.run_local" title="mlflow.sagemaker.run_local"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">deployments</span> <span class="pre">run-local</span> <span class="pre">-t</span> <span class="pre">sagemaker</span></code></a> deploys the
model locally in a Docker container. The image and the environment should be identical to how the
model would be run remotely and it is therefore useful for testing the model prior to deployment.</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-sagemaker-build-and-push-container">mlflow sagemaker build-and-push-container</a>
builds an MLfLow Docker image and uploads it to ECR. The caller must have the correct permissions
set up. The image is built locally and requires Docker to be present on the machine that performs
this step.</p></li>
<li><p><a class="reference internal" href="python_api/mlflow.sagemaker.html#mlflow.sagemaker.SageMakerDeploymentClient.create_deployment" title="mlflow.sagemaker.SageMakerDeploymentClient.create_deployment"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">deployments</span> <span class="pre">create</span> <span class="pre">-t</span> <span class="pre">sagemaker</span></code></a>
deploys the model on Amazon SageMaker. MLflow uploads the Python Function model into S3 and starts
an Amazon SageMaker endpoint serving the model.</p></li>
</ul>
<p class="rubric">Example workflow using the MLflow CLI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>sagemaker<span class="w"> </span>build-and-push-container<span class="w">  </span><span class="c1"># build the container (only needs to be called once)</span>
mlflow<span class="w"> </span>deployments<span class="w"> </span>run-local<span class="w"> </span>-t<span class="w"> </span>sagemaker<span class="w"> </span>--name<span class="w"> </span>&lt;deployment-name&gt;<span class="w"> </span>-m<span class="w"> </span>&lt;path-to-model&gt;<span class="w">  </span><span class="c1"># test the model locally</span>
mlflow<span class="w"> </span>deployments<span class="w"> </span>sagemaker<span class="w"> </span>create<span class="w"> </span>-t<span class="w">  </span><span class="c1"># deploy the model remotely</span>
</pre></div>
</div>
<p>For more info, see:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>sagemaker<span class="w"> </span>--help
mlflow<span class="w"> </span>sagemaker<span class="w"> </span>build-and-push-container<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>run-local<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span><span class="nb">help</span><span class="w"> </span>-t<span class="w"> </span>sagemaker
</pre></div>
</div>
</div>
</div>
<div class="section" id="export-a-python-function-model-as-an-apache-spark-udf">
<h3><a class="toc-backref" href="#id76">Export a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model as an Apache Spark UDF</a><a class="headerlink" href="#export-a-python-function-model-as-an-apache-spark-udf" title="Permalink to this headline"> </a></h3>
<p>You can output a <code class="docutils literal notranslate"><span class="pre">python_function</span></code> model as an Apache Spark UDF, which can be uploaded to a
Spark cluster and used to score the model.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">struct</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;&lt;path-to-model&gt;&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">(</span><span class="n">struct</span><span class="p">([</span><span class="o">...</span><span class="p">])))</span>
</pre></div>
</div>
<p>If a model contains a signature, the UDF can be called without specifying column name arguments.
In this case, the UDF will be called with column names from signature, so the evaluation
dataframe’s column names must match the model signature’s column names.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;&lt;path-to-model-with-signature&gt;&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">())</span>
</pre></div>
</div>
<p>If a model contains a signature with tensor spec inputs,
you will need to pass a column of array type as a corresponding UDF argument.
The values in this column must be comprised of one-dimensional arrays. The
UDF will reshape the array values to the required shape with ‘C’ order
(i.e. read / write the elements using C-like index order) and cast the values
as the required tensor spec type. For example, assuming a model
requires input ‘a’ of shape (-1, 2, 3) and input ‘b’ of shape (-1, 4, 5). In order to
perform inference on this data, we need to prepare a Spark DataFrame with column ‘a’
containing arrays of length 6 and column ‘b’ containing arrays of length 20. We can then
invoke the UDF like following example code:</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="c1"># Assuming the model requires input &#39;a&#39; of shape (-1, 2, 3) and input &#39;b&#39; of shape (-1, 4, 5)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;&lt;path-to-model-requiring-multidimensional-inputs&gt;&quot;</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
<span class="c1"># The `spark_df` has column &#39;a&#39; containing arrays of length 6 and</span>
<span class="c1"># column &#39;b&#39; containing arrays of length 20</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)))</span>
</pre></div>
</div>
<p>The resulting UDF is based on Spark’s Pandas UDF and is currently limited to producing either a single
value, an array of values, or a struct containing multiple field values
of the same type per observation. By default, we return the first
numeric column as a double. You can control what result is returned by supplying <code class="docutils literal notranslate"><span class="pre">result_type</span></code>
argument. The following values are supported:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'int'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType">IntegerType</a>: The leftmost integer that can fit in
<code class="docutils literal notranslate"><span class="pre">int32</span></code> result is returned or an exception is raised if there are none.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'long'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType">LongType</a>: The leftmost long integer that can fit in <code class="docutils literal notranslate"><span class="pre">int64</span></code>
result is returned or an exception is raised if there are none.</p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType">ArrayType</a> (<a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.IntegerType.html#pyspark.sql.types.IntegerType">IntegerType</a> | <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html#pyspark.sql.types.LongType">LongType</a>): Return all integer columns that can fit
into the requested size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'float'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType">FloatType</a>: The leftmost numeric result cast to
<code class="docutils literal notranslate"><span class="pre">float32</span></code> is returned or an exception is raised if there are no numeric columns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'double'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType">DoubleType</a>: The leftmost numeric result cast to
<code class="docutils literal notranslate"><span class="pre">double</span></code> is returned or an exception is raised if there are no numeric columns.</p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType">ArrayType</a> ( <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.FloatType.html#pyspark.sql.types.FloatType">FloatType</a> | <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.DoubleType.html#pyspark.sql.types.DoubleType">DoubleType</a> ): Return all numeric columns cast to the
requested type. An exception is raised if there are no numeric columns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'string'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType">StringType</a>: Result is the leftmost column cast as string.</p></li>
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType">ArrayType</a> ( <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html#pyspark.sql.types.StringType">StringType</a> ): Return all columns cast as string.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'bool'</span></code> or <code class="docutils literal notranslate"><span class="pre">'boolean'</span></code> or <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.BooleanType.html#pyspark.sql.types.BooleanType">BooleanType</a>: The leftmost column cast to <code class="docutils literal notranslate"><span class="pre">bool</span></code>
is returned or an exception is raised if the values cannot be coerced.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'field1</span> <span class="pre">FIELD1_TYPE,</span> <span class="pre">field2</span> <span class="pre">FIELD2_TYPE,</span> <span class="pre">...'</span></code>: A struct type containing
multiple fields separated by comma, each field type must be one of types
listed above.</p></li>
</ul>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="c1"># Suppose the PyFunc model `predict` method returns a dict like:</span>
<span class="c1"># `{&#39;prediction&#39;: 1-dim_array, &#39;probability&#39;: 2-dim_array}`</span>
<span class="c1"># You can supply result_type to be a struct type containing</span>
<span class="c1"># 2 fields &#39;prediction&#39; and &#39;probability&#39; like following.</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span>
    <span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;&lt;path-to-model&gt;&quot;</span><span class="p">,</span> <span class="n">result_type</span><span class="o">=</span><span class="s2">&quot;prediction float, probability: array&lt;float&gt;&quot;</span>
<span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">())</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">ArrayType</span><span class="p">,</span> <span class="n">FloatType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">struct</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span>
    <span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;path/to/model&quot;</span><span class="p">,</span> <span class="n">result_type</span><span class="o">=</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">FloatType</span><span class="p">())</span>
<span class="p">)</span>
<span class="c1"># The prediction column will contain all the numeric columns returned by the model as floats</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)))</span>
</pre></div>
</div>
<p>If you want to use conda to restore the python environment that was used to train the model,
set the <cite>env_manager</cite> argument when calling <a class="reference internal" href="python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf" title="mlflow.pyfunc.spark_udf"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.spark_udf()</span></code></a>.</p>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">ArrayType</span><span class="p">,</span> <span class="n">FloatType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">struct</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">pyfunc_udf</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">spark_udf</span><span class="p">(</span>
    <span class="n">spark</span><span class="p">,</span>
    <span class="s2">&quot;path/to/model&quot;</span><span class="p">,</span>
    <span class="n">result_type</span><span class="o">=</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">FloatType</span><span class="p">()),</span>
    <span class="n">env_manager</span><span class="o">=</span><span class="s2">&quot;conda&quot;</span><span class="p">,</span>  <span class="c1"># Use conda to restore the environment used in training</span>
<span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">pyfunc_udf</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="deployment-to-custom-targets">
<span id="deployment-plugin"></span><h2><a class="toc-backref" href="#id39">Deployment to Custom Targets</a><a class="headerlink" href="#deployment-to-custom-targets" title="Permalink to this headline"> </a></h2>
<p>In addition to the built-in deployment tools, MLflow provides a pluggable
<a class="reference external" href="python_api/mlflow.deployments.html#mlflow.deployments">mlflow.deployments Python API</a> and
<a class="reference external" href="cli.html#mlflow-deployments">mlflow deployments CLI</a> for deploying
models to custom targets and environments. To deploy to a custom target, you must first install an
appropriate third-party Python plugin. See the list of known community-maintained plugins
<a class="reference external" href="plugins.html#deployment-plugins">here</a>.</p>
<div class="section" id="id24">
<h3>Commands<a class="headerlink" href="#id24" title="Permalink to this headline"> </a></h3>
<p>The <cite>mlflow deployments</cite> CLI contains the following commands, which can also be invoked programmatically
using the <a class="reference external" href="python_api/mlflow.deployments.html#mlflow.deployments">mlflow.deployments Python API</a>:</p>
<ul class="simple">
<li><p><a class="reference external" href="cli.html#mlflow-deployments-create">Create</a>: Deploy an MLflow model to a specified custom target</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-delete">Delete</a>: Delete a deployment</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-update">Update</a>: Update an existing deployment, for example to
deploy a new model version or change the deployment’s configuration (e.g. increase replica count)</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-list">List</a>: List IDs of all deployments</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-get">Get</a>: Print a detailed description of a particular deployment</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-run-local">Run Local</a>: Deploy the model locally for testing</p></li>
<li><p><a class="reference external" href="cli.html#mlflow-deployments-help">Help</a>: Show the help string for the specified target</p></li>
</ul>
<p>For more info, see:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>deployments<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>create<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>delete<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>update<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>list<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>get<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span>run-local<span class="w"> </span>--help
mlflow<span class="w"> </span>deployments<span class="w"> </span><span class="nb">help</span><span class="w"> </span>--help
</pre></div>
</div>
</div>
</div>
<div class="section" id="id26">
<h2><a class="toc-backref" href="#id40">Community Model Flavors</a><a class="headerlink" href="#id26" title="Permalink to this headline"> </a></h2>
<p>Go to the <a class="reference external" href="community-model-flavors.html">Community Model Flavors</a>
page to get an overview of other useful MLflow flavors, which are developed and
maintained by the MLflow community.</p>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="projects.html" class="btn btn-neutral" title="MLflow Projects" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="model-registry.html" class="btn btn-neutral" title="MLflow Model Registry" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'./',
      VERSION:'2.8.2.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "_static/clippy.svg";</script>
  <script type="text/javascript" src="_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>